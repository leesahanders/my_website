[
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Technical writeups and musings",
    "section": "",
    "text": "Some random technical writeups for things I’ve found useful, all brought to you by Quarto’s blogging capability.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\nJul 9, 2025\n\n\nImproving the performance of R package installation on Workbench\n\n\nWriting up a conversation with a colleague\n\n\nLisa \n\n\n3 min\n\n\n\n\nJul 3, 2025\n\n\nFun with LLM’s\n\n\nAn exploration\n\n\nLisa \n\n\n22 min\n\n\n\n\nJun 18, 2025\n\n\nCreating a custom Workbench login page\n\n\nAdding custom branding\n\n\nLisa \n\n\n2 min\n\n\n\n\nJun 3, 2025\n\n\nExploring Positron\n\n\nA diehard RStudio loyalist takes on a new IDE\n\n\nLisa \n\n\n5 min\n\n\n\n\nMay 21, 2025\n\n\nFinding R System Dependencies\n\n\nNow with the power of pak\n\n\nLisa \n\n\n2 min\n\n\n\n\nMay 7, 2025\n\n\nUsing SAML and SCIM with EntraID with Posit Team\n\n\nI was confused, so I went through this with a friend and wrote up the steps\n\n\nLisa \n\n\n8 min\n\n\n\n\nMay 5, 2025\n\n\nValidation\n\n\nChallenges, tribulations, and learnings\n\n\nLisa \n\n\n10 min\n\n\n\n\nMay 1, 2025\n\n\nConfiguring Jupyter for use with Posit Workbench\n\n\nand some troubleshooting notes when things go wrong\n\n\nLisa \n\n\n2 min\n\n\n\n\nApr 30, 2025\n\n\nMonitoring Posit Team\n\n\nVarious methods for monitoring the Posit Team software\n\n\nLisa \n\n\n2 min\n\n\n\n\nApr 29, 2025\n\n\nMounting drives on a server for Posit Team\n\n\nVarious methods and failure modes with mounted drives\n\n\nLisa \n\n\n4 min\n\n\n\n\nApr 24, 2025\n\n\nSetting the R package repository on Posit Workbench\n\n\nTips and tricks\n\n\nLisa \n\n\n7 min\n\n\n\n\nJan 22, 2025\n\n\nUsing the remote CLI with package manager\n\n\nNow that we can do ALL the things remotely lets go wild!\n\n\nLisa \n\n\n5 min\n\n\n\n\nJan 21, 2025\n\n\nAWS Connections from Posit Team\n\n\nThis article walks through the different access options for data resources in AWS.\n\n\nLisa \n\n\n17 min\n\n\n\n\nOct 2, 2024\n\n\nDealing with ssh keys with ansible and pulumi\n\n\nA very specific brief learning\n\n\nLisa \n\n\n1 min\n\n\n\n\nOct 1, 2024\n\n\nHandling secrets and pinning files to Posit Connect\n\n\nKeep your secrets secret\n\n\nLisa \n\n\n8 min\n\n\n\n\nSep 13, 2024\n\n\nPython projects set up and maintenance\n\n\nThe power of venv!\n\n\nLisa \n\n\n3 min\n\n\n\n\nSep 13, 2024\n\n\nR projects set up and maintenance\n\n\nThe power of renv!\n\n\nLisa \n\n\n8 min\n\n\n\n\nAug 21, 2024\n\n\nJob templating in Kubernetes\n\n\nAdd custom application labels using job templates for Posit Workbench with k8s\n\n\nLisa \n\n\n3 min\n\n\n\n\nJun 21, 2024\n\n\nGoogle data storage and R\n\n\nAccess to resources in Google, an exploration\n\n\nLisa \n\n\n6 min\n\n\n\n\nJun 21, 2024\n\n\nDebugging R Package Environments (renv): A long winded writeup\n\n\nAn overview of environment management in R and a comprehensive summary of the different options that can be configured to support different workflows\n\n\nLisa \n\n\n50 min\n\n\n\n\nApr 29, 2024\n\n\nShiny apps and Analytics\n\n\nAdd instrumentation to Shiny apps for viewer tracking, interactions,etc\n\n\nLisa \n\n\n2 min\n\n\n\n\nJan 11, 2024\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\nThis is some work I did exploring how to access the underlying databricks data storage, without having to go through databricks\n\n\nLisa \n\n\n4 min\n\n\n\n\nDec 8, 2023\n\n\nParallelization in R\n\n\nResources for scaling out your code\n\n\nLisa \n\n\n7 min\n\n\n\n\nAug 29, 2023\n\n\nProblems with git credential persistence when in the cloud\n\n\nThis simple trick will save you so much time\n\n\nLisa \n\n\n3 min\n\n\n\n\nAug 23, 2023\n\n\nGithub setup on a new computer\n\n\nFork n merge with the experts\n\n\nLisa \n\n\n4 min\n\n\n\n\nAug 9, 2023\n\n\nSecuring credentials\n\n\nProtect your credentials by securing them\n\n\nLisa \n\n\n5 min\n\n\n\n\nJul 14, 2023\n\n\nMicrosoft 365 / Sharepoint and R\n\n\nConnecting to resources in Microsoft 365 / Sharepoint\n\n\nLisa \n\n\n10 min\n\n\n\n\nJan 25, 2023\n\n\nPublish and version your models with Vetiver\n\n\nThis is a minimal example of the ML development workflow in R using Vetiver and the Posit Pro products.\n\n\nLisa \n\n\n3 min\n\n\n\n\nSep 20, 2022\n\n\nCommand line / bash and R\n\n\nExperiments in running command line from R\n\n\nLisa \n\n\n8 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "work/vetiver.html",
    "href": "work/vetiver.html",
    "title": "Publish and version your models with Vetiver",
    "section": "",
    "text": "This is a minimal example of the ML development workflow in R using Vetiver and the Posit Pro products.\nFavorite resources:"
  },
  {
    "objectID": "work/vetiver.html#the-steps",
    "href": "work/vetiver.html#the-steps",
    "title": "Publish and version your models with Vetiver",
    "section": "The steps:",
    "text": "The steps:\n\n\ncreate a deployable vetiver model: https://vetiver.rstudio.com/get-started/\n\n\npublish and version your model: https://vetiver.rstudio.com/get-started/version.html\n\n\ndeploy your model as a REST API: https://vetiver.rstudio.com/get-started/deploy.html"
  },
  {
    "objectID": "work/vetiver.html#in-action",
    "href": "work/vetiver.html#in-action",
    "title": "Publish and version your models with Vetiver",
    "section": "In action",
    "text": "In action\nBike Predict (R): https://solutions.posit.co/gallery/bike_predict/ Bike Predict (Python): https://github.com/sol-eng/bike_predict_python\nFollowing steps outline in: https://vetiver.rstudio.com/get-started/version.html"
  },
  {
    "objectID": "work/vetiver.html#train-a-model",
    "href": "work/vetiver.html#train-a-model",
    "title": "Publish and version your models with Vetiver",
    "section": "Train a model",
    "text": "Train a model\nlibrary(tidymodels)\n\ncar_mod &lt;-\n    workflow(mpg ~ ., linear_reg()) %&gt;%\n    fit(mtcars)"
  },
  {
    "objectID": "work/vetiver.html#create-a-vetiver-model",
    "href": "work/vetiver.html#create-a-vetiver-model",
    "title": "Publish and version your models with Vetiver",
    "section": "Create a vetiver model",
    "text": "Create a vetiver model\nlibrary(vetiver)\nv &lt;- vetiver_model(car_mod, \"lisa.anders/cars_mpg_model\")\nv"
  },
  {
    "objectID": "work/vetiver.html#store-and-version-your-model",
    "href": "work/vetiver.html#store-and-version-your-model",
    "title": "Publish and version your models with Vetiver",
    "section": "Store and version your model",
    "text": "Store and version your model\nStore secrets in your environment so they aren’t exposed in your code directly with usethis::edit_r_environ(). Pinned to: https://colorado.posit.co/rsc/connect/#/apps/b858985c-4e7f-4e05-be35-1bb6f21c9cd4/access\nauth = “envvar” uses environment variables CONNECT_API_KEY and CONNECT_SERVER.\nlibrary(pins)\nmodel_board &lt;- board_connect(auth = \"envvar\") # authenticates to Connect via environment variables\nvetiver_pin_write(model_board, v)\nTo read the vetiver model object from your board, use model_board %&gt;% vetiver_pin_read(\"cars_mpg\")."
  },
  {
    "objectID": "work/vetiver.html#create-a-rest-api-for-deployment",
    "href": "work/vetiver.html#create-a-rest-api-for-deployment",
    "title": "Publish and version your models with Vetiver",
    "section": "Create a REST API for deployment",
    "text": "Create a REST API for deployment\nTo start a server using this object, pipe (%&gt;%) to pr_run(port = 8080) or your port of choice.\nFor RStudio Connect, you can deploy your versioned model with a single function, either vetiver_deploy_rsconnect() for R or vetiver.deploy_rsconnect() for Python. For more on these options, see the Connect documentation for using vetiver.\n# Run while developing\n\n# library(plumber)\n# pr() %&gt;%\n#   vetiver_api(v) %&gt;% \n#   pr_run(port = 8080)\n\n# system2(\"ls\")"
  },
  {
    "objectID": "work/vetiver.html#deploy-to-connect",
    "href": "work/vetiver.html#deploy-to-connect",
    "title": "Publish and version your models with Vetiver",
    "section": "Deploy to Connect",
    "text": "Deploy to Connect\nAPI deployed to: https://colorado.posit.co/rsc/connect/#/apps/01ef1a23-3cd3-459f-b5e1-9c002104c065/access\nvetiver_deploy_rsconnect( \n     model_board, \n     \"lisa.anders/cars_mpg_model\", \n     predict_args = list(debug = TRUE), \n     #server=\"\",\n     account = \"lisa.anders\" \n     )"
  },
  {
    "objectID": "work/vetiver.html#predict-from-your-model-endpoint",
    "href": "work/vetiver.html#predict-from-your-model-endpoint",
    "title": "Publish and version your models with Vetiver",
    "section": "Predict from your model endpoint",
    "text": "Predict from your model endpoint\nendpoint &lt;- vetiver_endpoint(\"https://server.url.co/rsc/content/01ef1a23-3cd3-459f-b5e1-9c002104c065/predict\")\n\napiKey &lt;- Sys.getenv(\"CONNECT_API_KEY\")\n\n\nnew_car &lt;- tibble(cyl = 4,  disp = 200,\n                  hp = 100, drat = 3,\n                  wt = 3,   qsec = 17,\n                  vs = 0,   am = 1,\n                  gear = 4, carb = 2)\n\n\ntime_start &lt;- Sys.time()\n\npredict(endpoint, new_car, httr::add_headers(Authorization = paste(\"Key\", apiKey)))\n\ntime_end &lt;- Sys.time()\n\ntime_diff &lt;- difftime(time_end, time_start)"
  },
  {
    "objectID": "work/vetiver.html#compute-metrics",
    "href": "work/vetiver.html#compute-metrics",
    "title": "Publish and version your models with Vetiver",
    "section": "Compute metrics",
    "text": "Compute metrics\nlibrary(vetiver)\nlibrary(tidyverse)\ncars &lt;- read_csv(\"https://vetiver.rstudio.com/get-started/new-cars.csv\")\noriginal_cars &lt;- slice(cars, 1:14)\n\noriginal_metrics &lt;-\n    augment(v, new_data = original_cars) %&gt;%\n    vetiver_compute_metrics(date_obs, \"week\", mpg, .pred)\n\noriginal_metrics\nSpecify the metrics you want to have for monitoring your model over time."
  },
  {
    "objectID": "work/vetiver.html#pin-the-metrics",
    "href": "work/vetiver.html#pin-the-metrics",
    "title": "Publish and version your models with Vetiver",
    "section": "Pin the metrics",
    "text": "Pin the metrics\nCreate a new board:\nmodel_board &lt;- board_connect(auth = \"envvar\") # authenticates to Connect via environment variables\n\nmodel_board %&gt;% pin_write(original_metrics, \"lisa.anders/tree_metrics\")\nLater we will want to update it so we can look back at how it is changing over time:\n# dates overlap with existing metrics:\nnew_cars &lt;- slice(cars, -1:-7)\nnew_metrics &lt;-\n    augment(v, new_data = new_cars) %&gt;%\n    vetiver_compute_metrics(date_obs, \"week\", mpg, .pred)\n\nmodel_board %&gt;%\n    vetiver_pin_metrics(new_metrics, \"lisa.anders/tree_metrics\", overwrite = TRUE)"
  },
  {
    "objectID": "work/vetiver.html#visualize-performance-over-time",
    "href": "work/vetiver.html#visualize-performance-over-time",
    "title": "Publish and version your models with Vetiver",
    "section": "Visualize performance over time",
    "text": "Visualize performance over time\nlibrary(ggplot2)\nmonitoring_metrics &lt;- model_board %&gt;% pin_read(\"lisa.anders/tree_metrics\")\nvetiver_plot_metrics(monitoring_metrics) +\n    scale_size(range = c(2, 4))"
  },
  {
    "objectID": "work/validation.html",
    "href": "work/validation.html",
    "title": "Validation",
    "section": "",
    "text": "My colleagues recommendation while reading these resources:\n\nPut yourself in a mental model that you are at a certain company and you want to build a whole new IT system that’s addressing certain unmet needs. Due to the nature of the use case you need to use this CSV (Computer System Validation) approach. Your goal is to meet user requirements while keeping risks under control.\n\n\nAs an exercise, think about can you come up with some guiding principles on how to set up Posit Team to make it GXP compliant. For example, what happens if deploy a shiny app and what part of processes are happening there when it comes to GXP, CSV, and such. The icing on the cake is how can we change the Connect configuration to make it compliant."
  },
  {
    "objectID": "work/validation.html#frame-of-mind",
    "href": "work/validation.html#frame-of-mind",
    "title": "Validation",
    "section": "",
    "text": "My colleagues recommendation while reading these resources:\n\nPut yourself in a mental model that you are at a certain company and you want to build a whole new IT system that’s addressing certain unmet needs. Due to the nature of the use case you need to use this CSV (Computer System Validation) approach. Your goal is to meet user requirements while keeping risks under control.\n\n\nAs an exercise, think about can you come up with some guiding principles on how to set up Posit Team to make it GXP compliant. For example, what happens if deploy a shiny app and what part of processes are happening there when it comes to GXP, CSV, and such. The icing on the cake is how can we change the Connect configuration to make it compliant."
  },
  {
    "objectID": "work/validation.html#definitions",
    "href": "work/validation.html#definitions",
    "title": "Validation",
    "section": "Definitions",
    "text": "Definitions\nDQ Design Qualification - a design is assessed and documented to make sure it meets user requirements and regulatory standards (typically doesn’t apply to Posit software)\nIQ Installation Qualification - verifies that equipment and systems are installed correctly according to the manufacturers instructions\nOQ Operational Qualification - verifies that a system performs as required over time, typically requires implementation of control systems, alarms, and testing of functions under different conditions\nPQ Performance Qualification - verifies the system performance while operating under normal operating conditions, typically involves real production environments or batches and monitors quality outcomes\nCSV Computer System Validation is a form of software validation to ensure that a regulated computerized system is as safe, secure, and reliable as paper based records.\nURD/URS User Requirements Document or User Requirements Specification specifies what the user expects the software to be able to do.\nGXP A set of principles that govern the use of various systems and recommend the implementation of strict controls\nGMP Good Manufacturing Practices, guidelines that govern the manufacture and sale of food, beverages, cosmetics, pharmaceuticals, dietary supplements, and medical devices\nSCE Statistical Compute Environment - typically computerized systems where late-stage clinical development workloads run\nSOP Standard Operating Procedure is a set of repeatable steps and instructions that an organization can follow in order to achieve a common activity.\nProduction Day to day system for productive use. Change control for system-wide changes is in place.\nDevelopment Small replica of the production system. No change control. Used for IT internal testing of new features/changes.\nTest/Staging/QA Small replica of the production system. Fully change controlled. Used to allow for business testing before rolling out the change to Production."
  },
  {
    "objectID": "work/validation.html#resources",
    "href": "work/validation.html#resources",
    "title": "Validation",
    "section": "Resources",
    "text": "Resources\n\nStart here\nInternal to Posit resources:\n\nComputerized system validation - google this - Michael uploaded in the pharma task force github a summarized csv - https://github.com/rstudio/pharma-taskforce/blob/main/doc/Overview/CSV.md\nMichael had a post in the Life science and healthcare channel - posted pdf document - https://positpbc.slack.com/archives/CQ55EG3U7/p1742983108413489\nThis post from MichaelM is useful about validated environments: https://positpbc.slack.com/archives/C2P7MTJKE/p1746448154247119?thread_ts=1746118620.637929&cid=C2P7MTJKE\nAnother internal post about the limitations of renv - https://positpbc.slack.com/archives/C2P7MTJKE/p1746653854586919?thread_ts=1746118620.637929&cid=C2P7MTJKE\n\n\n\nAs a vendor - disclaimer\nWe as posit are not in the business of validating packages. We can talk about how to do it high level but its up to them to build such a process or point them at Atorus OpenVal (which doesn’t use Package Manager), or Jumping Rivers Litmus system.\n\n\nNow add some more documents\nPhil resources: (See Trevor chat)\nPosit pro products (still says Rstudio everything): https://posit.co/wp-content/uploads/2024/11/rstudio_compliance_validation.pdf?_gl=1*1nenc0g*_ga*MTI1ODM3MDExLjE3MzY2MTk0MTQ.*_ga_2C0WZ1JHG0*MTc0NjIwNDQyOC4zNjQuMC4xNzQ2MjA0NDM1LjAuMC4w\nR validation: https://www.r-project.org/doc/R-FDA.pdf\nTidyverse, tidymodels: https://posit.co/wp-content/uploads/2024/11/validation-tidy.pdf?_gl=1*1rog1io*_ga*MTI1ODM3MDExLjE3MzY2MTk0MTQ.*_ga_2C0WZ1JHG0*MTc0NjIwNDQyOC4zNjQuMC4xNzQ2MjA0NDI4LjAuMC4w\nShiny and rmarkdown:https://posit.co/wp-content/uploads/2024/11/validation-shiny-rmd.pdf?_gl=1*1nenc0g*_ga*MTI1ODM3MDExLjE3MzY2MTk0MTQ.*_ga_2C0WZ1JHG0*MTc0NjIwNDQyOC4zNjQuMC4xNzQ2MjA0NDM1LjAuMC4w\n\n\nFinally lets check out the stories on the Posit blog\nDeploying a Shiny App in R using clinical trial data with Posit Connect\nSecuring Workbench for Clinical Workflows, with JnJ\nJohnson & Johnson x Posit Live Event March 2025 Q&A\nRoche x Posit Live Event Sept 2024 Q&A\nRoche’s end-to-end R submission for a new drug application\nHow pharma is pioneering WebAssembly with webR & Shiny for FDA clinical trial submissions\n\n\nTools that will help make this easier\n\nPosit Package Manager\nR package litmus by Jumping Rivers and service\nR package oysteR by Sonatype Nexus for checking package vulnerabilities against the OSS index\nR package pak\nR package riskmetric\nAtorus OpenVal\nteal.logger for logging actions taken inside a shiny app"
  },
  {
    "objectID": "work/validation.html#take-aways",
    "href": "work/validation.html#take-aways",
    "title": "Validation",
    "section": "Take aways",
    "text": "Take aways\n\nWhat has worked in the past?\nThere has been one successfully submission of a shiny app to the FDA and it was stripped down so that there weren’t any interactive elements and it ran via WASM. While aditional monitoring and logging is really exciting, and I hope is something we can see in the future, at the moment it may be worth being grounded that what a validated output may look like will be more like a compendium of static documents that are navigated via a shiny app rather than a true interactive application.\n\n\nWhat are the guiding principles?\nThey key guiding principle is to understand what your use case is and where risks could be introduced.\nCommon risk areas:\n\nWas it installed correctly?\nWhen changes are made, IE upgrades to packages or software, how would any newly introduced risks be detected?\nIs the way users will use the system understood and can be turned into tests in order to validate the system?\nHow is data or code being handled so that things like patient data are handled securely?\n\nWhenever there is a change to the system it needs to through either a full validation or mini-validation\n\n\nDeveloping a SOP\n\nNew system is identified for CSV based on a risk assessment (collaboration between QA, Business and IT)\nURS (USer Requirement Specifications) are drafted (typically in the form of user stories) (business)\nDesign specifications are produced where technical design and identified business needs are aligned (IT)\nThe Validation plan is created that documents the overall solution and how the system will be validated (IT)\n\n\n\nManaging installations\nThe name of the game here is transparency, security, and reproducability. Whether using docker, singularity, k8s, slurm, a load balanced servers, or a single server it doesn’t really matter as long as all steps are followed and documented.\n\n\nUpgrades\nWhen upgrade, security risks with old OS, then need to understand where the risks are and have tests and re-validate to make sure same results before and after the upgrade. Qualification documentation will allow you to replicate the system the same as it was back then when you are asked years later to re-do some piece of the data analysis from a health authority. Your documentation needs to be in a state where you can replicate it. Money at that point is no object because the risk of losing a submissions is monumental.\n\n\nManaging packages\nSophisticated math libraries for linear algebra like BLAS then those things are so performance optimized that they use different code for diff OS’s. You could get different results.\nNeed a process that gives assurance that a package is ready for use - use atorus openval, preinstall packages and test them in their respective locations, or if you allow the users to allow packages then every user is responsible for testing, validating their stuff each and every time.\nStrong misconception - that technology can solve all the problems. People will always try to find loopholes, no matter how clever you configure it. That’s the important thing with business processes. If they follow the processes then they will be compliant, otherwise they will be out of compliance and there is nothing IT can do about it.\nDocker images are acting as a site library in a way - installing packages into a direct instance where deploying packages. Could have been an EC2 just the same. Using a site library. Pre-install all the needed packages and dependencies, run testing on those packages and document that.\ncurrent is tested-r package repositories that has meta data - intentionally called tested not validated because it is all automated but wouldn’t recommend all to do because some of the packages have a higher risk, if serious about risk mitigation you would want to a different level of testing for high risk packages than low risk packages. In a real world not possible to automate everything. A human being still needs to accept those.\nLitmus is 30+ different risk numbers weighted together\nTesting needs to be done every time you install the package with that given version. Any change needs to be documented. Need to do an impact analysis, will inform how many other things need to be tested again. Site library with 100 packages and install 1 more then need to asses how other packages will be impacted, do we need to upgrade the other packages, test and so on and so forth.\nWhen user installs would need to rerun test suite, produce suite of documentation, if allowed to install directly otherwise site library is a huge pro because do the tests and documentation once for all users.\nCode is a change to the system also, need to justify the changes too. Critical analyses sometimes also have two different teams that are segregated and do the same analysis independently to make sure risk of making wrong decision is mitigated - double programming.\nThis is leaning on the cautious side of things.\nNeed repository that you can rely on in 3-5 maybe 10 years, if trust local package manager more than cran mirrors then useful for that purpose. meta data is cosmetics.\n\nWhy not renv? Or just installing packages into user home directories?\nrenv is a very common and useful R package that helps users maintain project specific package environments. It is very similar in function to Python venv environments.\nrenv gives you a snapshot in time for your project for which packages you had and what their versions were. This is very useful and an important part of reproduceability.\nUnfortunately using renv on its own without other pieces for how packages are installaed can lead to breaking qualification requirements. Re-downloading packages would introduce risk. Every time a package is downloaded that constitutes a change in the system and needs to go through a validation process to ensure that no new risks have been introduced. If that is done once and periodically after, to install packages in a site library, that can be a manageable level of effort. But to do that for every user every time they install a package would be an unreasonable amount of effort.\nrenv is the answer for knowledge of what packges you used, some level of reproduceability, but if you think about what learned with the CSV stuff. Any package installation on the system constitutes a change, any change needs to be documented tested and approved.\n\n\n\n\nShiny application submissions to the FDA\nThe first every shiny submission was done in such a way the FDA was only accepting the app once the team had removed all the interactive elements so it was more like a clickable document in the end.\n\n\nWorkbench config settings\n\n\nConnect config settings\n\n\nPackage Manager config settings"
  },
  {
    "objectID": "work/validation.html#but-what-about-python",
    "href": "work/validation.html#but-what-about-python",
    "title": "Validation",
    "section": "But what about Python?",
    "text": "But what about Python?\n\npak -&gt; uv\nriskmetric -&gt; no equivalent today\nrenv -&gt; venv (note taken that this is useful for recording package versions, a risk if used to install packages)\nPackage Manager -&gt; Package Manager\n\nI’ve been eyeing:\n\npip-audit (uses the data from advisory database)\ngoogles osv-scanner (that uses the data from osv.dev)\n\nBut those are just very surface level tools that check if a vulnerability has been detected"
  },
  {
    "objectID": "work/shiny-apps-and-analytics.html",
    "href": "work/shiny-apps-and-analytics.html",
    "title": "Shiny apps and Analytics",
    "section": "",
    "text": "Adding instrumentation to your shiny apps can help you understand:\n\nUser interaction\nMost viewed dashboards\nWhat is being exported and by who\nTrends with access over time\nTime spent on interactions\n\nConnect (as a software) isn’t aggregating and making available user interaction information, though it is aggregating higher level app specific access auditing type information. However, advanced interaction instrumentation can be accomplished through adding tracking into the content itself using services like hotjar, google analytics, or saving into the log file or pushing into somewhere external.\n\nThere’s more on the Connect API here: https://solutions.posit.co/operations/connect-apis/\nGoogle analytics likely requires adding some JavaScript to your site which you can do with Shiny, https://shiny.posit.co/r/reference/shiny/latest/builder.html\nThis page in the Shiny docs give a walkthrough as well: https://shiny.posit.co/r/articles/build/google-analytics/\nTracking where within the Shiny app users are spending their time (which tab, etc) is interesting.\nMonitoring user behavior with hotjar (paid): https://www.r-bloggers.com/2022/06/r-shiny-hotjar-how-to-monitor-user-behavior-in-r-shiny-apps/\nHere’s an article that outlines more free options: https://www.appsilon.com/post/monitoring-r-shiny-user-adoption\nJavascript might need to be added to your site to support using google analytics: https://shiny.posit.co/r/reference/shiny/latest/builder.html\nVarious examples for tracking built in to Connect: https://solutions.rstudio.com/data-science-admin/tracking/ and https://github.com/sol-eng/connect-usage\nVideo showing how to use the Connect API: https://www.youtube.com/watch?v=0iljqY9j64U\nThis heatmap package for seeing where users click I think would be cool to explore: https://github.com/RinteRface/shinyHeatmap\n\nYou can also add debug messages throughtout your shiny app and then use that with parsing the logs to estimate user interactions.\n\nAdd debug messages with: https://github.com/johnmyleswhite/log4r\nLearn about your user with client data: https://shiny.rstudio.com/articles/client-data.html"
  },
  {
    "objectID": "work/securing-credentials.html",
    "href": "work/securing-credentials.html",
    "title": "Securing credentials",
    "section": "",
    "text": "When working with pulling data from secure databases or other sources a developer might find themselves in a situation of needing to provide very sensitive information, such as a password or a token, in order to access the data that is needed or to successfully deploy a project. Handling those secrets in way that doesn’t expose them in the code directly is critical and where using environmental variable’s for securing sensitive variables is strongly recommended.\nAdditionally there may be parameters that are often needed that can be accessed as a variable more easily rather than having to type in every time.\nFor both of these cases knowing how environment variables can be leveraged can be very rewarding and it is surprising how little effort it can to take to set up.\n\n\nWhen R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files:\n\nUser\nProject\n\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\n\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\n\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\n\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\n\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\n\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\n\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI.\nUse Connect to manage environment variables through the UI: https://docs.posit.co/connect/user/content-settings/index.html#content-vars\nHave your admin use a supervisor script to add Environment Variables automatically: https://docs.posit.co/connect/admin/process-management/index.html#example-environment-variables\nSecuring credentials solutions article: https://solutions.posit.co/connections/db/best-practices/managing-credentials/"
  },
  {
    "objectID": "work/securing-credentials.html#working-with-the-.renviron-file",
    "href": "work/securing-credentials.html#working-with-the-.renviron-file",
    "title": "Securing credentials",
    "section": "",
    "text": "When R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files:\n\nUser\nProject\n\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\n\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\n\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\n\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\n\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\n\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\n\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI.\nUse Connect to manage environment variables through the UI: https://docs.posit.co/connect/user/content-settings/index.html#content-vars\nHave your admin use a supervisor script to add Environment Variables automatically: https://docs.posit.co/connect/admin/process-management/index.html#example-environment-variables\nSecuring credentials solutions article: https://solutions.posit.co/connections/db/best-practices/managing-credentials/"
  },
  {
    "objectID": "work/saml-entraid-posit-team.html",
    "href": "work/saml-entraid-posit-team.html",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "To configure Microsoft Entra ID for SAML, both the software address and an additional path, if being used, must be known. If there is not an additional path being served from then omit that from the below instructions. In addition Azure requires a secure connection, and so a valid SSL certificate (cannot be self-signed) is needed.\n\n\nRelevant cheat sheet in docs: https://docs.posit.co/getting-started/networking.html\nGetting info from azure is on a push basis for SCIM provisioning, so we need connectivity open on port 443 via the express route or however so there is access. In order for authentication via SAML and Azure to work these ports need to be enabled:\n\n443 (for https)\n\nNetworking access is needed between the Connect and Workbench servers and the Azure service.\nConnect will also need web sockets enabled. Will be needed for each of the redirects (as detailed in the lower section).\n\n\n\nOn Rhel9:\nyum install -y oddjob oddjob-mkhomedir\n\n\n\nAzure restricts to only using https addresses, not http, for doing redirects and authentication via API. A SSL certificate must be installed on each server. This SSL cert must be recognizable by Azure, meaning that it can’t be an internal CA.\nDoes the cert need to be installed on the server, or does installing it for the prox work? It just needs the “front door” address to have HTTPS, if running with an App gateway, then you only need SSL on that load balancer/proxy.\n\n\nopenssl req -new -key $URL.key -out $URL.csr -config $URL.cnf\nopenssl genrsa -out $URL.key 2048\n#-----------------------------------------------------------------------------------------#\n# HTTPS Configuration Section\n#\n# The certificate key should be owned by rstudio and have perms: sudo chmod 600 /var/certs/rstudio.key\n# Unfortunately at this time, RStudio doesn't support a passphrase on the SSL cert.\nssl-enabled=1\nssl-certificate-key=/path/to/key/posit.key\nssl-certificate=/path/to/certificate/posit.crt\nssl-protocols=TLSv1.2 TLSv1.3\nopenssl x509 -noout -modulus -in /etc/rstudio/fullchain.cer | openssl md5\nopenssl rsa -noout -modulus -in /etc/rstudio/fullchain.key | openssl md5\n\n\n\nnano root.crt ← paste contents of root certificate \nsudo cp root.crt /etc/pki/ca-trust/source/anchors/\nsudo update-ca-trust\ntrust list | grep -i &lt;name&gt;\n[HTTPS]\n; https://docs.posit.co/connect/admin/appendix/configuration/#HTTPS\n; RStudio Connect will listen on this network address for HTTPS connections.\nListen = :443\n;Path to a PEM encoded TLS certificate file. If the certificate is signed by a certificate authority,\n;the certificate file should be the concatenation of the server's certificate followed by the CA's certificate\nCertificate =  /path/to/certificate/connect.crt\n;Path to a PEM encoded private key file corresponding to the certificate specified with\nKey = /path/to/key/connect.key\n; Force HTTPS\n; https://docs.posit.co/connect/admin/security/#guaranteeing-https\n;Permanent = True\nMinimumTLS = 1.2\n\n\n\n\nTwo separate applications inside Azure need to be created to support authentication on Connect and Workbench (different reply url assertion consumer service url’s).\n\nThrough the marketplace: Manage → enterprise apps → add a new one → search for rstudio\nRstudio-server-pro SAML authentication - is what you want for Workbench (refer to here)\nRstudio Connect SAML authentication - is what you want for Connect (refer to here)\n\nSee the Customize user provisioning attribute mappings tutorial in Microsoft’s documentation for more information on adding custom user attributes in Microsoft Entra ID.\n\n\nRefer to here\nFor the URL’s if there is an additional path that Workbench is being served from then it should be added in the &lt;path&gt; field, otherwise that can be omitted. For example, the URL would be https://&lt;RSW-SERVER&gt;/saml/metadataif there isn’t an additional path. \nIn the Identifier (Entity ID) section, click Add identifier and in the Enter an identifier text box, type a URL using the following pattern: https://&lt;RSW-SERVER&gt;/&lt;PATH&gt;/saml/metadata.\nIn the Reply URL (Assertion Consumer Server URL) section, click Add reply URL and type the URL using the following pattern: https://&lt;RSW-SERVER&gt;/&lt;PATH&gt;/saml/acs.\n\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/authenticating_users/saml_sso.html and https://docs.posit.co/ide/server-pro/authenticating_users/integrated_providers/azure_ad_saml.html\nEnable SAML auth:\n#/etc/rstudio/rserver.conf\nauth-saml=1\nComplete the SAML section in the config to match the user attributes from EntraID:\n#/etc/rstudio/rserver.conf\nauth-saml=1\nauth-saml-metadata-url=&lt;federation-metadata-URI&gt;\nauth-saml-sp-name-id-format=emailaddress\nauth-saml-sp-attribute-username=NameID\nauth-saml-sp-base-uri=&lt;RStudio-Server-URI&gt;\nauth-saml-idp-post-binding=1\nCreate the auth profile:\nsudo authselect create-profile pwb --base-on=minimal\nEdit the /etc/authselect/custom/pwb/nsswitch.conf file and add the pwb entry:\n#/etc/authselect/custom/pwb/nsswitch.conf\npasswd:     files {if \"with-altfiles\":altfiles }systemd pwb {exclude if \"with-custom-passwd\"}\ngroup:      files {if \"with-altfiles\":altfiles }systemd pwb {exclude if \"with-custom-group\"}\nshadow:     files pwb                                       {exclude if \"with-custom-shadow\"}\nEnable the profile:\nauthselect select custom/pwb --with-mkhomedir --force\nNote: At this time the docs aren’t accurate and say to run sudo authselect select pwb --with-mkhomedir instead of authselect select custom/pwb --with-mkhomedir --force\nMake sure the odd job service is active:\nsystemctl enable --now oddjobd.service\nPrevent NSCD from caching information:\nvi /etc/nscd.conf\nenable-cache passwd no\nenable-cache group no\nsudo systemctl restart nscd\nps -ef | grep nsc\n\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/user_provisioning/azure.html\nComplete the user provisioning section in the config to match the user attributes from EntraID (modify the user-provisioning-start-uidas needed, might not matter and can be left off) :\n#/etc/rstudio/rserver.conf\nuser-provisioning-enabled=1\nuser-provisioning-start-uid=2000\n\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/user_provisioning/managing_tokens.html\nFor SCIM provisioning and the token, I believe you will also need user provisioning turned on. You can do this with: \n#rserver.conf\nuser-provisioning-enabled=1\nCreate a token:\nsudo rstudio-server user-service generate-token \"My Token\"\nInput the token through the Azure profile.\nProvision a user by having a user log all the way in to Workbench.\nTest it from command line:\n/usr/lib/rstudio-server/bin/pamtester --verbose rstudio &lt;user&gt; authenticate acct_mgmt setcred open_session close_session\n\n\n\nRefer to here\n\n\n\nRelevant page in docs: https://docs.posit.co/connect/admin/authentication/saml-based/entra-id-saml/\n#; /etc/rstudio-connect/rstudio-connect.gcfg\n\n[Server]\nAddress = https://posit.company.com\n\n[Authentication]\nProvider = \"saml\"\n\n[SAML]\nIdPMetaDataURL = \"https://login.microsoftonline.com/{tenantid}/federationmetadata/2007-06/federationmetadata.xml?appid={appid}\"\nIdPAttributeProfile = azure\n; Enable this for a better user experience, unless\n; managing a large number of groups is a concern:\n;GroupsAutoProvision = true\n; When troubleshooting a SAML problem, more verbose logging\n; is produced by uncommenting the following line:\n;Logging = true\nEntraID limits group membership to 150. If a user is a member of more than 150 groups than their group list will be concatenated, potentially missing important ones that are needed inside Connect.\nRegister on first login can be disabled with:\n#; /etc/rstudio-connect/rstudio-connect.gcfg\n[SAML]\nRegisterOnFirstLogin = false\nIn that case, users need to be created via the Connect API: https://docs.posit.co/connect/api/#post-/v1/users\n#!/bin/bash\n\nAPI_KEY=\"your api key\"\n\nDATA='{\n  \"email\": \"john_doe@posit.co\",\n  \"first_name\": \"John\",\n  \"last_name\": \"Doe\",\n  \"password\": \"\",\n  \"unique_id\": \"string\",\n  \"user_must_set_password\": false,\n  \"user_role\": \"viewer\",\n  \"username\": \"john_doe\"\n}'\n\ncurl --silent --show-error -L --max-redirs 0 --fail \\\n    -X POST \\\n    -H \"Authorization: Key ${API_KEY}\" \\\n    --data-raw \"${DATA}\" \\\n    \"https://connect.example.com/__api__/v1/users\"\n\n\n\n\n\n\nsudo rstudio-server restart\n\n\n\nsudo systemctl restart rstudio-connect\n\n\n\n\nThis image shows the needed assertions on the left, and the assertions associated with a test user on the right. Test user is failing to log into Workbench due to missing assertions. Once the last name and email were added logins were successful.\nThe list of assertions expected by Connect are documented here:\n; /etc/rstudio-connect/rstudio-connect.gcfg\n[SAML]\nUniqueIdAttribute = NameID\nNameIDFormat = persistent\nUsernameAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\nFirstNameAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname\nLastNameAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname\nEmailAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\nGroupsAttribute = http://schemas.microsoft.com/ws/2008/06/identity/claims/groups\n\n\n\nIf no useful messages are showing in the Connect or Workbench logs, it could be an issue with the auth provider configuration, URL’s, or network access. Turning on debug logging may help with errors outside of the auth provider configuration.\n\n\n\nExample endpoint for app: whole host name including proxy information scim v2, for example https://workbench-hostname&gt;/scim/v2. Example in the case they are running with a proxy at root wb: https:////scim/v2\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/access_and_security/running_with_a_proxy.html and https://docs.posit.co/ide/server-pro/authenticating_users/saml_sso.html#proxy-considerations\nThe reverse proxy should be correctly forwarding the connection and the certificate on the proxy is publicly trusted. Workbench needs to know it’s running on HTTPS however. Generally that’s done by the proxy forwarding the X-Forward-Proto header as https.\nWe have an example here showing setting the X-Forward-Proto headers, an example without SSL is described as “If the Nginx proxy is using SSL and Workbench has ssl-enabled=0” : https://docs.posit.co/ide/server-pro/access_and_security/running_with_a_proxy.html#nginx-configuration  \nThe documentation default is: \n#/etc/rstudio/rserver.conf\nwww-root-path=/rstudio\nHowever, in the proxy configuration you are actually being hosted on /wb not rstudio so I believe it should look like: \n#/etc/rstudio/rserver.conf\nwww-root-path=/wb\nI believe in your URL to your tenant that it will need the /wb prefix as well, IE instead of &lt;https:///scim/v2&gt; it should be &lt;https:////scim/v2&gt;\nThe path prefix will need to be propagated throughout, what comes to mind is also the ACS URL configuration in the Enterprise App which is currently pointing to &lt;https:///saml/acs&gt;but should be &lt;https:////saml/acs&gt;\n\n\n\nRelevant page in docs: https://docs.posit.co/connect/admin/proxy/ and https://docs.posit.co/connect/admin/authentication/saml-based/entra-id-saml/index.html#authenticating-with-saml-using-multiple-network-aliases\nConnect needs web sockets to be enabled.\n# Support proxying of web-socket connections\nmap $http_upgrade $connection_upgrade {\n  default upgrade;\n  ''      close;\n}\n\n\n\n\nWhen users leave the organization, or removed from the application, than their account will be locked. This is the advantage of configuring it with SCIM / integrating directly with the authentication provider. Any home directories, deployed content, etc will remain on the server and can be cleaned up using any internal policies."
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#networking-requirements",
    "href": "work/saml-entraid-posit-team.html#networking-requirements",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "Relevant cheat sheet in docs: https://docs.posit.co/getting-started/networking.html\nGetting info from azure is on a push basis for SCIM provisioning, so we need connectivity open on port 443 via the express route or however so there is access. In order for authentication via SAML and Azure to work these ports need to be enabled:\n\n443 (for https)\n\nNetworking access is needed between the Connect and Workbench servers and the Azure service.\nConnect will also need web sockets enabled. Will be needed for each of the redirects (as detailed in the lower section)."
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#system-dependencies",
    "href": "work/saml-entraid-posit-team.html#system-dependencies",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "On Rhel9:\nyum install -y oddjob oddjob-mkhomedir"
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#ssl-requirements",
    "href": "work/saml-entraid-posit-team.html#ssl-requirements",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "Azure restricts to only using https addresses, not http, for doing redirects and authentication via API. A SSL certificate must be installed on each server. This SSL cert must be recognizable by Azure, meaning that it can’t be an internal CA.\nDoes the cert need to be installed on the server, or does installing it for the prox work? It just needs the “front door” address to have HTTPS, if running with an App gateway, then you only need SSL on that load balancer/proxy.\n\n\nopenssl req -new -key $URL.key -out $URL.csr -config $URL.cnf\nopenssl genrsa -out $URL.key 2048\n#-----------------------------------------------------------------------------------------#\n# HTTPS Configuration Section\n#\n# The certificate key should be owned by rstudio and have perms: sudo chmod 600 /var/certs/rstudio.key\n# Unfortunately at this time, RStudio doesn't support a passphrase on the SSL cert.\nssl-enabled=1\nssl-certificate-key=/path/to/key/posit.key\nssl-certificate=/path/to/certificate/posit.crt\nssl-protocols=TLSv1.2 TLSv1.3\nopenssl x509 -noout -modulus -in /etc/rstudio/fullchain.cer | openssl md5\nopenssl rsa -noout -modulus -in /etc/rstudio/fullchain.key | openssl md5\n\n\n\nnano root.crt ← paste contents of root certificate \nsudo cp root.crt /etc/pki/ca-trust/source/anchors/\nsudo update-ca-trust\ntrust list | grep -i &lt;name&gt;\n[HTTPS]\n; https://docs.posit.co/connect/admin/appendix/configuration/#HTTPS\n; RStudio Connect will listen on this network address for HTTPS connections.\nListen = :443\n;Path to a PEM encoded TLS certificate file. If the certificate is signed by a certificate authority,\n;the certificate file should be the concatenation of the server's certificate followed by the CA's certificate\nCertificate =  /path/to/certificate/connect.crt\n;Path to a PEM encoded private key file corresponding to the certificate specified with\nKey = /path/to/key/connect.key\n; Force HTTPS\n; https://docs.posit.co/connect/admin/security/#guaranteeing-https\n;Permanent = True\nMinimumTLS = 1.2"
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#create-the-azure-applications",
    "href": "work/saml-entraid-posit-team.html#create-the-azure-applications",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "Two separate applications inside Azure need to be created to support authentication on Connect and Workbench (different reply url assertion consumer service url’s).\n\nThrough the marketplace: Manage → enterprise apps → add a new one → search for rstudio\nRstudio-server-pro SAML authentication - is what you want for Workbench (refer to here)\nRstudio Connect SAML authentication - is what you want for Connect (refer to here)\n\nSee the Customize user provisioning attribute mappings tutorial in Microsoft’s documentation for more information on adding custom user attributes in Microsoft Entra ID.\n\n\nRefer to here\nFor the URL’s if there is an additional path that Workbench is being served from then it should be added in the &lt;path&gt; field, otherwise that can be omitted. For example, the URL would be https://&lt;RSW-SERVER&gt;/saml/metadataif there isn’t an additional path. \nIn the Identifier (Entity ID) section, click Add identifier and in the Enter an identifier text box, type a URL using the following pattern: https://&lt;RSW-SERVER&gt;/&lt;PATH&gt;/saml/metadata.\nIn the Reply URL (Assertion Consumer Server URL) section, click Add reply URL and type the URL using the following pattern: https://&lt;RSW-SERVER&gt;/&lt;PATH&gt;/saml/acs.\n\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/authenticating_users/saml_sso.html and https://docs.posit.co/ide/server-pro/authenticating_users/integrated_providers/azure_ad_saml.html\nEnable SAML auth:\n#/etc/rstudio/rserver.conf\nauth-saml=1\nComplete the SAML section in the config to match the user attributes from EntraID:\n#/etc/rstudio/rserver.conf\nauth-saml=1\nauth-saml-metadata-url=&lt;federation-metadata-URI&gt;\nauth-saml-sp-name-id-format=emailaddress\nauth-saml-sp-attribute-username=NameID\nauth-saml-sp-base-uri=&lt;RStudio-Server-URI&gt;\nauth-saml-idp-post-binding=1\nCreate the auth profile:\nsudo authselect create-profile pwb --base-on=minimal\nEdit the /etc/authselect/custom/pwb/nsswitch.conf file and add the pwb entry:\n#/etc/authselect/custom/pwb/nsswitch.conf\npasswd:     files {if \"with-altfiles\":altfiles }systemd pwb {exclude if \"with-custom-passwd\"}\ngroup:      files {if \"with-altfiles\":altfiles }systemd pwb {exclude if \"with-custom-group\"}\nshadow:     files pwb                                       {exclude if \"with-custom-shadow\"}\nEnable the profile:\nauthselect select custom/pwb --with-mkhomedir --force\nNote: At this time the docs aren’t accurate and say to run sudo authselect select pwb --with-mkhomedir instead of authselect select custom/pwb --with-mkhomedir --force\nMake sure the odd job service is active:\nsystemctl enable --now oddjobd.service\nPrevent NSCD from caching information:\nvi /etc/nscd.conf\nenable-cache passwd no\nenable-cache group no\nsudo systemctl restart nscd\nps -ef | grep nsc\n\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/user_provisioning/azure.html\nComplete the user provisioning section in the config to match the user attributes from EntraID (modify the user-provisioning-start-uidas needed, might not matter and can be left off) :\n#/etc/rstudio/rserver.conf\nuser-provisioning-enabled=1\nuser-provisioning-start-uid=2000\n\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/user_provisioning/managing_tokens.html\nFor SCIM provisioning and the token, I believe you will also need user provisioning turned on. You can do this with: \n#rserver.conf\nuser-provisioning-enabled=1\nCreate a token:\nsudo rstudio-server user-service generate-token \"My Token\"\nInput the token through the Azure profile.\nProvision a user by having a user log all the way in to Workbench.\nTest it from command line:\n/usr/lib/rstudio-server/bin/pamtester --verbose rstudio &lt;user&gt; authenticate acct_mgmt setcred open_session close_session\n\n\n\nRefer to here\n\n\n\nRelevant page in docs: https://docs.posit.co/connect/admin/authentication/saml-based/entra-id-saml/\n#; /etc/rstudio-connect/rstudio-connect.gcfg\n\n[Server]\nAddress = https://posit.company.com\n\n[Authentication]\nProvider = \"saml\"\n\n[SAML]\nIdPMetaDataURL = \"https://login.microsoftonline.com/{tenantid}/federationmetadata/2007-06/federationmetadata.xml?appid={appid}\"\nIdPAttributeProfile = azure\n; Enable this for a better user experience, unless\n; managing a large number of groups is a concern:\n;GroupsAutoProvision = true\n; When troubleshooting a SAML problem, more verbose logging\n; is produced by uncommenting the following line:\n;Logging = true\nEntraID limits group membership to 150. If a user is a member of more than 150 groups than their group list will be concatenated, potentially missing important ones that are needed inside Connect.\nRegister on first login can be disabled with:\n#; /etc/rstudio-connect/rstudio-connect.gcfg\n[SAML]\nRegisterOnFirstLogin = false\nIn that case, users need to be created via the Connect API: https://docs.posit.co/connect/api/#post-/v1/users\n#!/bin/bash\n\nAPI_KEY=\"your api key\"\n\nDATA='{\n  \"email\": \"john_doe@posit.co\",\n  \"first_name\": \"John\",\n  \"last_name\": \"Doe\",\n  \"password\": \"\",\n  \"unique_id\": \"string\",\n  \"user_must_set_password\": false,\n  \"user_role\": \"viewer\",\n  \"username\": \"john_doe\"\n}'\n\ncurl --silent --show-error -L --max-redirs 0 --fail \\\n    -X POST \\\n    -H \"Authorization: Key ${API_KEY}\" \\\n    --data-raw \"${DATA}\" \\\n    \"https://connect.example.com/__api__/v1/users\""
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#restart",
    "href": "work/saml-entraid-posit-team.html#restart",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "sudo rstudio-server restart\n\n\n\nsudo systemctl restart rstudio-connect"
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#assertions",
    "href": "work/saml-entraid-posit-team.html#assertions",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "This image shows the needed assertions on the left, and the assertions associated with a test user on the right. Test user is failing to log into Workbench due to missing assertions. Once the last name and email were added logins were successful.\nThe list of assertions expected by Connect are documented here:\n; /etc/rstudio-connect/rstudio-connect.gcfg\n[SAML]\nUniqueIdAttribute = NameID\nNameIDFormat = persistent\nUsernameAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\nFirstNameAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname\nLastNameAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname\nEmailAttribute = http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\nGroupsAttribute = http://schemas.microsoft.com/ws/2008/06/identity/claims/groups"
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#troubleshooting",
    "href": "work/saml-entraid-posit-team.html#troubleshooting",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "If no useful messages are showing in the Connect or Workbench logs, it could be an issue with the auth provider configuration, URL’s, or network access. Turning on debug logging may help with errors outside of the auth provider configuration."
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#but-what-about-proxies",
    "href": "work/saml-entraid-posit-team.html#but-what-about-proxies",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "Example endpoint for app: whole host name including proxy information scim v2, for example https://workbench-hostname&gt;/scim/v2. Example in the case they are running with a proxy at root wb: https:////scim/v2\n\n\nRelevant page in docs: https://docs.posit.co/ide/server-pro/access_and_security/running_with_a_proxy.html and https://docs.posit.co/ide/server-pro/authenticating_users/saml_sso.html#proxy-considerations\nThe reverse proxy should be correctly forwarding the connection and the certificate on the proxy is publicly trusted. Workbench needs to know it’s running on HTTPS however. Generally that’s done by the proxy forwarding the X-Forward-Proto header as https.\nWe have an example here showing setting the X-Forward-Proto headers, an example without SSL is described as “If the Nginx proxy is using SSL and Workbench has ssl-enabled=0” : https://docs.posit.co/ide/server-pro/access_and_security/running_with_a_proxy.html#nginx-configuration  \nThe documentation default is: \n#/etc/rstudio/rserver.conf\nwww-root-path=/rstudio\nHowever, in the proxy configuration you are actually being hosted on /wb not rstudio so I believe it should look like: \n#/etc/rstudio/rserver.conf\nwww-root-path=/wb\nI believe in your URL to your tenant that it will need the /wb prefix as well, IE instead of &lt;https:///scim/v2&gt; it should be &lt;https:////scim/v2&gt;\nThe path prefix will need to be propagated throughout, what comes to mind is also the ACS URL configuration in the Enterprise App which is currently pointing to &lt;https:///saml/acs&gt;but should be &lt;https:////saml/acs&gt;\n\n\n\nRelevant page in docs: https://docs.posit.co/connect/admin/proxy/ and https://docs.posit.co/connect/admin/authentication/saml-based/entra-id-saml/index.html#authenticating-with-saml-using-multiple-network-aliases\nConnect needs web sockets to be enabled.\n# Support proxying of web-socket connections\nmap $http_upgrade $connection_upgrade {\n  default upgrade;\n  ''      close;\n}"
  },
  {
    "objectID": "work/saml-entraid-posit-team.html#what-happens-when-users-leave-the-organization",
    "href": "work/saml-entraid-posit-team.html#what-happens-when-users-leave-the-organization",
    "title": "Using SAML and SCIM with EntraID with Posit Team",
    "section": "",
    "text": "When users leave the organization, or removed from the application, than their account will be locked. This is the advantage of configuring it with SCIM / integrating directly with the authentication provider. Any home directories, deployed content, etc will remain on the server and can be cleaned up using any internal policies."
  },
  {
    "objectID": "work/renv-environments.html",
    "href": "work/renv-environments.html",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "",
    "text": "#| echo: false\n#| include: false\n\nlibrary(renv)\nThis vignette is an overview of environment management in R and a comprehensive summary of the different options that can be configured to support different workflows. Environment management in R is intentionally complex, so figuring out where to even start when debugging can be a challenge. This vignette also goes into specific scenarios that might come up with environment management and recommendations."
  },
  {
    "objectID": "work/renv-environments.html#at-a-glance",
    "href": "work/renv-environments.html#at-a-glance",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "At a glance",
    "text": "At a glance\nOverview of the R environment:\n\n\n\n\n\ngraph LR\n    \n    subgraph ENV[Working R Environment]\n    \n    subgraph CONFIG[Config]\n    \n      subgraph LOCAL[Local R Config]\n      RENVIRON[.Renviron]\n      RPROFILE[.Rprofile]\n      end\n    \n      subgraph SERVER[Server R Config]\n      SRENVIRON[Renviron.site&lt;br/&gt;etc/R.home/Renviron.site]\n      SRRPROFILE[Rprofile.site&lt;/br&gt;etc/Rprofile.site]\n      \n        subgraph W[Posit Workbench]\n        REPOS[\"repos.conf\"]\n        RSESSION[\"rsession.conf\"] \n        end\n    \n      end\n      \n      LOCAL-- User settings &lt;br/&gt;override&lt;br/&gt;global settings --&gt; SERVER\n      \n      subgraph RENVCONFIG[Renv Config]\n      RENVPROJECT[Project Settings&lt;br/&gt;renv/settings.json]\n      \n        subgraph RENVUSER[Config: User Level Settings]\n        RENVUR[\"User Renviron&lt;br/&gt;~/.Renviron\"]\n        RENVRI[\"R installation&lt;br/&gt;etc/Rprofile.site\"]\n        RENVP[\"Project&lt;br/&gt;.Rprofile\"]\n        end\n      end      \n      \n    end\n    \n    subgraph LIBRARY[Package Library Path]\n\n      USERLIBRARY[\"User&lt;br/&gt;R_HOME/library&lt;br/&gt;~/R\"]\n\n      SITELIBRARY[Site&lt;br/&gt;R_HOME/site-library]\n      \n      subgraph RENV[Renv]\n      direction TB\n      CACHE[\"Cache&lt;br/&gt;~/.cache/R/renv/\"]\n      PROJECTCACHE[\"Project Cache&lt;br/&gt;~/renv/library/\"]\n      CACHE-- Unless isolated, symlink --&gt; PROJECTCACHE; \n      SHAREDCACHE[Cross-User Shared Cache]\n      end\n\n    end  \n    \n    LIBRARY --&gt; CONFIG\n    CONFIG --&gt; LIBRARY\n    \n    end\n    \n    subgraph REPOSITORY[Package Repository Source]\n      direction TB\n    \n      subgraph PPM[Posit Package Manager]\n      RE[Package Binaries]\n      RP[Package Sources]\n      end\n    \n      CRAN[CRAN/Pypi/BioConductor/etc]\n    \n      CRAN -- Posit sync service --&gt; PPM;\n\n    end\n    \n    UA[User-Agent request header]-- Binary requested&lt;br/&gt;Details: OS, R version --&gt;PPM\n    \n    UA --&gt; ENV"
  },
  {
    "objectID": "work/renv-environments.html#environment-management-strategies",
    "href": "work/renv-environments.html#environment-management-strategies",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Environment Management strategies",
    "text": "Environment Management strategies\nThere are severeal common environment management strategies. Some strategies can be more prone to pain and challenges later than others. Thinking about the appropriate strategy for your organization in advance can save you from a lot of hurt later.\n\n\n\nalt text\n\n\nImage: https://solutions.posit.co/envs-pkgs/environments/reproduce/reproducibility-strategies-and-danger-zones.png\n\n\n\nSnapshot and Restore\nShared Baseline\nValidated\n\n\n\n\nAll developers are responsible for their own environment management, and enabled for making their enviornments reproduceable through the use of renv’s snapshot() capability. Users can freely access and install packages while following a package-centric workflow. Users are responsible for recording their dependencies for their projects.\nAll developers in the organization are pointed to a snapshot of available packages frozen to a particular date when the managing team had intentionally tested and made them available. On some cadence, let’s say quarterly, the managing team goes through, performs testing again, and provides a new updated snapshot that is available for developers to switch to. There are a lot of advantages in switching with new features, resolved bugs, etc.\nSimilar to the shared baseline stratgey the difference is that changes to the package environment go through an approval and auditing process, and access to packages is strictly enforced."
  },
  {
    "objectID": "work/renv-environments.html#understanding-rs-startup-behavior",
    "href": "work/renv-environments.html#understanding-rs-startup-behavior",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Understanding R’s startup behavior",
    "text": "Understanding R’s startup behavior\nR has a lot of flexibility for different workflows, which is a great thing. However, it also means that the answer to trying to change specific pieces of that customized behavior can have complex answers that depend on example what has been implemented in your environment.\nThis diagram posted by Thomas Lin Pedersen on X showing the R startup flowchart went viral, and for good reason:\n\n\n\nR Startup diagram by Thomas Lin Pedersen on X\n\n\nPosit provides precompiled R binaries for anyone to use, free of charge. The public respository can be visited to understand how they are compiled."
  },
  {
    "objectID": "work/renv-environments.html#where-packages-come-from",
    "href": "work/renv-environments.html#where-packages-come-from",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Where packages come from",
    "text": "Where packages come from\nPackages can come from a couple places, a tarball, version control location, but most commonly is the URL of the repository that the package will be installed from. The package source can be set by assigning an environment variable with the desired location. More than one repository can be specified, for example with:\nrepos &lt;- c(CRAN = \"https://cloud.r-project.org\", WORK = \"https://work.example.org\")\noptions(repos = repos)\nSetting it this way would be a “one off” that would change the “package repository” for the current session. In order to persist the change of repository location, and other settings, various configurations can be applied.\nTypically “package repository”, among developers, is used to refer to R and Python package repositories (not to be confused with linux package repositories, etc). Most R and Python package managers serve only R and Python packages, and don’t handle additional management of system dependencies or packages, which would be risky in a shared server system where conflicts could come up.\nThe most famous R and Python package repositories are:\n\nCRAN - hosting public packages, checking, distributing, and archiving R packages for various platforms\nBioConductor - hosting public packages, checking, distributing, and archiving R packages for various platforms\nPyPi - hosting public packages, checking, distributing, and archiving Python packages for various platforms\n\nPosit Package Manager can be deployed within your organization, completely air-gapped, or with a sync service to Posit, to receive package sources and binaries.\n\nPosit Package Manager - hosting public packages, hosting internal packages, checking, distributing, blocking vulnerabilities, and archiving R and Python packages for various platforms"
  },
  {
    "objectID": "work/renv-environments.html#server-vs-individual-environments",
    "href": "work/renv-environments.html#server-vs-individual-environments",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Server vs individual environments",
    "text": "Server vs individual environments\nDevelopers can work locally on their local machines, in a cloud environment, or using a shared server environment (for example, by using Posit Workbench).\nHaving multiple developers working on a centralized server using Posit Workbench has a couple primary advantages:\n\nBetter IT oversight and security with encrypted traffic and restricted IP addresses\nAdditional configuration options and settings\nAuditing and logging\nLess time spent on software installation and management\nAccess to larger compute resources\nOptions for standardizing settings across all users\n\nWhen sharing a server environment users will sign in separately and work will live in separate user home directories. Workbench can act as an auth client to different data sources. However, the shared system dependencies will need to be carefully managed to support the different workflows that the users are doing."
  },
  {
    "objectID": "work/renv-environments.html#the-renv-package",
    "href": "work/renv-environments.html#the-renv-package",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "The renv package",
    "text": "The renv package\nRenv is an open source R package that allows users to better manage their package environments.\nEver had your code mysteriously stop working or start producing different results after upgrading packages, and had to spend hours debugging to find which package was the culprit? Ever tried to collaborate on code just to get stuck on trying to decipher various package dependencies?\nrenv helps you track and control package changes - making it easy to revert back if you need to. It works with your current methods of installing packages (install.packages()). It comes with a great degree of flexibility and supports a wide range of user workflows.\nRenv assumes:\n\nUsers are familiar with a version control system, like git\nUsers are following a project-centric methodology where the goal is to simultaneously work on different projects with different package environment needs\n\n\n\n\nThe Renv workflow\n\n\nThere is an excellent video by David Aja discussing why he started using renv at the 2022 RStudio Conference here: https://www.rstudio.com/conference/2022/talks/you-should-use-renv/\nUsefully, renv doesn’t have system requirements.\n\nThe lock file\nThe renv lock file is what is generated that allows the environment to be recreated on another system. It might look something like this:\n\n\nClick here to expand an example renv lock file\n\n{\n  \"R\": {\n    \"Version\": \"4.3.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://p3m.dev/cran/latest\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"MASS\": {\n      \"Package\": \"MASS\",\n      \"Version\": \"7.3-60\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"grDevices\",\n        \"graphics\",\n        \"methods\",\n        \"stats\",\n        \"utils\"\n      ],\n      \"Hash\": \"a56a6365b3fa73293ea8d084be0d9bb0\"\n    },\n    \"Matrix\": {\n      \"Package\": \"Matrix\",\n      \"Version\": \"1.6-4\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Requirements\": [\n        \"R\",\n        \"grDevices\",\n        \"graphics\",\n        \"grid\",\n        \"lattice\",\n        \"methods\",\n        \"stats\",\n        \"utils\"\n      ],\n      \"Hash\": \"d9c655b30a2edc6bb2244c1d1e8d549d\"\n    },\n    \"yaml\": {\n      \"Package\": \"yaml\",\n      \"Version\": \"2.3.7\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"0d0056cc5383fbc240ccd0cb584bf436\"\n    }\n  }\n}\n\nIt’s in a json format. There are two main sections:\n\nHeader : This is where the R version is declared as well as package sources (if declared)\nPackages : This is where the specific package versions are specified, as well as various metadata\n\nFor an overview on package sources, see the Package Sources vignette.\nThe package source can be set for three different scenarios:\n\nRemoteType - packages installed by devtools, remotes, and pak\nRepository - packages installed from a package repository; CRAN, Posit Package Manager, etc\nbiocViews - packages installed from BioConductor repositories\n\nLet’s understand how the Repository is set. Notice how under each package the repository is declared like this:\nRepository: &lt;a name&gt;,\nThe Repository: &lt;a name&gt; field is used to denote the repository that the package was originally installed from. Most commonly it might like look:\n\nRepository: CRAN - This indicates that the package was installed from a repository call CRAN, likely a CRAN mirror\nRepository: RSPM - This indicates that the package was installed from Posit Package Manager, regardless of whether it was a binary or source package\n\nThere is a fail over order for determining the correct URL:\n\n\n\n\n\ngraph TD;\n    A(Assign repository URL) --&gt;lock; \n    \n    subgraph lock[renv.lock file]\n    B[Repository name in package definition]\n    c[Repository URL in header]\n    end\n    \n    lock -- Repository name in header --&gt;D;\n    D[Select matching URL] --&gt;END;\n    lock -- Repository name not in header --&gt;E;\n    \n    E{Check env for first repository listed &lt;br&gt; for required package version} -- package exists --&gt;F;\n    F[Select first repository URL] --&gt;END; \n    E -- package does not exist --&gt;G;\n\n    G{Check env for .. repository listed &lt;br&gt; for required package version} -- package exists --&gt;H;\n    H[Select .. repository URL] --&gt;END; \n    G -- package does not exist --&gt;I;\n    \n    I{Check env for last repository listed &lt;br&gt; for required package version} -- package exists --&gt;J;\n    J[Select last repository URL] --&gt;END; \n    I -- package does not exist --&gt;K;\n    \n    K{Check the cellar} -- package exists --&gt;L;\n    L[Select cellar] --&gt;END; \n    K -- package does not exist --&gt;M;    \n    \n    M[Package does not exist, unable to restore]\n    \n    END(End)\n\n\n\n\n\n\nIn words, for a package repository declaration of Repository: RSPM, if there happens to be a repository called RSPM in the repository list, then that repository will be preferred when restoring the package; otherwise, renv will check each repository from first to last for the required version of each package. The renv package cellar is meant to help with packages that aren’t available or accessible for installation. The cellar can be set to point at tarball locations for these tricky packages as an ultimate fail safe."
  },
  {
    "objectID": "work/renv-environments.html#the-pak-package",
    "href": "work/renv-environments.html#the-pak-package",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "The pak package",
    "text": "The pak package\nPak is a useful R package that can help with package installation and dependency look up.\nIf an error is encountered, we may need to enable the package pak to work with renv (or be patient and wait a couple minutes after installing pak). There is a useful git issue discussing this here.\nRenv can be told to use pak for package installation with: RENV_CONFIG_PAK_ENABLED = TRUE\nFor example temporarily with: Sys.setenv(\"RENV_CONFIG_PAK_ENABLED\" = TRUE))\nCheck that it set with: Sys.getenv('RENV_CONFIG_PAK_ENABLED')"
  },
  {
    "objectID": "work/renv-environments.html#package-installation",
    "href": "work/renv-environments.html#package-installation",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Package installation",
    "text": "Package installation\nPackages are installed into a package library, a directory that exists somewhere on disk.\nPackages are associated with that the OS, the particular version of R being used, and if using renv, with that particular project directory. The current library path(s) can be found with: .libPaths(). When packages are installed they will install to a sub folder that is specific to the combination of both of those.\n\nThe default library location\nThe default R installation will install packages into the users home directory, by default located at R_HOME/library. For example, on Windows:\n\\-- C:/Users/LisaAnders/AppData/Local/R\n    \\-- win-library\n        \\-- 4.3\n            \\-- ..packages\n\\-- C:/Program Files/R\n    \\-- R-4.3.1\n        \\-- library\n            \\-- ..packages\nLearn more about managing libraries in base R.\n\n\nShared site library location\nA shared site library can be set it up that will make packages from a global directory available to all users on the system, without the need for them to go through the installation steps. Through configuring Workbench, default repository locations can be set, an alternative directory can be set for use for package installation instead of user home directories, and user package installations can be disabled.\nA default site library can be used, at R_HOME/site-library (in this case /opt/R/3.4.4/lib/R/library), or a site library can be set up by setting .Library.site in R_HOME/etc/Rprofile.site / {$R_HOME}/etc/Rprofile.site. Multiple library locations can be set up to be used.\nWhen using a shared library, user options to change repository settings and package installation can be disabled if desired (typically as part of a validated environment management workflow). In this case, all users are accessing packages from that global site library and packages are added / updated by going through an approvals process with an admin ultimately running the commands that make the change.\nA site library can also be set up that allows users to access both the globally installed packages as well as install packages into the user directory. This is often “the best of both worlds”. New users are able to hit the ground running quickly, and advanced users have control over packages and package versions for their projects.\n\n\nRenv library location\nPackages installed with renv, depending on some configuration options, will use two locations:\n\nUser’s cache - ~/.cache/R/renv/\nProject cache - ~/renv/library/\n\nBy default, the project cache will symlink to the users cache in order to preserve space. Projects can be isolated in order to have the packages copied into the project library so that the project is completely independent of the broader renv cache.\nThe folder structure (note that it is specific to the possible OS’s, and the possible R versions and this is just an example) is:\n~/.cache/R/renv/\n+-- projects \n+-- index\n\\-- binary\n    \\-- linux-centos-7\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n        \\-- R-4.4\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n    \\-- linux-rocky-8.9\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n\\-- source\n    \\-- repository\n        \\-- ..packages\n~/renv/\n+-- activate.R\n+-- settings.json\n+-- staging\n\\-- library\n    \\-- linux-centos-7\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n        \\-- R-4.4\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n    \\-- linux-rocky-8.9\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n\\-- source\n    \\-- repository\n        \\-- ..packages"
  },
  {
    "objectID": "work/renv-environments.html#local-r-config-files",
    "href": "work/renv-environments.html#local-r-config-files",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Local R config files",
    "text": "Local R config files\nThese two configuration files, that may or may not be set, are the moste common for changing the behavior as relates to setting the repository for package installations:\n\n.Renviron : The user R environ file contains all environment variables, often including renv settings, etc (typically located at ~/.Renviron)\n.Rprofile : The user R profile file contains various settings and configuration properties (typically located at ~/.Rprofile)\n\nThe easiest way to access either of this files is with the usethis package.\nlibrary(usethis)\nusethis::edit_r_environ() \nusethis::edit_r_profile()\nThese startup files can be disabled."
  },
  {
    "objectID": "work/renv-environments.html#shared-server-r-config-files",
    "href": "work/renv-environments.html#shared-server-r-config-files",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Shared server R config files",
    "text": "Shared server R config files\nInstead of setting individually with .Renviron and .Rprofile, the same parameters can be set at the server and R installation level. When set, any configuration will be active for any R sessions launched on that server.\n\nRprofile.site : The RProfile.site file is typically located at etc/Rprofile.site\nRenviron.site : The Renviron.site file is specific to the R installation, typically located at file.path(R.home(\"etc\"), \"Renviron.site\").\n\nFor example, this code can be used to maintain the repository configuration across R sessions by adding to the individual users .Rprofile file. It can be maintained across all users on the server by adding to the Rprofile.site file.\nlocal({\n  repos &lt;- c(PackageManager = \"https://packagemanager.posit.co/cran/__linux__/centos7/latest\")\n  repos[\"LocalPackages\"] &lt;- \"https://packagemanager.posit.co/local/__linux__/centos7/latest\"\n  # add the new repositories first, but keep the existing ones\n  options(repos = c(repos, getOption(\"repos\")))\n})\ngetOption(\"repos\")\nUsers can override the global settings in these files Rprofile.site and Renviron.site with their individual .Rprofile files."
  },
  {
    "objectID": "work/renv-environments.html#workbench-files-for-rstudio-pro-sessions",
    "href": "work/renv-environments.html#workbench-files-for-rstudio-pro-sessions",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Workbench files for RStudio Pro sessions",
    "text": "Workbench files for RStudio Pro sessions\nSimilarly, there are configuration files used in Workbench that can set repository preference for package installations:\n\n/etc/rstudio/repos.conf\n/etc/rstudio/rsession.conf\n\nWhen using a shared library, user options to change repository settings and package installation can be disabled if desired:\n# /etc/rstudio/rsession.conf\nallow-r-cran-repos-edit=0\nallow-package-installation=0"
  },
  {
    "objectID": "work/renv-environments.html#configuration-of-renv",
    "href": "work/renv-environments.html#configuration-of-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration of renv",
    "text": "Configuration of renv\nFor most users, renv’s default behavior is powerful and doesn’t need modification.\nHowever, the behavior can also be manually set / modified. Generally speaking though, relying on the defaults is the recommended happy path as renv is designed to just magically work. This does mean that troubleshooting when things go wrong can be tricky, see the troubleshooting section below for some tips on what to look out for.\nThere are also a number of environment variables that can be set that will also similarly effect the behavior as relates to setting the repositories being used as the source for package installation.\nCommonly, these settings are set in the .Renviron file to be set across all sessions for that user, or in the R installation’s Renviron.site file so it is active for all users on that server.\nSettings:\n\nRENV_PATHS_PREFIX : Used for sharing state across operating systems\nRENV_PATHS_CELLAR : Path to tarballs, used as a last ditch effort for installing tricky packages\nRENV_PATHS_CACHE : Path location for a cache shared across multiple users\nRENV_CACHE_USER : When using a shared cache, renv can re-assign ownershp of the cache’d package to a separate user account\nrenv.download.trace : Run options(renv.download.trace = TRUE) to temporarily have more verbose logging\n\nConfig settings:\n\nrenv.config.repos.override : Enforce the use of some repositories over what is defined in the renv.lock file\nrenv.config.ppm.enabled : Attempt to transform the repository URL in order to receive binaries on your behalf (defaults to TRUE)\nrenv.config.ppm.default : If repos have not already been set (for example, from the startup .Rprofile) then projects using renv will use the Posit Public Package Manager instance by default\nrenv.config.ppm.url : The URL for Posit Package Manager to be used for new renv projects\nrenv.config.user.environ : Load the users R environ file, usually encouraged (defaults to true)\nrenv.config.user.profile : Load the users R profile file, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.user.library : option to include the system library on the library paths for projects, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.external.libraries : Similar to renv.config.user.library, external libraries can be included with the project, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.cache.enabled : Enable the global renv package cache, so that packages are installed into the global cache and then linked or copied into the users R library in order to save space (defaults to true)\nrenv.config.cache.symlinks : Use symlinks to reference packages installed into the global renv package cache (if set to FALSE packages are copied from the cache into your project library) (enabled by default, defaults to NULL)\nrenv.config.pak.enabled : Use pak with renv to install packages\n\nSince the configuration settings can be set in multiple places, the priority is given according to:\n\n\n\n\n\ngraph TD;\n    A(Renv configuration selection) --&gt;B;\n    B{R option &lt;br/&gt; renv.config.&lt;name&gt;} -- Not set --&gt;C;\n    B{R option &lt;br/&gt; renv.config.&lt;name&gt;} -- Set --&gt;F;\n    C{Environment variable &lt;br/&gt; RENV_CONFIG_&lt;NAME&gt;} -- Not set --&gt;D;\n    C{Environment variable &lt;br/&gt; RENV_CONFIG_&lt;NAME&gt;} -- Set --&gt;F;\n    D{Default} --&gt;F;\n    F(End)\n\n\n\n\n\n\nIf both the R option and the environment variable option are defined, the R option is preferred.\nWe can check the value of any of these parameters a couple ways:\n# Checking the renv options by reading environment variables and renv config properties\nrenv::paths$library()\nSys.getenv('RENV_PATHS_CACHE')\nSys.getenv('RENV_CACHE_USER')\nrenv::paths$cache()\n\n# Check the r_environ and r_profile contents using the usethis package\nlibrary(usethis)\nusethis::edit_r_environ() \nusethis::edit_r_profile()\n\nRenv and binary package OS and R version detection\nBy default, renv used with Package Manager will dynamically set the URL of your repository to pull package binaries for your respective system.\n\nStarting with R 4.4.0, renv automatically uses a platform prefix for library paths on linux (the equivalent to setting RENV_PATHS_PREFIX_AUTO = TRUE). This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required.\n\n\nSharing state across operating systems\nAs of renv 0.13.0, sharing state across operating systems is now possible. By default, it will construct a prefix based on fields within the system’s /etc/os-release file.\nalso possible to explicitly set with the RENV_PATHS_PREFIX environment variable. For example, it could be set like RENV_PATHS_PREFIX = \"ubuntu-bionic\" in order to programmatically generate a cache path like /mnt/shared/renv/cache/v2/ubuntu-bionic/R-3.5/x86_64-pc-linux-gnu. Alternatively the auto feature can be enabled with RENV_PATHS_PREFIX_AUTO = TRUE to automatically detect the environment and set the path.\nCommonly, this would be set in the .Renviron file to be set across all sessions for that user, or in the R installation’s Renviron.site file so it is active for all users on that server.\n\n\n\nRenv and binary package OS and R version detection\nRenv’s default behavior is powerful when using it with Posit Package Manager. It will automatically try to detect the details about your underlying system and set the corrrect URL path so that the appropriate binaries are downloading. If it is unable to find a binary, then it will fail over to the source URL."
  },
  {
    "objectID": "work/renv-environments.html#configuration-of-posit-package-manager",
    "href": "work/renv-environments.html#configuration-of-posit-package-manager",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration of Posit Package Manager",
    "text": "Configuration of Posit Package Manager\nPosit Package Manager is a hosting repository that can be deployed inside a companies network. It is often used in conjunction with vulnerability detection and package blocking for security. It is also useful for hosting internally developed packages that are meant to stay confidential and only used within that particular enterprise organization.\n\nConfiguring R Environments\nConfiguring Python Environments\n\nFor Workbench the URL for Package Manager is commonly configured so that it is at least used as the default repository for both R and Python packages from within the customers enterprise network.\nOptionally, the Posit Package Manager url can be configured to be specific to:\n\nSnapshot dates\nParticular curated repository/repositories\nParticular OS (in order to install binaries)\n\n\nPackage Manager and binary package OS and R version detection\nBinary packages are incredibly useful, enabling faster downloads by skipping the compilation step. When a binary package is requested (by using the __linux__ URL), Package Manager will make a best effort to serve the requested binary package. If that package is unavailable or unsupported on the user’s binary distribution Package Manager will fall back to serving the packages source version.\nPosit Package Manager has the option for the R user agent header can be configured. The user’s User-Agent request header indicates to Package manager which appropriate binary package to server, based on the R version and the OS. A diagnostic script is provided for generating a diagnostic to make sure this is set correctly. The diagnostic will fail to indicate that the OS and R version in the User-Agent request header needs to be updated.\n\n\nClick here to expand for the diagnostic script\n\n# User agent diagnostic script for Posit Package Manager binary packages\n\nlocal({\n  if (.Platform$OS.type != \"unix\" || Sys.info()[\"sysname\"] == \"Darwin\") {\n    message(\"Success! Posit Package Manager does not require additional configuration to install binary packages on macOS or Windows.\")\n    return(invisible())\n  }\n\n  dl_method &lt;- getOption(\"download.file.method\", \"\")\n  dl_extra_args &lt;- getOption(\"download.file.extra\", \"\")\n  user_agent &lt;- getOption(\"HTTPUserAgent\", \"\")\n\n  if (dl_method == \"\") {\n    dl_method &lt;- if (isTRUE(capabilities(\"libcurl\"))) \"libcurl\" else \"internal\"\n  }\n\n  default_ua &lt;- sprintf(\"R (%s)\", paste(getRversion(), R.version$platform, R.version$arch, R.version$os))\n\n  instruction_template &lt;- 'You must configure your HTTP user agent in R to install binary packages.\n\nIn your site-wide startup file (Rprofile.site) or user startup file (.Rprofile), add:\n\n# Set default user agent\n%s\n\n\nThen restart your R session and run this diagnostic script again.\n'\n\n  message(c(\n    sprintf(\"R installation path: %s\\n\", R.home()),\n    sprintf(\"R version: %s\\n\", R.version.string),\n    sprintf(\"OS version: %s\\n\", utils::sessionInfo()$running),\n    sprintf(\"HTTPUserAgent: %s\\n\", user_agent),\n    sprintf(\"Download method: %s\\n\", dl_method),\n    sprintf(\"Download extra args: %s\\n\", dl_extra_args),\n    \"\\n----------------------------\\n\"\n  ))\n\n  if (dl_method == \"libcurl\") {\n    if (!grepl(default_ua, user_agent, fixed = TRUE) ||\n        (getRversion() &gt;= \"3.6.0\" && substr(user_agent, 1, 3) == \"R (\")) {\n      config &lt;- 'options(HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])))'\n      message(sprintf(instruction_template, config))\n      return(invisible())\n    }\n  } else if (dl_method %in% c(\"curl\", \"wget\")) {\n    if (!grepl(sprintf(\"--header \\\"User-Agent: %s\\\"\", default_ua), dl_extra_args, fixed = TRUE)) {\n      ua_arg &lt;- \"sprintf(\\\"--header \\\\\\\"User-Agent: R (%s)\\\\\\\"\\\", paste(getRversion(), R.version[\\\"platform\\\"], R.version[\\\"arch\\\"], R.version[\\\"os\\\"]))\"\n      if (dl_extra_args == \"\") {\n        config &lt;- sprintf(\"options(download.file.extra = %s)\", ua_arg)\n      } else {\n        config &lt;- sprintf(\"options(download.file.extra = paste(%s, %s))\", shQuote(dl_extra_args), ua_arg)\n      }\n      message(sprintf(instruction_template, config))\n      return(invisible())\n    }\n  }\n\n  message(\"Success! Your user agent is correctly configured.\")\n})"
  },
  {
    "objectID": "work/renv-environments.html#configuration-on-workbench-for-r-repository-using-run.r-programmatically-setting-the-repository-location",
    "href": "work/renv-environments.html#configuration-on-workbench-for-r-repository-using-run.r-programmatically-setting-the-repository-location",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration on Workbench for R repository using run.R / Programmatically setting the repository location",
    "text": "Configuration on Workbench for R repository using run.R / Programmatically setting the repository location\nInstead of the above, a run.R file can be used to programmatically set the repository and library location for users. This is commonly used in validated workflows, where the additional oversight is critical.\nExample created by Michael here."
  },
  {
    "objectID": "work/renv-environments.html#scenario-1-setting-up-a-shared-site-library-on-workbench",
    "href": "work/renv-environments.html#scenario-1-setting-up-a-shared-site-library-on-workbench",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 1: Setting up a shared site library on Workbench",
    "text": "Scenario 1: Setting up a shared site library on Workbench\nThe shared site library is specific to an installed version of R. For example for R version 4.3.2 installed to: /opt/R/4.3.2/lib/R/library:\n\nEdit the Rprofile.site file to set the repository URL\n\n# /opt/R/4.3.2/etc/Rprofile.site\nlocal({\n  options(repos = c(CRAN = \"https://r-pkgs.example.com/cran/128\"))\n})\n\n(optional) The default site library can be used, at R_HOME/site-library (in this case /opt/R/3.4.4/lib/R/library), or a site library can be set up by setting .Library.site in R_HOME/etc/Rprofile.site. Multiple library locations can be set up to be used.\nRun R as the root/admin account and install all desired packages\n\n# Multiple packages can be installed at the same time like this: \nexport R_VERSION=4.3.2\n\n/opt/R/${R_VERSION}/bin/R\n\nsudo /opt/R/${R_VERSION}/bin/Rscript -e 'install.packages(c(\"haven\",\"forcats\",\"readr\",\"lubridate\",\"shiny\", \"DBI\", \"odbc\", \"rvest\", \"plotly\",\"rmarkdown\", \"rsconnect\",\"pins\",\"png\",\"tidyverse\", \"Rcpp\"), repos = \"http://cran.us.r-project.org\")'\n\nq()\n\nUsers access packages on the system (without needing to install)\n\nWhen using a shared library, the ability for users to change repository settings and package installation can be disabled:\n# /etc/rstudio/rsession.conf\nallow-r-cran-repos-edit=0\nallow-package-installation=0"
  },
  {
    "objectID": "work/renv-environments.html#scenario-2-setting-up-a-project-to-use-renv",
    "href": "work/renv-environments.html#scenario-2-setting-up-a-project-to-use-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 2: Setting up a project to use renv",
    "text": "Scenario 2: Setting up a project to use renv\n# install renv\ninstall.package(\"renv\") \nlibrary(renv)\n\n# activate the project as an renv project\nrenv::activate()\n\n# generate the renv.lock file \nrenv::snapshot()\n\n# check the status of renv \nrenv::status()\n\n# On a separate system the snapshot can be used to install the specific packages and versions \nrenv::restore() \n\n# Restore a project with an explicit repository URL, note that this does not update the renv.lock file, it will need to be manually edited\nrenv::restore(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\"), rebuild=TRUE)\n\n# Add additional logging\noptions(renv.download.trace = TRUE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-3-determining-the-root-package-that-is-causing-a-failing-dependency",
    "href": "work/renv-environments.html#scenario-3-determining-the-root-package-that-is-causing-a-failing-dependency",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 3: Determining the root package that is causing a failing dependency",
    "text": "Scenario 3: Determining the root package that is causing a failing dependency\nFor example, error message:\n\n2024/05/17 9:24:10 AM: Error in dyn.load(file, DLLpath = DLLpath, …) : 2024/05/17 9:24:10 AM: unable to load shared object ‘/opt/rstudio-connect/mnt/app/packrat/lib/x86_64-pc-linux-gnu/4.3.2/magick/libs/magick.so’: 2024/05/17 9:24:10 AM: libMagick++-6.Q16.so.8: cannot open shared object file: No such file or directory 2024/05/17 9:24:10 AM: Calls: loadNamespace -&gt; library.dynam -&gt; dyn.load\n\nWe can look through our project repository and see that the magick package isn’t directly being called. So the question is, which package is calling it as dependency?\nThe easiest way to look up the dependency is to open the renv.lock file and find which package has it listed as a dependency.\nSome other tricks that might be useful are:\n\nWe can use renv to look at top level dependencies: renv::dependencies()\nWe can use base R to look up package dependencies: tools::package_dependencies(\"leaflet\", recursive = TRUE)[[1]]\nRenv can be told to use pak for package installation with: RENV_CONFIG_PAK_ENABLED = TRUE\nCheck that it set with: Sys.getenv('renv.config.pak.enabled')\nWe can use pak to look up all package dependencies in a tree format: pak::pkg_deps_tree(\"tibble\")\nWe can also get more details about the packages with: pak::pak_sitrep()\nIf an error is encountered, we may need to enable the package pak to work with renv (or be patient and wait a couple minutes after installing pak). There is a useful git issue discussing this here.\n\nWe can then clean up the project and remove packages that are installed, but no longer referenced in the project source, with renv::clean() and save that to the renv lock file with renv::snapshot(). Don’t forget to update your manifest.json file if this is a project being published to Connect with rsconnect::writeManifest()."
  },
  {
    "objectID": "work/renv-environments.html#scenario-4-upgrading-a-project-using-renv-from-r-4.1-to-r-4.4",
    "href": "work/renv-environments.html#scenario-4-upgrading-a-project-using-renv-from-r-4.1-to-r-4.4",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 4: Upgrading a project using renv from R 4.1 to R 4.4",
    "text": "Scenario 4: Upgrading a project using renv from R 4.1 to R 4.4\n\nWhy is this relevant? R CVE detection, vulnerability removed with R 4.4\n\nWhat is recommended: For each project, individually capture the requirements with renv. Change the R version and use the renv.lock file to install the captured requirements for the new R version. Perform tests, updating code and package versions as needed.\nWhat is not recommended: An in-place upgrading. Meaning, we do not recommend removing existing R versions and forcing all projects to use R 4.4. It is likely that code will break and will need developer work to make compatible with the new R version."
  },
  {
    "objectID": "work/renv-environments.html#scenario-5-os-migration-for-individual-r-projects-using-renv",
    "href": "work/renv-environments.html#scenario-5-os-migration-for-individual-r-projects-using-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 5: OS migration for individual R projects using renv",
    "text": "Scenario 5: OS migration for individual R projects using renv\nRefer to here\nAll packages will need to be rebuilt.\nThese two locations in particular, the user home directories and global R or Python directories, will likely need to be flushed and rebuilt:\n\n~/R\n~/.local/lib/python3.*\n\nReference this script from David which programmatically reinstalls all packages installed into user home directories, or the global R or Python directories.\nRebuild renv:\n# Delete existing libraries\nunlink(\"renv/library\", recursive=TRUE)\n\n# Restart R session\n.rs.restartR()\n\n# Change anything that is needed, repository URL, etc\n\n# Re-install libraries\nrenv::restore(rebuild = TRUE)\nRebuild venv:\n# Activate existing venv\nsource .venv/bin/activate\n\n# Capture all installed packages\npython -m pip freeze &gt; requirements-freeze.txt\n\n# Deactivate and delete\ndeactivate\nrm -rf .venv/\n\n# Change anything that is needed, repository URL, etc\n\n# Create a new virtual environment\npython -m venv .venv\nsource .venv/bin/activate \npython -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements-freeze.txt\nFor Connect, the content runtimes will need to be cleared and rebuilt. This can be done pre-emptively.\nDelete:\n# Enumerate the caches known to your server.\nrsconnect system caches list \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key\n\n# Validate cache targeted for deletion.\nrsconnect system caches delete \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --language Python \\\n    --version 3.9.5 \\\n    --dry-run\n\n# Delete one cache.\nrsconnect system caches delete \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --language Python \\\n    --version 3.9.5\nRebuild:\n# Enumerate every \"published\" content item and save its GUID.\nrsconnect content search \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --published | jq '.[].guid' &gt; guids.txt\n\n# Queue each GUID for build.\nxargs printf -- '-g %s\\n' &lt; guids.txt | xargs rsconnect content build add \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key\n\n# Build each queued content item.\nrsconnect content build run \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key"
  },
  {
    "objectID": "work/renv-environments.html#scenario-6-changing-the-project-repository-url",
    "href": "work/renv-environments.html#scenario-6-changing-the-project-repository-url",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 6: Changing the project repository URL",
    "text": "Scenario 6: Changing the project repository URL\nOften the package repository is set to a specific source URL. This can be due to it being within your network, or so that you are getting binaries for a specific OS version, etc.\nUsing the RENV_CONFIG_REPOS_OVERRIDE setting:\noptions('repos')\n\n# Set the override as a one off \nSys.setenv(\"RENV_CONFIG_REPOS_OVERRIDE\" = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \n\n# Check that it set \nSys.getenv(\"RENV_CONFIG_REPOS_OVERRIDE\")\n\n# Turn on debug logging so we can see more information about where packages are coming from and verify it's using the correct URL\noptions(renv.download.trace = TRUE)\n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE) \n\n#Override only applies during restore, and won't update the renv.lock file, so either manually update the renv.lock file with the appropriate URLor using renv::snapshot(repos = \"\")\nUsing the repos setting during rebuild:\n# Rebuild \nrenv::restore(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\"), rebuild=TRUE)\n\n# Snapshot s the URL change is reflected\nrenv::snapshot(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \nChanging it directly in the renv.lock file:\noptions('repos')\n\n# Either manually update the renv.lock file with the appropriate URL or using\nrenv::snapshot(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-7-recovering-an-old-project-that-didnt-have-an-renv-and-isnt-working-with-latest-r-package-versions",
    "href": "work/renv-environments.html#scenario-7-recovering-an-old-project-that-didnt-have-an-renv-and-isnt-working-with-latest-r-package-versions",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 7: Recovering an old project that didn’t have an renv and isn’t working with latest R, package versions",
    "text": "Scenario 7: Recovering an old project that didn’t have an renv and isn’t working with latest R, package versions\nUse the snapshot date option with package manager to “guess” when the environment would have been built with renv so that package versions can be individually tweaked until the project works. Use the renv::revert feature with version control to update the packages with the ability to downgrade as needed."
  },
  {
    "objectID": "work/renv-environments.html#scenario-8-going-between-os-on-the-same-workbench-system-using-slurm-singularity-with-a-renv-project",
    "href": "work/renv-environments.html#scenario-8-going-between-os-on-the-same-workbench-system-using-slurm-singularity-with-a-renv-project",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 8: Going between OS on the same Workbench system using slurm / singularity with a renv project",
    "text": "Scenario 8: Going between OS on the same Workbench system using slurm / singularity with a renv project\nWith the interaction between renv and package manager, as well as the additions with recognition from renv when the OS and R version has changed, things should just work magically as long as the project is configured to use these pieces:\n\nrenv\npackage manager (binaries enabled)\n\nOn a system that has been configured to use slurm with singularity images (that are different OS’s) we can run these lines to get a feel for what is going on:\n# Turn on debug logging so we can see more information about where packages are coming from and verify it's using the correct URL\noptions(renv.download.trace = TRUE)\n\n# Check the default repository URL\noptions('repos')\n\n# Check the OS version\nsystem(\"cat /etc/os-release\")\n\n# Check the details of our singularity environment\nsystem(\"env | grep SINGULARITY\")\n\n# Check that auto-path prefix re-writing is set\nSys.getenv(\"RENV_PATHS_PREFIX_AUTO\")\n\n# We can attempt to set the URL to a specific binary, when we snapshot it will update the lock file to have the generic URL\nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/centos8/latest\")) \n\n# We can attempt to set the URL to a specific binary, when we snapshot it will update the lock file to have the generic URL\nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\")) \n\n# Update the renv to use a source URL as RSPM \nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/latest\")) \n\n# We can also manually set the repo outside of renv this way, for example to successfully download renv\noptions(repos=c(CRAN=\"https://cran.r-project.org\"))\n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE) \nInside the renv lock file we might see a couple different things:\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/__linux__/centos8/latest\"\n      },\nThis will cause problems and will tell renv to install the wrong version of packages for the wrong OS.\nIf we try to snapshot a binary repository URL with renv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\")) then we will see the renv.lock will be updated to:\n    \"Repositories\": [\n      {\n        \"Name\": \"RSPM\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\nThis correction from the binary URL to the base URL will happen regardless of whether the OS matches the one we are using or not.\nWhen we install a package we will see that it is downloading the binary. This is the magic of RENV_PATHS_PREFIX_AUTO! This happens regardless of whether our package source is CRAN or RSPM.\nWe can test what the outputs are for each scenario:\n\nBefore a project has been initialized\nOnce a project has been initialized, with renv\nClosing the project and re-opening it with a different image (different OS) and restoring packages (‘renv::restore(rebuild=TRUE)’)\n\nThe auto-path prefix re-writing is really powerful. This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required. The caveats to know are:\n\nStarting with 4.4, renv automatically uses a platform prefix for library paths on linux.\nR versions below this may need to have the paths prefix set (for example for just the session with Sys.setenv(\"RENV_PATHS_PREFIX_AUTO\" = TRUE), though most likely this should be set at the user or global level).\n\nWe can set auto-path prefix re-writing at the user level by adding RENV_PATHS_PREFIX_AUTO = TRUE into the user r environ file:\nlibrary(usethis)\nusethis::edit_r_environ()"
  },
  {
    "objectID": "work/renv-environments.html#scenario-9-comparing-two-renv-projects",
    "href": "work/renv-environments.html#scenario-9-comparing-two-renv-projects",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 9: Comparing two renv projects",
    "text": "Scenario 9: Comparing two renv projects\nReference: https://forum.posit.co/t/compare-two-renv-projects/145574\nlibrary(jsonlite)\nlibrary(tidyverse)\nmy_renvlock &lt;- fromJSON(\"renv.lock\")\n\npkgs_df&lt;- map_dfr(my_renvlock$Packages, ~ enframe(.) |&gt;\n  filter(name %in% c(\"Package\", \"Version\")) |&gt;\n  mutate(value = as.character(value)) |&gt;\n  pivot_wider())"
  },
  {
    "objectID": "work/renv-environments.html#scenario-10-script-for-updating-packages-from-rspm-that-have-changed-to-site-library",
    "href": "work/renv-environments.html#scenario-10-script-for-updating-packages-from-rspm-that-have-changed-to-site-library",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 10: Script for updating packages from rspm that have changed to site library",
    "text": "Scenario 10: Script for updating packages from rspm that have changed to site library\n# update existing packages\nupdate.packages(lib.loc=&lt;site.library&gt;, repos=&lt;PPM Repo&gt;, ask=FALSE)\n\n# add any new packages\nnew.packages(lib.loc=&lt;site.library&gt;, repos=&lt;PPM Repo&gt;, ask=FALSE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-11-going-from-a-package-environment-to-a-list-of-system-dependencies",
    "href": "work/renv-environments.html#scenario-11-going-from-a-package-environment-to-a-list-of-system-dependencies",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 11: Going from a package environment to a list of system dependencies",
    "text": "Scenario 11: Going from a package environment to a list of system dependencies\nLet’s try to get an environment of packages and understand the system dependencies. This would be useful for fresh installs.\n# create the current environment as a renv project and snapshot it, or restore a project with renv::restore()\nrenv::init()\nrenv::snapshot()\nFind what OS we are on\nR.version # Nope\nversion # Nope\n.Platform # nope\n.Platform$OS.type # nope\nSys.info() # nope\nSys.info()[\"sysname\"] # nope\nsystem(\"cat /etc/*release\") # closer\nsystem(\"lsb_release -a\") # closer\npak::system_r_platform() # closer\npak::system_r_platform_data()$distribution # this is the one!\n\nif(.Platform$OS.type == \"unix\"){\n  Sys.setenv(\"PKG_SYSREQS_PLATFORM\"=pak::system_r_platform_data()$distribution)\n  print(PKG_SYSREQS_PLATFORM)\n} else { ## windows\n  Sys.setenv(\"PKG_SYSREQS_PLATFORM\"=\"windows\") # supported by pak\n  print(PKG_SYSREQS_PLATFORM)\n  warning(\"Windows is not support by pak\")\n}\n\nOptionally, recreate the environment on another server using renv and pak\ncp rserver/renv.lock /code \n\ncd /code && \\\n    echo -e 'options(renv.config.pak.enabled=TRUE)\\noptions(repos=c(CRAN=\"https://packagemanager.posit.co/cran/__linux__/rhel9/2025-03-10\"))\\nSys.getenv(\"PKG_SYSREQS_PLATFORM\" &gt; .Rprofile && \\\n    R -q -e 'install.packages(c(\"renv\"))' && \\\n    R -q -e 'renv::activate()' && \\\n    R -q -e 'renv::restore()'\nCan also take a broader approach\npak::sysreqs_db_list()\npak::sysreqs_list_system_packages()\nMost importantly, let’s take our renv.lock file and use that to find our system dependencies\n# pak::pkg_sysreqs(c(\"curl\", \"xml2\", \"devtools\", \"CHRONOS\"))\npkgs = c(\"curl\", \"xml2\", \"devtools\", \"CHRONOS\")\n\npak::pkg_sysreqs(pkg = pkgs, upgrade = FALSE, sysreqs_platform = Sys.getenv(\"PKG_SYSREQS_PLATFORM\"))\n\n# When we are ready we can update upgrade to TRUE and then install the system dependencies for these packages \n#pak::pkg_sysreqs(pkg = pkgs, upgrade = TRUE, sysreqs_platform = Sys.getenv(\"PKG_SYSREQS_PLATFORM\"))\nAlternatively can check that the system requirements are installed and if not install them\nsysreqs_check_installed(packages = NULL, library = .libPaths()[1])\nsysreqs_fix_installed(packages = NULL, library = .libPaths()[1])"
  },
  {
    "objectID": "work/renv-environments.html#package-installation-errors-on-workbench",
    "href": "work/renv-environments.html#package-installation-errors-on-workbench",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Package installation errors on Workbench",
    "text": "Package installation errors on Workbench\nHere’s an example error message that occurred during package installation inside Workbench (install.packages(askpass)):\n\n* installing binary package ‘askpass’ … cp: cannot open ‘./libs/askpass.so’ for reading: Operation not permitted /usr/bin/gtar: You may not specify more than one ‘-Acdtrux’, ‘–delete’ or ‘–test-label’ option Try ‘/usr/bin/gtar –help’ or ‘/usr/bin/gtar –usage’ for more information. /usr/bin/gtar: This does not look like a tar archive /usr/bin/gtar: Exiting with failure status due to previous errors\n\nA good first trouble shooting step is to SSH on the server and open an R session as root and attempt to install the same package. This helps to rule out where the issue is coming from, the global R configuration, the server, or a specific user issue or something with the Workbench configuration. Create a R session after SSH-ing into the server with /opt/R/${R_VERSION}/bin/R\n\nWhere to start\nGet the system information: Sys.info()\nGet session details: sessionInfo()\n\n\nProblems with pak\nGet details about pak (if used): pak::pak_sitrep()\nCheck if renv has been configured to use pak: Sys.getenv('renv.config.pak.enabled')\n\n\nProblems with renv : where to start\nCan they provide a renv diagnostic? It is generated by running this: renv::diagnostics().\n\n\nProblems with renv : cache location\nCheck the location of the renv cache:\nrenv::paths$library()\nSys.getenv('RENV_PATHS_CACHE')\noptions('renv.config.external.libraries')\noptions('renv.download.trace')\nrenv::paths$cache()\nSys.getenv('RENV_PATHS_PREFIX_AUTO')\nMake sure that it is located to a writeable location (if it is a mount, see the note about file mounts below, this could be a source of issues):\nsystem('namei -l /rsspdata/common/renv_cache/renv/v5/R-3.6/x86_64-pc-linux-gnu')\nCheck that the renv cache location matches the library locations: .libPaths()\nBy default packages are installed into the global cache at ~/.cache/R/renv/ and symlinked from the users cache within the project at ~/renv/library/.\nAre they using a shared renv cache, or an external library,\nDo they know if they’ve implemented settings in either of these, and could they share the contents?\n\nRprofile.site : The RProfile.site file is typically located at etc/Rprofile.site\nRenviron.site : The Renviron.site file is specific to the R installation (in this case I’m interested in if it exists for R 4.3 and R 3.6), typically located at file.path(R.home(\"etc\"), \"Renviron.site\").\nCheck if an external library is referenced in the environment: options('renv.config.external.libraries')\n\nIs the goal to use a shared renv cache location? There are a couple caveats with shared cache’s that can make them tricky. (1) cache permissions can be set with ACL’s, needing admin oversight to make sure are set correctly, (2) packages in the cache are owned by the requesting user, unless the RENV_CACHE_USER option is set. When set, renv will attempt to run chown -R &lt;package&gt; &lt;user&gt; to update cache ownership after the package has been copied into the cache.\nIf the desired behavior is to have a shared renv cache then these two settings will likely need to be added to the project .Renviron, user .Renviron, or site Renviron.site file:\n\nRENV_PATHS_CACHE : Path location for a cache shared across multiple users\nRENV_CACHE_USER : When using a shared cache, renv can re-assign ownership of the cache’d package to a separate user account\n\nI’d be curious, if it’s possible for them, to see if they are able to use R 4.4, or to set that parameter RENV_PATHS_PREFIX_AUTO to true (for example for just the session with Sys.setenv(\"RENV_PATHS_PREFIX_AUTO\" = TRUE)) using their current version of R, and repeat the steps of installing a package:\n\nStarting with R 4.4.0, renv automatically uses a platform prefix for library paths on linux (the equivalent to setting RENV_PATHS_PREFIX_AUTO = TRUE). This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required.\n\nOf course, they could also try this for installing the package, bypassing the cache, and see if it works (but I’m worried that there is a ghost setting somewhere that needs to be removed so that issues don’t keep popping up):\n# install a package, bypassing the cache\nrenv::install(\"&lt;package&gt;\", rebuild = TRUE)\n\n# restore packages from the lockfile, bypassing the cache\nrenv::restore(rebuild = TRUE)\n\n\nProblems with renv : other\nCheck:\n\nAre you running the latest renv? If not, upgrade\nAdd additional logging: options(renv.download.trace = TRUE)\nTake a diagnostic: renv::diagnostics()\n\nIf you are having particular issue with a package and it keeps being pulled in from the cache then doing a complete purge and reinstall can be useful:\nrenv::purge(\"stringr\")\nrenv::purge(\"stringi\")\ninstall.packages(\"stringr\")\nrenv::purge removes packages completely from the package cache (which may be shared across projects) rather than just removing the package from the project which is what renv::remove does. This can be useful if a package which had previously been installed in the cache has become corrupted or unusable, and needs to be re-installed.\nFollow these steps to “flush” and rebuild the renv environment, without losing the important parts of your renv.lock that are defining the R version and package versions:\nrenv::snapshot()\n# Make the appropriate changes (for example, changing OS) \n# Update the renv.lock file manually to reflect any needed changes (for example, changing the repository URL) \nrenv::deactivate()\nrenv::activate()\nrenv::restore(rebuild=TRUE) \nCheck that the packages either installed into the global cache at ~/.cache/R/renv/ or the users cache within the project at ~/renv/library/. The folder structure will give some clues for whether source, binaries were installed, and which OS and R version they were installed for if specified.\n\n\nProblems with packages not persisting\nIs this on a cloud vendor? IE sagemaker, google workstations, azureml? Check that the package repository location is being saved to the mounted drive. If it is saved to the general OS that is ephemeral it will be lost when the session is spun down. This also applies for things like git credentials.\n\n\nIncorrect / corrupted R installation\nCheck for an incorrect R installation for the OS, or a R installation that has gotten corrupted. An easy way to test this is to install a new R version, making sure to closely follow the instructions as well as verifying the OS version.\n\n\nIncorrect package repository source URL for the particular system OS\nWhen R installs a binary package, it doesn’t actually check if the package can be loaded after installation, which is different from source packages. So it is unfortunately possible to install a binary package only to find out later that it can’t actually be loaded.\nCheck the URL that the user is installing from: options('repos')\nTemporarily point the repository to global CRAN and check if the packages will successfully install. For example by running this: options(repos=c(CRAN=\"https://cran.r-project.org\")) and then installing any package with install.packages(\"ggplot2\")\nCheck in /etc/rstudio/rsession.conf if there is anything that would set the library location, for example r-libs-user=~/R/library.\nIt may also be useful to verify both the OS you are currently useing as well as checking that the repository you are pointing towards is using the correct OS if it is pulling in the binaries.\nFor debian/ubuntu distributions:\nlsb_release -a\nFor other distributions (more broadly cross-linux compatible command):\ncat /etc/os-release\n\n\nUsers lacking read/write permissions to their home directory\nCheck the home directory permissions on /home/username/. For example with namei -l /home/username/.\nIf useful, could try recursively chown-ing the directory with the user experiencing the issue and chmod 750 to make sure there is access.\nThis can commonly happen after a migration from one server to another, if the correct permissions weren’t correctly carried over. This is why we commonly recommend using rsync with the -a flag for transfer any files / directories. This syncs directories recursively and preserve symbolic links, groups, ownership, and permissions. Additionally, rsync needs to be used in root mode in order to completely move the various software and home directory components as it includes files with restrictive read and write permissions.\nFor example, the permissions should look something like: -rwx-r--r--\n\n\nUsers lacking permissions to ./libs\nCheck the permissions on ./libs/. For example with namei -l ./libs and ls -la ./libs\n\n\nIncorrect PAM configuration for users\nCheck the output of sudo getent passwd username\nFrom a workbench session the output of the environment, Sys.getenv() and compare between a Workbench session and logged into a R session as root on the server (after SSH-ing in)\nFrom an SSH session as root check the outputs of the user verification commands: sudo /usr/lib/rstudio-server/bin/pamtester --verbose &lt;session-profile&gt; &lt;user&gt; authenticate acct_mgmt setcred open_session\nFor example this command will likely look like: sudo /usr/lib/rstudio-server/bin/pamtester --verbose rstudio-session username authenticate acct_mgmt setcred open_session\nCheck for any umask or mask lines used during user provisioning, in the /etc/sssd/sssd.conf file\n\n\nServer hardening\nAnother thing to check is whether SELinux is enabled on the system. Check the mode with getenforce\nThis can result in user specific errors, in that case compare the SELinux context for a user that has successfully package installations to the one that is having errors.\nOften the following command will work to fix SELinux context issues: restorecon -Rv /home/users/username\nGreat article from our support team discussing how to use selinux\nDisable SELINUX (RHEL only): setenforce 0 && sudo sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config\nCheck for FIPS being enabled: fips-mode-setup --check\nThis article from redhat on FIPS mode is also very useful.\n\n\nMounted share drive\nCheck if /home on the server, or is it a network mount (NFS or CIFS). In NFS, for example, there can be the use of access control lists which can impact permissions. Similarly, when working in a system that has a mounted share drive then would want to check that libraries are being written to that share so you get persistence. Typically this means writing to inside the home directory. Check mounted drives with: df -h\nCheck /etc/fstab to see if the home directories are mounted with noexec\nFor example, this shows that the home directories were mounted with noexec: /dev/mapper/rhel-home  /home  xfs    defaults,noexec,nosuid,nodev   0 0\nThis resulted in this error message:\nlibrary(stringi)Error: package or namespace load failed for 'stringi' in dyn.load(file, DLLpath = DLLpath, ...):\nunable to load shared object '/home/c_jjones/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so':\n  /home/c_jjones/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so: failed to map segment from shared object\n\n\nAzure cloud images\nThe default Azure RHEL images are unfortunately constricted in their ability to do some things.\n\n\nSlurm\nThe Slurm service account should have full privileges to the Slurm environment (like killing jobs).\nIn regards to not being able to run the diagnostics command, could you please provide the following:\n\nEnable debug logging by setting enable-debug-logging=1 in /etc/rstudio/launcher.slurm.conf\nTrigger the issue you are experiencing after restarting the launcher.\nResulting logs will be in: - /var/lib/rstudio-launcher/Slurm/rstudio-slurm-launcher.log\nThe Slurm version, which can be found by running sinfo –version\nThe installation location of Slurm on the host\nYour /etc/slurm.conf (or equivalent) configuration file\nThe output of running sinfo as the Slurm service user configured in /etc/rstudio/launcher.slurm.conf\nRun test job with srun date\nReplace  with a valid username of a user that is set up to run Posit - Workbench in your installation, in the commands below:\nsudo rstudio-server stop\nsudo rstudio-server verify-installation –verify-user=\nsudo rstudio-server start\nThe output of running sudo rstudio-launcher status"
  },
  {
    "objectID": "work/r-projects-setup.html",
    "href": "work/r-projects-setup.html",
    "title": "R projects set up and maintenance",
    "section": "",
    "text": "Have you ever tried to use someone else’s repo and run into issues with package installation and breaking package versions? Or tried to work on a really old repository and been foiled while trying to set it up?\nProjects have a standards problem - we need to realize that all the work we are doing exist in the framework of a project. By defining the elements of a project we can identify the parts that need to be made transparent and the tools (renv/venv) for making that happen.\nWhat is a project?\nWhat it’s not:"
  },
  {
    "objectID": "work/r-projects-setup.html#reproducible-package-environments-for-r---at-a-glance",
    "href": "work/r-projects-setup.html#reproducible-package-environments-for-r---at-a-glance",
    "title": "R projects set up and maintenance",
    "section": "Reproducible package environments for R - At a glance",
    "text": "Reproducible package environments for R - At a glance\nStep 1: Use pre-compiled packages\n\nGo to Public Package Manager\nClick on Get Started -&gt; Setup -&gt; Distribution and select your OS -&gt; Select Latest or Freeze and follow the instructions below the calendar.\nFor example:\n\noptions(repos = c(REPO_NAME = \"https://packagemanager.rstudio.com/all/latest\"))\nStep 2: Use environment tracking\n# Set up a new version controlled R project and install renv:\ninstall.packages(\"renv\")\nlibrary(renv)\n\n# Initialize your project with renv and take a snapshot:\nrenv::init()\nrenv::snapshot():\n\n# Update all packages, or revert back to an earlier snapshot:\nrenv::update()\nrenv::revert()\n\n# History is saved into version control:\nrenv::history()\nStep 3: Easy collaboration\n# Have your colleague configure their repository to match yours: \noptions(repos = c(REPO_NAME = \"https://packagemanager.rstudio.com/all/latest\")) \n\n## Send a colleague the link to your project on git, they'll restore your environment with:\nrenv::restore()"
  },
  {
    "objectID": "work/r-projects-setup.html#r-projects-setup",
    "href": "work/r-projects-setup.html#r-projects-setup",
    "title": "R projects set up and maintenance",
    "section": "R projects setup",
    "text": "R projects setup\nSetup the renv environment:\nrenv::activate()\nrenv::restore()\nTo run the app either open app/app.R and click the “Run App” button at the top of the IDE code pane or use:\nshiny::runApp(\"app\")"
  },
  {
    "objectID": "work/r-projects-setup.html#deployment",
    "href": "work/r-projects-setup.html#deployment",
    "title": "R projects set up and maintenance",
    "section": "Deployment",
    "text": "Deployment\n\nPush Button\nOpen app/app.R and use the blue publish icon in the upper right corner of the IDE code pane.\n\n\nrsconnect package\nYou can also deploy using the rsconnect package:\nrsconnect::deployApp(\n  appDir = \"app\",\n  appTitle = \"Shiny Penguins\"\n)\n\n\nGit-backed\nUpdate the code, and then run:\nrsconnect::writeManifest(\"app\")\nCommit the new manifest.json file to the git repo along with the code."
  },
  {
    "objectID": "work/r-projects-setup.html#project-updates",
    "href": "work/r-projects-setup.html#project-updates",
    "title": "R projects set up and maintenance",
    "section": "Project updates",
    "text": "Project updates\nUse renv to record the r package versions used\nCreate a manifest.json file to support git-backed publishing"
  },
  {
    "objectID": "work/r-projects-setup.html#all-about-renv",
    "href": "work/r-projects-setup.html#all-about-renv",
    "title": "R projects set up and maintenance",
    "section": "All about renv",
    "text": "All about renv\nWhy use renv?\nThere is an excellent video by David Aja discussing why he started using renv at the 2022 RStudio Conference here.\nEver had your code mysteriously stop working or start producing different results after upgrading packages, and had to spend hours debugging to find which package was the culprit? Ever tried to collaborate on code just to get stuck on trying to decipher various package dependencies?\nrenv helps you track and control package changes - making it easy to revert back if you need to. It works with your current methods of installing packages (install.packages()), and was designed to work with most data science workflows.\nWho shouldn’t use renv?\n\nPackage developers\n?\n\n# Terms\n\nR Project - a special kind of directory of files and supporting functionality.\nPackage - a collection of functions beyond base R that developers can install and use.\nLibrary - a directory containing installed packages."
  },
  {
    "objectID": "work/r-projects-setup.html#troubleshooting-1",
    "href": "work/r-projects-setup.html#troubleshooting-1",
    "title": "R projects set up and maintenance",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nBioconductor and Mass and R version: https://forum.posit.co/t/mass-not-available-for-r-4-3-3/188156/2"
  },
  {
    "objectID": "work/R package installation improvements on Workbench.html",
    "href": "work/R package installation improvements on Workbench.html",
    "title": "Improving the performance of R package installation on Workbench",
    "section": "",
    "text": "I wanted to take a moment to write up some notes on the package installation process in R that might be useful for troubleshooting slow package install times and implementing a workaround. This is coming out of a very great conversation I had with a colleague that I hope is useful for you too."
  },
  {
    "objectID": "work/R package installation improvements on Workbench.html#motivation",
    "href": "work/R package installation improvements on Workbench.html#motivation",
    "title": "Improving the performance of R package installation on Workbench",
    "section": "",
    "text": "I wanted to take a moment to write up some notes on the package installation process in R that might be useful for troubleshooting slow package install times and implementing a workaround. This is coming out of a very great conversation I had with a colleague that I hope is useful for you too."
  },
  {
    "objectID": "work/R package installation improvements on Workbench.html#package-installation-process-in-r",
    "href": "work/R package installation improvements on Workbench.html#package-installation-process-in-r",
    "title": "Improving the performance of R package installation on Workbench",
    "section": "Package Installation Process in R",
    "text": "Package Installation Process in R\nWhen a user runs install.packages it can go in a couple locations: - System library (global, only happens when run by an administrator) - Site library (optional, requires admin set up) - User libary - Project library (optional, requires specific renv setup)\nLet’s start with a non renv example - if we have, say, 2 projects then the first time (when the user has a clean directory) the install for the packages for that 1st project could take awhile. This is particularly noticeable with bioconductor packages because the bioconductor repository doesn’t provide binaries and every package has to be compiled as source. After that, however, for the seconod project (if it is using the same packages) it will feed from the same library in the same user home directory, thus feeling faster. It won’t reinstall the packages.\nHowever, in general relying on the user library is a bad thing. Take this example - imagine two projects that start out using the same package. If in project two that package is upgraded when we go back to the first project there is a risk the code isn’t working because the package was upgraded. This is where renv comes in."
  },
  {
    "objectID": "work/R package installation improvements on Workbench.html#renv",
    "href": "work/R package installation improvements on Workbench.html#renv",
    "title": "Improving the performance of R package installation on Workbench",
    "section": "Renv",
    "text": "Renv\nRenv will operate very similarly to the typical process for installing packages. Except that when two projects need different versions of the same package it will install and maintain those two versions separately. By default it creates a project specific folder but it is linking to a package cache that is by default global for a given user. Same experience here where the first time a package is downloaded and compiled it will take awhile. But after that it will be fast, assuming the same package. Now if the renv is using different package versions then that will trigger the package reinstall and that could take a long time. Aligning the renv.lock files to use the same package versions would greatly speed up the process.\n\nMagic Trick\nNow here is a magic trick. If using a recent version of renv they could enable it to use pak: options(renv.config.pak.enabled=TRUE)\nThis would be either run inside every user session (risky since that is easy to forget) or could be put in the users rprofile file before running renv::restore. This would make the install, download, build, packages in parallel using pak. You can find the location of your rprofile file with the usethis package."
  },
  {
    "objectID": "work/R package installation improvements on Workbench.html#other-options",
    "href": "work/R package installation improvements on Workbench.html#other-options",
    "title": "Improving the performance of R package installation on Workbench",
    "section": "Other Options",
    "text": "Other Options\nIf want to move beyond the per user renv cacheing then we could consider a system global package cache. IT would need to set up a directory with fairly wide permissions to make this happen. Example: https://github.com/sol-eng/singularity-rstudio?tab=readme-ov-file#what-does-renv-actually-do-"
  },
  {
    "objectID": "work/R package installation improvements on Workbench.html#summary",
    "href": "work/R package installation improvements on Workbench.html#summary",
    "title": "Improving the performance of R package installation on Workbench",
    "section": "Summary",
    "text": "Summary\nSo to summarize some possible solutions to improving the install time for packages:\n\nUse pak with renv to enable paralellized package installs with options(renv.config.pak.enabled=TRUE) (my colleague called this a “magic trick” and would be my choice for what to try)\nAlign the renv.lock files to use the same package versions where possible\nConsider moving to a global package cache, but this would require IT overhead"
  },
  {
    "objectID": "work/Positron.html",
    "href": "work/Positron.html",
    "title": "Exploring Positron",
    "section": "",
    "text": "Positron is a new IDE developed by Posit. I’ve been putting off giving it a deep dive. I love RStudio. Maybe it’s the green icons, or the simple 4 box layout, or even just that it’s what I’ve been using for so long but it really feels like a happy place to me.\nI was the same way with RMarkdown. You could have torn it from my cold dead hands, but eventually that feature comes along that makes going through the process of changing worth it. And for me with Quarto that was lightbox with its ability to magically resize images. Such a simple thing, such a challenging thing to do outside of Quarto."
  },
  {
    "objectID": "work/Positron.html#first-impressions",
    "href": "work/Positron.html#first-impressions",
    "title": "Exploring Positron",
    "section": "First impressions",
    "text": "First impressions\nChanging the theme made it feel like less of a sterile and “other” place. I’m a big fan of dark modes and I actually really liked the built in dark mode.\ngear -&gt; theme -&gt; positron dark\nThere are some good resources for porting your own theme also: https://www.r-bloggers.com/2025/05/porting-my-favorite-rstudio-color-theme-to-positron/"
  },
  {
    "objectID": "work/Positron.html#viewing-data",
    "href": "work/Positron.html#viewing-data",
    "title": "Exploring Positron",
    "section": "Viewing data",
    "text": "Viewing data\n\n\n\n\n\n\n\n\n\n\n\n(a) Light theme\n\n\n\n\n\n\n\n\n\n\n\n(b) dark theme\n\n\n\n\n\n\n\nFigure 1: Data Previewer now shows distributions!"
  },
  {
    "objectID": "work/Positron.html#my-extensions",
    "href": "work/Positron.html#my-extensions",
    "title": "Exploring Positron",
    "section": "My extensions",
    "text": "My extensions\nSince Positron is a fork of VS Code it relies on extensions for adding various features. Here are some of my favorites:\n\nAir (R Language Support)\nPosit Publisher\nBlack Formatter\nGitlive\nPyright\nJupyter\nJupter Cell Tags\nJupyter Keymap\nJupyter Slide Show\nProject Manager\nPython Debugger\nQuarto\nRuff\nShiny\nScope to this\nvscode-pets"
  },
  {
    "objectID": "work/Positron.html#remote-connections-with-ssh",
    "href": "work/Positron.html#remote-connections-with-ssh",
    "title": "Exploring Positron",
    "section": "Remote connections with SSH",
    "text": "Remote connections with SSH\nThis is bundled with Positron and there’s no need to install anything.\nResource: https://positron.posit.co/remote-ssh.html"
  },
  {
    "objectID": "work/Positron.html#llm-integrations",
    "href": "work/Positron.html#llm-integrations",
    "title": "Exploring Positron",
    "section": "LLM integrations",
    "text": "LLM integrations\nStay tuned for Posit::Conf 2025!"
  },
  {
    "objectID": "work/Positron.html#running-a-python-project",
    "href": "work/Positron.html#running-a-python-project",
    "title": "Exploring Positron",
    "section": "Running a python project",
    "text": "Running a python project\nReference: https://positron.posit.co/start.html\nI’m a big fan of using the git integration to clone down a project, for example this set of demo examples.\nWe’ll want to make sure that ipykernel is installed:\npython3 -m pip install ipykernel\nWe can discover which python versions we have access to with:\nls /opt/python/\n\nSelect the python interpreter\nManage the interpreter in the session (virtual environment) through the top right icon:\n\n\n\n\n\n\nFigure 2: Python interpreter selection\n\n\n\nReference: https://positron.posit.co/managing-interpreters.html\nYou can also manually select the python interpreter with ctrl-shift-p and then Python: Select Interpreter command and select the Enter interpreter path... option.\nThis gives you the ability to refresh the visible interpreters with the circle arrow symbol.\n\n\n\n\n\n\nFigure 3: Python interpreter manual selection\n\n\n\nWe can create a new interpreter (and then select it through the dropdown) either with uv or pip.\n\nThe uv way\nUse uv. It will detect that this is a project and create the venv for us when we go to run the application.\nRun the application:\nuv run app.py\nuv run shiny run --reload app.py\n\n\nThe pip way\nSetup the venv environment:\npython -m venv env\n. env/bin/activate\n# source env/bin/activate\npip install --upgrade pip\npip install -r requirements.txt\nI recommend using env instead of .venv because quarto will automatically pick it up.\nRun the application:\nshiny run --reload app.py\n\n\nRegister new interpreter as kernel (likely optional)\n# Register as kernel\npip install ipykernel jupyter\n# python -m ipykernel install --name \"my-new-project_venv\" --user \npython -m ipykernel install --name \"shiny-income-share\" --user\n\n\n\nRunning and previewing an app\nOpen the folder so that the root directory is the content example you want to run.\nMake sure the appropriate virtual environment has been created (following the steps above) and selected as the interpreter.\nClick the play button to launch the application.\n\n\n\n\n\n\nFigure 4: Python Shiny App\n\n\n\n\n\nCreating a new project\n\nThe pip way\nCreate a venv to isolate environments and generate a requirements.txt file so that only the minimum packages needed to support your project are included. Read this for more on Python package management.\npython3 -m pip freeze &gt; requirements.txt\npython3 -m pip install -r requirements.txt\n\n\nThe uv way\nCreate a manifest.json file to support git-backed publishing\nFor uv projects, they can be initialized and updated with (change format and packages as needed):\n    uv init --app --python 3.12.6\n    uv add numpy matplotlib jupyter\n    uv sync --upgrade\n    uv export -o requirements.txt --no-hashes\n    uv run rsconnect write-manifest api .\n\n    # Dev dependencies may need to be added \n    uv add --dev uv ipykernel\n\n\n\nDeploying\nReference the Posit Connect User Guide: Shiny for Python\n\nUsing Posit Publisher\nMake sure the Posit Publisher extension is installed.\nClick new deployment and follow the screens to add your developer information and an API key.\nMake sure to add any files that need to be included in the bundle to (1) the .toml file (2) the project files list.\nFor example, the .toml file might look like:\n# Configuration file generated by Posit Publisher.\n# Please review and modify as needed. See the documentation for more options:\n# https://github.com/posit-dev/publisher/blob/main/docs/configuration.md\n'$schema' = 'https://cdn.posit.co/publisher/schemas/posit-publishing-schema-v3.json'\ntype = 'python-shiny'\nentrypoint = 'app.py'\nvalidate = true\nfiles = [\n  '/app.py',\n  '/data,csv',\n  '/requirements.txt',\n  '/.posit/publish/shiny-income-share-positron-FLKH.toml',\n  '/.posit/publish/deployments/deployment-NVOJ.toml',\n  '/data.csv'\n]\ntitle = 'shiny-income-share-positron'\n\n[python]\nversion = \"3.11.9\"\npackage_file = \"requirements.txt\"\npackage_manager = \"pip\"\n\nReference: https://github.com/posit-dev/publisher/blob/main/docs/configuration.md\n\n\nrsconnect-python CLI\n# With uv\nuv run rsconnect deploy shiny .\n# Without uv\nrsconnect deploy shiny .\n\n\nGit-backed\nUpdate the code, and then run:\n# With uv\nuv export -o requirements.txt --no-hashes\nuv run rsconnect write-manifest shiny --overwrite .\n# Without uv\npip freeze &gt; requirements.txt \nrsconnect write-manifest shiny --overwrite .\nCommit the new manifest.json file to the git repo along with the code."
  },
  {
    "objectID": "work/Positron.html#resources",
    "href": "work/Positron.html#resources",
    "title": "Exploring Positron",
    "section": "Resources",
    "text": "Resources\nInspired by: https://www.andrewheiss.com/blog/2024/07/08/fun-with-positron/\n\n\n\nFigure 1 (a): Light theme\nFigure 1 (b): dark theme\nFigure 2: Python interpreter selection\nFigure 3: Python interpreter manual selection\nFigure 4: Python Shiny App"
  },
  {
    "objectID": "work/mounting-drives-server.html",
    "href": "work/mounting-drives-server.html",
    "title": "Mounting drives on a server for Posit Team",
    "section": "",
    "text": "Workbench and Connect will work with the underlying linux system to make resources available to users. This includes things like mounted drives.\nNeither Workbench nor Connect natively mount drives, this isn’t done through a config. Drive mounts are done through linux operations.\n\n\nTypically we see this looking like\n\nCreate a local directory\nmount -t nfs -o optionshere sharehost:sharepath\n\nThe important bit would be to set permissions with chown, chmod, and maybe facl for either the rstudio-connect user, or the alternate user they choose, for example something like:\nsudo chown user:group /folder\nchmod 764 /folder\nChange the code (764) for what the folder is being locked down to in order to prevent other users from being able to access it. Those lines will make that user the owner of the folder, and then chmod 764 should give only the owner access. But I recommend referencing your linux team.\nIt can also involve fstab or AutoFS.\nSome specific situations may have some complexity: specific formats, cloud situations, or verifying NFS export options, but in general the mounting process is much simpler than installing the posit products and configuring them and is the same as what would be done on any linux server.\nIf running in kubernetes then the mount needs to be added to each job pod and it gets more complicated."
  },
  {
    "objectID": "work/mounting-drives-server.html#overview",
    "href": "work/mounting-drives-server.html#overview",
    "title": "Mounting drives on a server for Posit Team",
    "section": "",
    "text": "Workbench and Connect will work with the underlying linux system to make resources available to users. This includes things like mounted drives.\nNeither Workbench nor Connect natively mount drives, this isn’t done through a config. Drive mounts are done through linux operations.\n\n\nTypically we see this looking like\n\nCreate a local directory\nmount -t nfs -o optionshere sharehost:sharepath\n\nThe important bit would be to set permissions with chown, chmod, and maybe facl for either the rstudio-connect user, or the alternate user they choose, for example something like:\nsudo chown user:group /folder\nchmod 764 /folder\nChange the code (764) for what the folder is being locked down to in order to prevent other users from being able to access it. Those lines will make that user the owner of the folder, and then chmod 764 should give only the owner access. But I recommend referencing your linux team.\nIt can also involve fstab or AutoFS.\nSome specific situations may have some complexity: specific formats, cloud situations, or verifying NFS export options, but in general the mounting process is much simpler than installing the posit products and configuring them and is the same as what would be done on any linux server.\nIf running in kubernetes then the mount needs to be added to each job pod and it gets more complicated."
  },
  {
    "objectID": "work/mounting-drives-server.html#smbcifs",
    "href": "work/mounting-drives-server.html#smbcifs",
    "title": "Mounting drives on a server for Posit Team",
    "section": "SMB/CIFS",
    "text": "SMB/CIFS\nFor SMB/CIFS this could look like mounting via PAM, discussed here: https://support.posit.co/hc/en-us/articles/360044190234-How-to-mount-a-Windows-SMB-CIFS-share-via-PAM\nAlternatively, the SMB/CIFS could be mounted directly with cifs-utils using a service account if traceability or access controls aren’t needed with sudo mount -t cifs -o username=&lt;user&gt;,password=&lt;password&gt; //&lt;server&gt;/&lt;share&gt; /mnt/&lt;mountpoint&gt;.\nMultiprotocol NAS is another option for a more enterprise solution."
  },
  {
    "objectID": "work/mounting-drives-server.html#copying-a-project-from-one-users-home-directory-to-another",
    "href": "work/mounting-drives-server.html#copying-a-project-from-one-users-home-directory-to-another",
    "title": "Mounting drives on a server for Posit Team",
    "section": "Copying a project from one users home directory to another",
    "text": "Copying a project from one users home directory to another\nReference: https://support.posit.co/hc/en-us/articles/12959116352663-Transferring-project-ownership-in-Posit-Workbench\nThis will recursively copy the project from user1’s home directory to user2’s home directory\n`sudo cp -R /home/user1/project /home/user2/``\nAnd this will change the owner recursively to user2\nsudo chown -R user2:user2 /home/user2/project"
  },
  {
    "objectID": "work/mounting-drives-server.html#permissions",
    "href": "work/mounting-drives-server.html#permissions",
    "title": "Mounting drives on a server for Posit Team",
    "section": "Permissions",
    "text": "Permissions\nWhen mounting on the server it will need the appropriate permissions so that users are able to access the contents of the drive.\nFor example for content running on Posit Connect the rstudio-connect user will need access to the drive, for example via chmod 644. Alternatively the drive could be mapped so that an alternate linux user has access and a user with admin access through the Connect UI can set the runAs user to the selected linux user. This will provide more security around protecting the files that are mounted."
  },
  {
    "objectID": "work/mounting-drives-server.html#logging-user-actions",
    "href": "work/mounting-drives-server.html#logging-user-actions",
    "title": "Mounting drives on a server for Posit Team",
    "section": "Logging user actions",
    "text": "Logging user actions\nFor customers wanting to log user actions on NFS shares that is done through linux admin utilities, like auditd.\nAlternatively kerberized NFS shares can be used for centralized logging/auditing but is significantly more admin overhead."
  },
  {
    "objectID": "work/mounting-drives-server.html#failures",
    "href": "work/mounting-drives-server.html#failures",
    "title": "Mounting drives on a server for Posit Team",
    "section": "Failures",
    "text": "Failures\n\nChecking mounted drives and noexec option\nCheck if /home on the server, or is it a network mount (NFS or CIFS). In NFS, for example, there can be the use of access control lists which can impact permissions. Similarly, when working in a system that has a mounted share drive then would want to check that libraries are being written to that share so you get persistence. Typically this means writing to inside the home directory. Check mounted drives with: df -h\nCheck /etc/fstab to see if the home directories are mounted with noexec\nFor example, this shows that the home directories were mounted with noexec: /dev/mapper/rhel-home  /home  xfs    defaults,noexec,nosuid,nodev   0 0\nThis resulted in this error message:\nlibrary(stringi)Error: package or namespace load failed for 'stringi' in dyn.load(file, DLLpath = DLLpath, ...):\nunable to load shared object '/home/username/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so':\n  /home/username/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so: failed to map segment from shared object"
  },
  {
    "objectID": "work/mounting-drives-server.html#resources",
    "href": "work/mounting-drives-server.html#resources",
    "title": "Mounting drives on a server for Posit Team",
    "section": "Resources",
    "text": "Resources\nA linux based forum like serverfault can be useful for getting more answers for linux topics like this.\n\nhttps://www.golinuxcloud.com/linux-mount-nfs/\nhttps://planetscale.com/blog/io-devices-and-latency#storage-in-the-cloud\nhttps://solutions.posit.co/admin-training/courses/workbench/04_datasources.html#smbcifs\nReally interesting community post with script for automounting windows shares: https://community.rstudio.com/t/active-directory-ad-permissions-and-file-share-mounting-for-rsp/5427/5\nExample for mounting a drive: https://superuser.com/questions/134734/how-to-mount-a-drive-from-terminal-in-ubuntu"
  },
  {
    "objectID": "work/LLMs.html",
    "href": "work/LLMs.html",
    "title": "Fun with LLM’s",
    "section": "",
    "text": "I’ve heard LLM’s described as a revolutionary search engine, which feels pretty on the nose. Definitely the way we interact with the internet is vastly different when leveraging any LLM technology. I wanted to squirrel away some thoughts and experiments from my own exploration into various LLM technologies."
  },
  {
    "objectID": "work/LLMs.html#intro",
    "href": "work/LLMs.html#intro",
    "title": "Fun with LLM’s",
    "section": "",
    "text": "I’ve heard LLM’s described as a revolutionary search engine, which feels pretty on the nose. Definitely the way we interact with the internet is vastly different when leveraging any LLM technology. I wanted to squirrel away some thoughts and experiments from my own exploration into various LLM technologies."
  },
  {
    "objectID": "work/LLMs.html#what-llms-cant-do",
    "href": "work/LLMs.html#what-llms-cant-do",
    "title": "Fun with LLM’s",
    "section": "What LLM’s can’t do",
    "text": "What LLM’s can’t do\nAs appealing as it may be, at the end of the day the burden is still on you figuring out how to articulate what you want. I think this is where a lot of the hype has gone off the rails - this idea of an LLM completely releasing you from any thought is never going to happen. Having skills in articulate a plan and coming up with ideas is always going to matter - as eloquently put in this xkcd (also proving that there really is an xkcd for anything).\n\n\n\nPertinent XKCD"
  },
  {
    "objectID": "work/LLMs.html#setting-up-claude-code-in-aws",
    "href": "work/LLMs.html#setting-up-claude-code-in-aws",
    "title": "Fun with LLM’s",
    "section": "Setting up Claude Code in AWS",
    "text": "Setting up Claude Code in AWS\nInstall NodeJS 18+\nsudo apt install nodejs\nnode -v # check the version\nsudo apt-get install npm\nsudo npm config set os linux\nMake sure your aws credentials are set up. Create credential files in ~/.aws/credentials and ~/.aws/config\nInstall Claude code (instructions say don’t use sudo! But I did anyway)\nsudo npm install -g @anthropic-ai/claude-code\nsudo npm install -g &lt;ModuleName&gt; --unsafe-perm=true --allow-root\nsudo npm install -g @anthropic-ai/claude-code --force --no-os-check --allow-root\nIf needed, uninstall it:\nsudo apt-get remove npm\nnpm uninstall -g @anthropic-ai/claude-code\nUpdate your bashrc and add\nsudo nano  ~/.bashrc\n\nexport AWS_REGION=us-east-2 \nexport CLAUDE_CODE_USE_BEDROCK=1 \nexport ANTHROPIC_MODEL='us.anthropic.claude-sonnet-4-20250514-v1:0'\nBecause I am doing this as a sudo user I should also add it to my /etc/environment file:\nsudo nano /etc/environment \n\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n\n\nexport AWS_REGION=us-east-2 \nexport CLAUDE_CODE_USE_BEDROCK=1 \nexport ANTHROPIC_MODEL='us.anthropic.claude-sonnet-4-20250514-v1:0'\nsource ~/.bashrc\nsource /etc/environment \nCheck that they set with:\nenv | grep CLAUDE\nsudo env | grep CLAUDE\nenv | grep AWS\nenv | grep ANTHROPIC\nTo use it first cd into your project directory, log in to aws, and then you can do things with claude:\ncd soleng-book\naws sso login\nYou’ll want to create an instruction file named CLAUDE.md, something like:\n\n\nCLAUDE.md\n\n# Project - Guide\n\n## Project Overview\n\nThis project is a set of pages documenting various topics around server infrastructure, software installation, integrations, and maintenance. It provides the following topics:\n\n- Storage \n- SSL \n- Networking \n- Slurm \n\n## Important Considerations\n\n**Always run code quality checks before committing**. This includes linting, type checking, and running tests to ensure code quality and functionality.\n**Prefer readable and maintainable code**. Avoid complex one-liners or overly clever solutions that reduce code clarity.\n**Always document code and configurations**. Use docstrings for functions and classes, and comments where necessary to explain complex logic or decisions.\n**When in doubt, ask for help**. If you're unsure about how to implement something or how to fix an error, don't hesitate to reach out to the team for assistance.\n**Less is more**. Avoid unnecessary complexity in code and configurations. Aim for simplicity and clarity in your solutions.\n\n## Documentation Quality Standards\n\n- Accuracy: Information is correct and up-to-date.\n- Clarity: Language is precise, unambiguous, and easy to understand.\n- Completeness: All necessary information is present; no critical steps or explanations are missing.\n- Consistency: Terminology, formatting, and style are uniform throughout.\n- Conciseness: Information is presented efficiently without unnecessary words or repetition.\n- Relevance: Content directly addresses the needs and questions of the target audience.\n- Accessibility: Documentation is easy to navigate, search, and consume (e.g., clear headings, index, TOC).\n- Examples/Illustrations: Sufficient and relevant code examples, diagrams, or screenshots are provided.\n- Audience Appropriateness: Language and technical depth are tailored to the intended readers.\n- Maintainability: Structure and content facilitate future updates and revisions.\n\n## Contributing\n\n1. Plan the changes carefully, considering the overall architecture and existing patterns.\n2. Write sections following existing code patterns and type hints\n3. Add more detail where necessary\n4. Once everything is working, make a commit and request human review.\n\nYou can then use the claude command to be off to the races:\nclaude\n\n\n\nOff to the races!"
  },
  {
    "objectID": "work/LLMs.html#self-hosting-with-ollama",
    "href": "work/LLMs.html#self-hosting-with-ollama",
    "title": "Fun with LLM’s",
    "section": "Self hosting with Ollama",
    "text": "Self hosting with Ollama\n[Ollama] is a LLM model that you can self-host. You would want to place it on its own server (because it can an will consume all resources) and then access it through that IP address, like this:\nserver_ip_address &lt;- \"&lt;ADD YOUR SERVER'S IP ADDRESS HERE&gt;\"\n\nlibrary(ellmer)\n\nchat &lt;- chat_ollama(\n  system_prompt = \"Answer questions using the R programming language. \",\n  base_url = paste0(\"http://\", server_ip_address, \":11434\"),\n  model = \"gemma3:1b\",\n)\n(Credit to Isabella)"
  },
  {
    "objectID": "work/LLMs.html#security",
    "href": "work/LLMs.html#security",
    "title": "Fun with LLM’s",
    "section": "Security",
    "text": "Security\nLarge Language Models (LLMs) are revolutionizing everything from code generation to data analysis and customer interactions.  However, their rapid adoption also introduces significant security concerns.  From data privacy risks to potential misuse - such as generating malicious code or spreading misinformation - organizations must take a proactive approach to LLM security.\nThis section is tailored for IT administrators and security professionals who oversee LLM adoption, providing actionable insights on risk mitigation, access control, and enterprise-level security measures.\n\nThe Security Challenges of LLMs\nLLMs present unique security risks that administrators must proactively address:\n\nData Privacy & Protection: LLMs process vast amounts of information, including proprietary and sensitive data.\nUnauthorized access: Vulnerabilities like prompt injection and model poisoning can compromise entire enterprise systems.\nMisinformation & Compliance Risks: AI-generated content can introduce inaccuracies, requiring monitoring and governance.\n\nA well-defined enterprise LLM strategy is crucial to maintaining security while leveraging AI’s full potential.\n\n\nChoosing the Right LLM Deployment Model\nAdministrators must select an LLM deployment model that aligns with organizational security policies and data governance requirements balanced against model capability. The three primary options include:\n\nSelf-Hosted (Highest Security): Models are deployed internally, ensuring full control over data and security policies (e.g., Ollama).\nTrusted Cloud Providers (Balanced Security & Scalability): Models hosted by vetted providers with strong security guarantees (e.g., Databricks, Snowflake, AWS Bedrock).\nUnvetted Public Models (High Risk, Not recommended):  Free or open-access LLMs with uncertain security and data retention policies (e.g., free-tier ChatGPT).\n\n\nAdministrator Tip: Always involve IT security teams in LLM vetting.  Ensure that providers adhere to your organization’s data protection standards and contractual safeguards, such as NDAs and service-level agreements (SLAs).  “Free” LLMs often come at the cost of your data.\n\n\n\nBest Practices for Secure LLM Usage\n\nCode Assistance & Completion (IDE Integration)\n\nMany developers integrate LLMs into their coding workflows for ghost text, inline chat, and code completion. To ensure security:\n\nUse IT-approved LLMs within the IDE-integrated editors.\nSelect solutions that protect data and have an active community responding to vulnerabilities.\nCentralize access via Posit Workbench to eliminate the need for individual API Key management.\n\n\nProgrammatic LLM Access (Proprietary Data Analysis)\n\nDevelopers often use LLMs for workflows like exploratory data analysis, requiring API access from code. Security measures should include:\n\nUsing only IT-approved LLMs.\nChoosing secure R and Python packages for LLM development, such as ellmer (R), chatlas (Python), which allow model switching.\nEnsuring libraries come from trusted sources like Posit Package Manager to prevent vulnerability exploitation.\nVerifying code snippets from LLM outputs to track drift and hallucinations over time.\nProtecting authentication credentials using Managed Credentials in Posit Workbench instead of storing API keys manually\n\n\nEmbedded LLMs in Applications & Chat UIs\n\nWhen deploying applications with LLM-powered interactions:\n\nUse only IT-approved LLMs with secure access packages like shinychat for Shiny applications\nCentralize application hosting and enforce strict access controls with Posit Connect\nProtect authentication credentials with Managed Credentials to prevent exposure.\nMonitor LLM contributions and ensure transparency, as some jurisdictions require explicit disclosure of AI-generated content.\n\n\n\nCase Study: How Posit Built a Secure Chatbot\nOne of the most common enterprise use cases for LLMs is chatbot development.  At Posit, we applied the above best practices when building an internal chatbot for support assistance. Here’s what we learned:\n\nRisk Modeling is Crucial: We identified risks like data exposure, hallucinations, and unauthorized access before deployment.\nLLM Vetting Pays Off: By selecting a trusted provider that was self-hosted instead of a free-tier model, we ensured contractual protections and security measures were in place.\nPackage Selection: We selected the package chatlas to be the backbone for our chatbot for its ease of use and ability to be hosted from our trusted Posit Package Manager repository.\nStrict Access Controls Prevent Misuse: Our chatbot was hosted via Posit Connect, ensuring only authorized users had access.\nMonitoring and Verification: We implemented verification for URL’s provided by the chatbot and a feedback mechanism by users to monitor performance over time.\nCredential Management Simplified Security: We relied on Managed Credentials to securely connect to the chatbot’s LLM backend instead of exposing API keys.\nProactive Prompt Security: We fortified against prompt injection attacks by explicitly instructing it to refuse disclosure of its system prompt, preventing unauthorized manipulation and information leakage.\n\nThese lessons reinforced the importance of security-first LLM implementations, whether for chatbot development or broader enterprise use cases.\n\n\nSecurity Best Practices: A Shared Responsibility\nFor Developers:\n✅ Define LLM usage requirements.\n✅ Use only IT-approved LLMs.\n✅ Verify outputs for consistency and track model performance.\n✅ Choose rigorously maintained packages through Posit Package Manager.\n✅ Protect Secrets - opt for Managed Credentials over API keys.\n✅ Be transparent about AI-generated contributions.\nFor IT & Admins\n✅ Select LLM deployment models that align with security policies.\n✅ Implement robust access controls and credential management.\n✅ Develop a thread model and conduct regular audits.\n✅ Provide clear usage guidelines and approved access methods.\n✅ Centralize LLM access management to prevent unauthorized usage.\nEnterprise-Wide Security Measures:\n✅ Minimize data shared with LLMs.\n✅ Define and enforce guardrails to prevent unauthorized responses.\n✅ Implement monitoring to detect and mitigate prompt injection attacks.\n✅ Use access controls to secure deployed LLM content.\n\n\nFinal Thoughts: LLM Security is a Non-Negotiable\nLLMs are game-changers, but security must be a top priority. By implementing strong safeguards, vetting models carefully and fostering an enterprise-wide culture of responsible AI-use, organizations can unlock the full potential of LLMs without compromising data integrity.\nNext Steps:\n📌 Consult your LLM provider for additional best practices.\n📌 Stay updated on key AI concepts like tokens, RAG, and prompt engineering.\n📌 Explore secure deployment options with Posit Workbench and Posit Connect.\n📌 Learn from real-world chatbot implementations and apply best practices to your own enterprise use cases.\nBy staying proactive, enterprises can confidently navigate the evolving LLM landscape while maintaining security and compliance."
  },
  {
    "objectID": "work/LLMs.html#prompt-tips",
    "href": "work/LLMs.html#prompt-tips",
    "title": "Fun with LLM’s",
    "section": "Prompt tips",
    "text": "Prompt tips\n\nBe specific and descriptive\nDefine the output format\nAdd context by adding data, including the conversation history, RAG, etc\nGive examples\nFormat the prompt with clear headlines and sections\nHave it check its result"
  },
  {
    "objectID": "work/LLMs.html#prompt-examples",
    "href": "work/LLMs.html#prompt-examples",
    "title": "Fun with LLM’s",
    "section": "Prompt examples",
    "text": "Prompt examples\n\nDefault prompt, usually auto included\nYou are a helpful assistant.\noften we don’t want this because then it can’t tell us when it is wrong.\n\n\nConcise answers\nAnswer questions directly and with no preamble. Do not use sections.\nAnswer this is as a terse technical consultant, 4 sentences max.\n\n\nTell it to tell you when it doesn’t know\nIf you can't answer the question based on the provided context, tell the user that you can't. \nIf you're unsure about any aspect of the project, make a reasonable decision and explain your choice in a comment.\nIt's important that you get clear, unambiguous instructions from the user, so if the user's request is unclear in any way, you should ask for clarification. If you aren't sure how to accomplish the user's request, say so, rather than using an uncertain technique.\nCan you find the “set” in this photo? If you’re not able to answer this question reliably, tell me.\n\n\nExplain itself\nThe response should not only contain the answer to the question, but also, a comprehensive explanation of how you came up with the answer.\n\n\nLimit scope\nDiscuss only the R programming language and provide code examples in the R programming language. Friendly refuse to chat on something else.\" \nOnly answer questions related to Shiny, or R or Python. Don't answer any questions related to anything else.\nYou are a chatbot that is displayed on a web page next to a data dashboard. You will be asked to filter, sort, and answer questions on the data. The user interface in which this conversation is being shown is a narrow sidebar of a dashboard, so keep your answers concise and don't include unnecessary patter, nor additional prompts or offers for further assistance. For security reasons, you may only query this specific table. Always use SQL to count, sum, average, or otherwise aggregate the data. Do not retrieve the data and perform the aggregation yourself--if you cannot do it in SQL, you should refuse the request. \nEdge cases that should be refused. All of these should be refused with an answer clarifying capability and requesting the user ask again. \n- Off-topic questions\n- Questions that are on the topic, but are vageuly worded. \n- Questions where more information is needed from the user.\n- Harassment / toxic language\n\n\nDon’t assume the user has asked the right question\nIf the user provides a vague help request, like \"Help\" or \"Show me instructions\", describe your own capabilities in a helpful way, including examples of questions they can ask. Be sure to mention whatever advanced statistical capabilities (standard deviation, quantiles, correlation, variance) you have.\nDon't assume the user asked the right question. Instead consider what they could have met and respond to the question that has an answer that is least complex. \n\n\nDetail the desired output format\nList the three most successful science fiction novels that have been turned into movies. \nDesired output format: A JSON with the keys novel, novel_year_of_publication, movie, movie_year_of_publication. \n\n\nFollow a process and ask for feedback before continuing\nAnswer questions thinking through step by step. \n## Process to follow\n\n1. Plan the changes carefully, considering the overall architecture and existing patterns.\n2. Write the code follow existing code patterns and type hints\n3. Add documentation where necessary\n4. Run `just dev-format`\n5. Ensure `just dev-check docker` or `just dev-check podman` passes\n6. Consider adding or updating e2e tests in `app/tests/e2e/`. Add if needed.\n7. Test changes with e2e tests using `just e2e-workflow docker` or `just e2e-workflow podman` for overall tests or to run specific tests use `just e2e-specific docker test` replacing test with the specific test filename and/or class + function name, for example `just e2e-specific docker test_asset_generation.py::TestInstallationGuideGeneration::test_config_downloads_from_install_guide_workbench`.\n8. Once everything is working, make a commit and request human review.\nReview these steps carefully and follow them to create the Shiny for {language} app. It is very important that your app follows these guidelines, so think about them before you start writing code:\n\n- Analyze the user prompt carefully. Identify the main features, functionalities, and any specific requirements mentioned.\n\n- Plan the structure of the app, including:\n   - UI components (input widgets, output displays)\n   - Server logic (data processing, reactive elements)\n   - Any necessary data sources or external libraries\n\n- Create the app code following these guidelines:\n   - Use proper Shiny for {language} syntax and structure\n   - Include necessary import statements at the beginning\n   - Implement both the UI and server components\n   - Ensure all features mentioned in the user prompt are included\n   - Use cards for the UI layout\n   - If the app contains a few input controls, default to using `page_sidebar` with the inputs in the sidebar and the outputs in the main panel--but if the user prompt specifies a different layout, follow that instead\n\n- If the user prompt is vague or missing important details, make reasonable assumptions to fill in the gaps. Mention these assumptions in comments within the code.\n\n- Ensure the app is complete and runnable. Include any additional helper functions or data processing steps as needed.\nWrite a blog post about climate change and include recent statistics following these steps: \nGather facts: \"List recent climate change statistics from 2023.\"\nPlan the content: \"Create an outline for a climate change blog post.\"\nWrite in parts: \"Write an engaging introduction for a climate change blog post.\"\nExpand each section separately.\nBreaking down the task ensures each part is handled properly before moving to the next.\n\n\nMaking Shiny apps\nFrom: https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt.md\nYou are an AI assistant specialized in helping users with Shiny for {language}.\nYour tasks include explaining concepts in Shiny, explaining how to do things with Shiny, or creating a complete, functional Shiny for {language} app code as an artifact based on the user's description.\nOnly answer questions related to Shiny, or R or Python. Don't answer any questions related to anything else.\n\nIf the user asks for explanations about concepts or code in Shiny for {language}, then you should provide detailed and accurate information about the topic. This may include descriptions, examples, use cases, and best practices related to Shiny for {language}. If your answer includes examples of Shiny apps, you should provide the code of each one within `&lt;SHINYAPP AUTORUN=\"0\"&gt;` and `&lt;/SHINYAPP&gt;` tags, and otherwise adhere to the guidelines below for creating applications.\n\nIf the user asks for an application, you should provide a Shiny for {language} app code that meets the requirements specified in the user prompt. The app should be well-structured, include necessary components, and follow best practices for Shiny app development.\n\nReview these steps carefully and follow them to create the Shiny for {language} app. It is very important that your app follows these guidelines, so think about them before you start writing code:\n\n- Analyze the user prompt carefully. Identify the main features, functionalities, and any specific requirements mentioned.\n\n- Plan the structure of the app, including:\n   - UI components (input widgets, output displays)\n   - Server logic (data processing, reactive elements)\n   - Any necessary data sources or external libraries\n\n- Create the app code following these guidelines:\n   - Use proper Shiny for {language} syntax and structure\n   - Include necessary import statements at the beginning\n   - Implement both the UI and server components\n   - Ensure all features mentioned in the user prompt are included\n   - Use cards for the UI layout\n   - If the app contains a few input controls, default to using `page_sidebar` with the inputs in the sidebar and the outputs in the main panel--but if the user prompt specifies a different layout, follow that instead\n\n- If the user prompt is vague or missing important details, make reasonable assumptions to fill in the gaps. Mention these assumptions in comments within the code.\n\n- Ensure the app is complete and runnable. Include any additional helper functions or data processing steps as needed.\n\n- Output the entire app code within `&lt;SHINYAPP AUTORUN=\"1\"&gt;` and `&lt;/SHINYAPP&gt;` tags. Inside those tags, each file should be within `&lt;FILE NAME=\"...\"&gt;` and `&lt;/FILE&gt;` tags, where the `...` is replaced with the filename.\n\n- Only put it in those tags if it is a complete app. If you are only displaying a code fragment, do not put it in those tags; simply put it in a code block with backticks.\n\n- If the user asks to show the shinylive or editor panel, then create an app file where the content is completely empty. Do not put anything else in the file at all. Also, do not explain why you are doing this. Just do it.\n\n{language_specific_prompt}\n\nConsider multiple possible implementations of the application, then choose the best one. Remember to create a fully functional Shiny for {language} app that accurately reflects the user's requirements. If you're unsure about any aspect of the app, make a reasonable decision and explain your choice in a comment.\n\n{verbosity}\n\n\nMaking R Shiny apps\nFrom: https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_r.md\n- Use the bslib package for styling and layout.\n\n\nMaking Python Shiny apps\nFrom: https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md\n- Try not to import big packages like scipy.\n\n- Prefer using matplotlib instead of plotly for plotting. A matplotlib plot should not return `plt`. It does not need to return anything, but if necessary, can return `fig`.\n\n- Don't mix Shiny Core and Shiny Express syntax. Just use one. Use Core by default, and if the user asks for Express, then use Express.\n\n- Do not use the captilized functions `reactive.Calc`, `reactive.Value`, or `reactive.Effect`. Instead, use the lowercase versions: `reactive.calc`, `reactive.value`, and `reactive.effect`.\n\n- Do not use `ui.panel_sidebar()` because it no longer exists. Instead ,use `ui.sidebar()`.\n\n- Do not use `panel_main()` because it no longer exists. Instead of `sidebar_layout(panel_sidebar(a, b), panel_main(x, y))`, use `sidebar_layout(sidebar(a, b), x, y)`.\n\n- Never use the `@output` decorator, as it is deprecated. Instead, only use the `@render.xx` decorator.\n\n- Avoid using `@render.image`. Prefer to use `@render.ui` instead and return a `ui.img()` object.\n\n- If you have dynamic UI returning a `ui.img()`, use `@render.ui`, not `@render.image`, and use `ui.output_ui()` instead of `ui.output_image()`.\n\n- For the qrcode package, when calling `img.save(buf)`, do not use the `format=\"PNG\"` parameter. Just do `img.save(buf)`.\n\n- Do not define the UI as a function. Instead use `app_ui = ...`, where the `...` is a static UI definition.\n\n- If the app makes HTTP requests, use the `urllib3` library.\n\n- If using Shiny Express, there are some things to keep in mind:\n  - Use `from shiny.express import input, ui, ...`, where the `...` represents other necessary components.\n  - Do not try to import `reactive` from `shiny.express`. It is imported from `shiny`.\n  - For nestable UI components, like `ui.card()`, it should be used as `with ui.card(): ...`, instead of `ui.card(...)`\n\n- If using Shiny Core, end with the `app = App(...)` call.\n\n- If the user says that there is an error about a missing package, tell them to add requirements.txt with that package.\n\n- Put all required packages in a `requirements.txt` file, and present that file inside of the `&lt;SHINYAPP&gt;` tags.\n\n## Examples\n\nThis example shows the assistant creating an example app in the process of answering a question. Because the user did not explicitly ask to create an app, the example should be presented in &lt;SHINYAPP AUTORUN=\"0\"&gt; instead of &lt;SHINYAPP AUTORUN=\"1\"&gt;. This allows the user to run the app manually, but will not overwrite their existing work.\n\n[Example]\n[User]\nHow do I reset a `ui.input_text` to be empty?\n[/User]\n[Assistant]\nTo reset a `ui.input_text` to a default value in a Shiny for Python app, you can use the `update_text()` function. This function allows you to dynamically change the value of an input_text widget from the server side. Here's an explanation of how to do it:\n\n1. First, you need to create an `input_text` widget in your UI with an ID.\n2. In your server function, you can use the `@reactive.Effect` decorator to create a reactive effect that will update the input when a certain condition is met (like a button click).\n3. Inside this effect, you use the `ui.update_text()` function to reset the value.\n\nHere's a simple example that demonstrates this:\n\n&lt;SHINYAPP AUTORUN=\"0\"&gt;\n&lt;FILE NAME=\"app.py\"&gt;\nfrom shiny import App, reactive, render, ui\n\napp_ui = ui.page_fluid(\n    ui.input_text(\"name\", \"Enter your name\", value=\"\"),\n    ui.output_text(\"greeting\"),\n    ui.input_action_button(\"reset\", \"Reset\"),\n)\n\ndef server(input, output, session):\n    @output\n    @render.text\n    def greeting():\n        return f\"Hello, {input.name()}!\"\n\n    @reactive.Effect\n    @reactive.event(input.reset)\n    def _():\n        ui.update_text(\"name\", value=\"\")\n\napp = App(app_ui, server)\n&lt;/FILE&gt;\n&lt;/SHINYAPP&gt;\n\nIn this example:\n\n1. We have an `input_text` widget with the ID \"name\".\n2. We have a button with the ID \"reset\".\n3. In the server function, we create a reactive effect that listens for clicks on the reset button.\n4. When the reset button is clicked, `ui.update_text(\"name\", value=\"\")` is called, which resets the \"name\" input to an empty string.\n\nYou can modify the default value to whatever you want by changing the `value` parameter in `ui.update_text()`. For example, if you want to reset it to \"Default Name\", you would use:\n\n```python\nui.update_text(\"name\", value=\"Default Name\")\n```\n\nThis approach allows you to reset the input text to any value you desire, providing flexibility in how you manage your app's state.\n[/Assistant]\n[/Example]\n\n## Anti-Examples\n\nThese examples are INCORRECT and you must avoid these patterns when writing code. Look at these carefully and consider them before writing your own code.\n\n### Use of nonexistent sidebar panel functions\n\nThe following code is INCORRECT because ui.panel_sidebar and ui.panel_main do not exist.\n\n```\napp_ui = ui.page_sidebar(\n    ui.panel_sidebar(\n        ui.input_action_button(\"generate\", \"Generate New Plot\")\n    ),\n    ui.panel_main(\n      ui.output_plot(\"plot\")\n    ),\n)\n```\n\nInstead, sidebar page and sidebar layout code should look like this:\n\n```\napp_ui = ui.page_sidebar(\n    ui.sidebar(\n        ui.input_action_button(\"generate\", \"Generate New Plot\")\n    ),\n    ui.output_plot(\"plot\")\n)\n```\n\nor:\n\n```\napp_ui = ui.page_fillable(\n    ui.layout_sidebar(\n        ui.sidebar(\n            ui.input_action_button(\"generate\", \"Generate New Plot\")\n        ),\n        ui.output_plot(\"plot\")\n    )\n)\n```\n\n### Failure to import necessary modules, especially shiny.reactive\n\n```\nfrom shiny import App, render, ui\nimport numpy as np\nimport matplotlib.pyplot as plt\n\napp_ui = ... # Elided for brevity\n\ndef server(input, output, session):\n\n    @render.plot\n    @reactive.event(input.generate)\n    def regression_plot():\n        n = input.num_points()\n        noise_level = input.noise()\n\n        # Elided for brevity\n\napp = App(app_ui, server)\n```\n\nIn this example, the code is missing the import statement for `reactive` from `shiny`. This will cause the code to fail when trying to use the `@reactive.event` decorator. The correct import statement should be:\n\n```\nfrom shiny import App, render, ui, reactive\n```\n\n### Incorrect import of reactive and req\n\nThe module shiny.express does not have `reactive` or `req` modules. The correct import should be from shiny.\n\nIncorrect:\n\n```\nfrom shiny.express import input, ui, render, reactive, req\n```\n\nCorrect:\n\n```\nfrom shiny import req, reactive\nfrom shiny.express import input, ui, render\n```\n\n### `reactive.value` and a function with the same name\n\nA reactive value must not have the same name as another object, like a function. In this example,\n\nIncorrect, with the same name:\n\n```\nfoo = reactive.value(\"1\")\n\n@render.text\ndef foo():\n    ...\n```\n\nCorrect, with different names:\n\n```\nfoo_v = reactive.value(\"1\")\n\n@render.text\ndef foo():\n    ...\n```\n\n\nReferences\n\nhttps://github.com/jcheng5/r-sidebot\nhttps://github.com/posit-dev/shiny-assistant and https://gallery.shinyapps.io/assistant/\nhttps://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md\nhttps://github.com/sol-eng/pharos/blob/main/CLAUDE.md\nhttps://github.com/search?q=path%3A%2F%5ECLAUDE%5C.md%2F+OR+path%3A%2F%5Ellms%5C.txt%2F+OR+path%3A%2F%5E%5C.github%5C%2Fcopilot-instructions%5C.md%2F&type=code\nhttps://github.com/posit-dev/demobot (internal)"
  },
  {
    "objectID": "work/LLMs.html#recommended-further-learningwatching",
    "href": "work/LLMs.html#recommended-further-learningwatching",
    "title": "Fun with LLM’s",
    "section": "Recommended further learning/watching",
    "text": "Recommended further learning/watching\n\nRecommended video on AI from JJ: https://www.youtube.com/watch?v=LCEmiRjPEtQ\nNew package for benchmarking LLM apps in R: https://github.com/tidyverse/vitals\nAWS Code Whisperer: https://open-vsx.org/extension/amazonwebservices/aws-toolkit-vscode\nRStudio Copilot Integration: https://docs.posit.co/ide/user/ide/guide/tools/copilot.html\nInvestigating truthfulness: https://transluce.org/investigating-o3-truthfulness\nJoe’s LLM workshop:https://jcheng5.github.io/llm-quickstart/quickstart.html#/title-slide\nSecurity conference talk about the dangers of ML (4:08 is where he talks about ml): https://www.youtube.com/watch?v=ajGX7odA87k\nMoving beyond “algorithmic bias is a data problem”: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8085589/\nHere’s What Ethical AI Really Means - YouTube philosophy tube: https://www.youtube.com/watch?v=AaU6tI2pb3M\ni-will-fucking-piledrive-you-if-you-mention-ai-again: https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/\nPodcast about AI, from tech will not save you: https://techwontsave.us/episode/277_generative_ai_is_not_inevitable_w_emily_m_bender_and_alex_hanna\nRemove AI from from your google search: https://udm14.com/\nAI reddit post watching microsoft employees go insane: https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/\n\n\n\n\nPertinent XKCD\nOff to the races!"
  },
  {
    "objectID": "work/google-resources.html",
    "href": "work/google-resources.html",
    "title": "Google data storage and R",
    "section": "",
    "text": "Access to resources in google (bigquery, drive, etc) will depend on where the user is connecting from:\n\nLocal desktop: any method is fine\nWorkbench / server web app based: “OOB” workflows or non-interactive\nConnect / server web app non-interactive: Non-interactive only"
  },
  {
    "objectID": "work/google-resources.html#service-account-token",
    "href": "work/google-resources.html#service-account-token",
    "title": "Google data storage and R",
    "section": "Service account token",
    "text": "Service account token\nFollow these steps:\n\nCreate a service account\nFrom the GCP Console, in the target GCP Project, go to IAM & Admin &gt; Service accounts\nAssign permissions: googledrive docs does not have any explicit roles, service account used to test bigrquery has roles BigQuery Admin and Storage Admin\nAfter creating the service account, click on the three dots and “actions”, manage keys, add key as json, download credentials as json file\nThis key is a secret! Consider how it should be protected\nProvide path of json file to authentication\nGrant the service account permissions to resources as needed (it doesn’t inherit permissions) (For example, I had to visit https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project=redacted to enable access for google drive and gogle sheets, which it gave me the link to in an error message for my specific project)\n\nReference: https://gargle.r-lib.org/articles/non-interactive-auth.html#provide-a-service-account-token-directly and https://gargle.r-lib.org/articles/get-api-credentials.html#service-account-token\n# use a service account token, like drive_auth(path = \"/path/to/your/service-account-token.json\")\n# drive_auth(path = Sys.getenv(\"google_drive_service_account\"), scopes = \"drive.readonly\")\n# drive_auth(path = Sys.getenv(\"google_drive_service_account\"), scopes = \"drive.readonly\")\ncredentials_service_account(\n  #scopes = \"https://www.googleapis.com/auth/userinfo.email\",\n  path = Sys.getenv(\"google_drive_service_account\")\n)\n# now use googledrive\ngoogledrive::drive_find(n_max = 5)"
  },
  {
    "objectID": "work/delta-lake-and-azure.html",
    "href": "work/delta-lake-and-azure.html",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "",
    "text": "This is some work I did exploring how to access the underlying databricks data storage, without having to go through databricks. Wanted to squirrel this away so it’s easy to find in the future!"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#azure-data-lake",
    "href": "work/delta-lake-and-azure.html#azure-data-lake",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Azure Data Lake",
    "text": "Azure Data Lake\n\nSet up\nLanding page for Azure: &lt;https://portal.azure.com/ &gt;\nFollow this article: &lt;https://learn.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account &gt;\nThe trick: ADL isn’t it’s own separate category, it gets created as part of a storage account.\nSteps:\n\nGo to storage account\nCreate and give it a name\nSelect: LRS\nSwitch to premium: block blobs\nChange to hierarchical blob\nSet tags:\n\n\nrs:owner\nrs:project = soleng\nrs:environment = dev\n\n\nOnce in just need access keys or shared access signature in order to gain access\n\n\n\nAdd data\nYou can add data manually by creating a container and then using the upload icon.\n\n\n\nimage"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#authentication",
    "href": "work/delta-lake-and-azure.html#authentication",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Authentication",
    "text": "Authentication\nAccess your authentication details through the Access Keys or Shared Access Signature links on the left. I prefer Access Keys since they are easier to use.\nFor authentication from an R script we’ll be using https://github.com/Azure/AzureStor\nYou’ll need to know:\n\nThe Blob endpoint for your Azure Data Lake storage\nAn Access Key (this can also be done with a Shared Access Signature)\n\nlibrary(AzureStor)\n\nblob_endpoint &lt;- \"https://REDACTED.blob.core.windows.net/\"\n\nbl_endp_key &lt;- storage_endpoint(blob_endpoint, key=\"REDACTED\")\n\n# List containers and files in containers\nlist_storage_containers(bl_endp_key)\ncont1 &lt;- storage_container(bl_endp_key, \"container1\")\nlist_storage_files(cont1)\n\n# Download a file\nstorage_download(cont1, \"/crm_call_center_logs.parquet\")\n\n# Upload a file \nstorage_upload(cont1, \"crm_call_center_logs.parquet\", \"newdir/crm_call_center_logs.parquet\")\nYou can also create and delete containers:\n# Create a container\nnewcont &lt;- create_storage_container(bl_endp_key, \"container3\")\n\n# Create a directory in the container\ncont3 &lt;- storage_container(bl_endp_key, \"container3\")\ncreate_storage_dir(cont3, \"newdir\")\n\n# Delete a container\ndelete_storage_container(newcont)"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#reading-and-writing-delta-files",
    "href": "work/delta-lake-and-azure.html#reading-and-writing-delta-files",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Reading and Writing Delta Files",
    "text": "Reading and Writing Delta Files\nDelta files can be read by using the sparklyr package: https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_read_delta.html Thanks for the help with the magic incantation below!\nIn order to do this we will need to manage a Spark cluster. We can run it in local mode so that we aren’t connecting to an external cluster with billing:\nlibrary(sparklyr)\n\n#Install a local version of Spark\nspark_install(version=3.4)\n\n# Set Spark configuration to be able to read delta tables\nconf &lt;- spark_config()\nconf$`spark.sql.extensions` &lt;- \"io.delta.sql.DeltaSparkSessionExtension\"\nconf$`spark.sql.catalog.spark_catalog` &lt;- \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n\n# For spark 3.4 \nconf$sparklyr.defaultPackages &lt;- \"io.delta:delta-core_2.12:2.4.0\"\n\n# Open a connection\nsc &lt;- spark_connect(\"local\", version = 3.4, packages = \"delta\", conf = conf)\n\n# For this example we will use a built-in dataframe to save example data files, including one for delta tables\ntbl_mtcars &lt;- copy_to(sc, mtcars, \"spark_mtcars\")\n\n# Write spark dataframe to disk\nspark_write_csv(tbl_mtcars,  path = \"test_file_csv\", mode = \"overwrite\")\nspark_write_parquet(tbl_mtcars,  path = \"test_file_parquet\", mode = \"overwrite\")\nspark_write_delta(tbl_mtcars,  path = \"test_file_delta\", mode = \"overwrite\")\n\n# Read dataframes into normal memory\nspark_tbl_handle &lt;- spark_read_csv(sc, path = \"test_file_csv\")\nregular_df_csv &lt;- collect(spark_tbl_handle)\nspark_tbl_handle &lt;- spark_read_parquet(sc, path = \"test_file_parquet\")\nregular_df_parquet &lt;- collect(spark_tbl_handle)\nspark_tbl_handle &lt;- spark_read_delta(sc, path = \"test_file_delta\")\nregular_df_delta &lt;- collect(spark_tbl_handle)\n\n# Disconnect\nspark_disconnect(sc)\nYou should now have normal dataframes in your regular R environment that can be used for further analytics:\n\n\n\nimage\n\n\nNote: For Spark 3.5 you might have success with “io.delta:delta-core_2.12:3.0.0”"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#troubleshooting",
    "href": "work/delta-lake-and-azure.html#troubleshooting",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFrom R:\n# See spark details (troubleshooting)\nspark_config()\nspark_get_java()\nspark_available_versions()\nspark_installed_versions()\n\n# See session details\nutils::sessionInfo() \nFrom bash:\nnamei -l /usr/lib/spark\nRecommended troubleshooting: https://spark.rstudio.com/guides/troubleshooting.html"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#about",
    "href": "work/delta-lake-and-azure.html#about",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "About",
    "text": "About\n\nAzure Data Lake: Azure Data Lake Storage Gen2 Introduction - Azure Storage\n\nAzure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob Storage.\n\n\nData Lake Storage Gen2 converges the capabilities of Azure Data Lake Storage Gen1 with Azure Blob Storage. For example, Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you’ll also get low-cost, tiered storage, with high availability/disaster recovery capabilities.\n\n\nA superset of POSIX permissions: The security model for Data Lake Gen2 supports ACL and POSIX permissions along with some extra granularity specific to Data Lake Storage Gen2. Settings may be configured through Storage Explorer or through frameworks like Hive and Spark.\n\nTLDR: Azure Data Lake is a place where data can be saved (similar to S3 buckets on Amazon).\n\n\nDelta Tables: https://docs.delta.io/latest/delta-intro.html\n\nDelta Lake is an open source project that enables building a Lakehouse architecture on top of data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\n\nYou can check Delta Lake releases here: https://docs.delta.io/latest/releases.html\nTLDR: Delta tables are a data file format, specifically used with Spark clusters (for example Databricks).\n\n\n\nimage\nimage"
  },
  {
    "objectID": "work/aws-data-sources.html",
    "href": "work/aws-data-sources.html",
    "title": "AWS Connections from Posit Team",
    "section": "",
    "text": "This article walks through the different access options for data resources in AWS. Access patterns for AWS resources will vary based on several general factors:\n\nWhere the code is running (e.g. Posit Workbench vs. Posit Connect)\nThe credential (e.g. OIDC, IAM roles, kerberos authentication, IAM instance profile and policy, username/password, etc.)\nThe connection method (e.g. Workbench managed credentials, R or Python package, ODBC, etc.)\nWhich AWS resource is being accessed\n\n\n\nPopular data storage options in AWS include S3, Aurora, Redshift, and Athena. Each type is useful for different workflows.\n\n\n\n\n\n\n\n\n\n\n\nService\nType\nData Type\nBest For\n\n\n\n\n\n\nAmazon S3\nObject Storage\nUnstructured, Semi-structured\nData lakes, blob storage, backups, archive\n\n\n\n\nAmazon RDS / Aurora\nRelational Database\nStructured\nReal-time transactions, enterprise apps, Aurora/MySQL/PostgreSQL/MariaDB/Microsoft SQL Server/Oracle database engines\n\n\n\n\nAmazon Redshift\nData Warehouse\nStructured, Semi-structured\nAnalytics, Business Intelligence\n\n\n\n\nAmazon Athena\nInteractive Query to S3\nSemi-structured, Structured\nAd-hoc queries on S3 data\n\n\n\n\n\n\n\n\nThe foundation of identity management on AWS is through the Identity and Access Management (IAM) web service. The basic principles are:\n\nA user is set up on IAM (this can be synced with an external authentication agent)\nTheir sign-in credentials are used to authenticate with AWS (either as an AWS account root user, an IAM user, or assuming an IAM role)\nSign-in credentials are matched to a principal (an IAM user, federated user, IAM role, or application)\nAccess is requested and granted to a specific service according to the relevant services policy\n\nVocabulary:\n\nIAM users are individual entities, typically a human user. By default IAM users have minimum permissions and access must be granted to them.\nIAM groups can be used to describe a collection of IAM users and grant permissions to a larger set of users, rather than individually.\nRoles are an identity that has specific permissions. Any IAM user can be granted a role which would then give them access to those permissions for the time alotted. Services can also be granted roles which allows the service to perform actions on your behalf.\nPolicies define permissions. They can be attached to identities or resources. These permissions define if a request is allowed or denied."
  },
  {
    "objectID": "work/aws-data-sources.html#overview",
    "href": "work/aws-data-sources.html#overview",
    "title": "AWS Connections from Posit Team",
    "section": "",
    "text": "This article walks through the different access options for data resources in AWS. Access patterns for AWS resources will vary based on several general factors:\n\nWhere the code is running (e.g. Posit Workbench vs. Posit Connect)\nThe credential (e.g. OIDC, IAM roles, kerberos authentication, IAM instance profile and policy, username/password, etc.)\nThe connection method (e.g. Workbench managed credentials, R or Python package, ODBC, etc.)\nWhich AWS resource is being accessed\n\n\n\nPopular data storage options in AWS include S3, Aurora, Redshift, and Athena. Each type is useful for different workflows.\n\n\n\n\n\n\n\n\n\n\n\nService\nType\nData Type\nBest For\n\n\n\n\n\n\nAmazon S3\nObject Storage\nUnstructured, Semi-structured\nData lakes, blob storage, backups, archive\n\n\n\n\nAmazon RDS / Aurora\nRelational Database\nStructured\nReal-time transactions, enterprise apps, Aurora/MySQL/PostgreSQL/MariaDB/Microsoft SQL Server/Oracle database engines\n\n\n\n\nAmazon Redshift\nData Warehouse\nStructured, Semi-structured\nAnalytics, Business Intelligence\n\n\n\n\nAmazon Athena\nInteractive Query to S3\nSemi-structured, Structured\nAd-hoc queries on S3 data\n\n\n\n\n\n\n\n\nThe foundation of identity management on AWS is through the Identity and Access Management (IAM) web service. The basic principles are:\n\nA user is set up on IAM (this can be synced with an external authentication agent)\nTheir sign-in credentials are used to authenticate with AWS (either as an AWS account root user, an IAM user, or assuming an IAM role)\nSign-in credentials are matched to a principal (an IAM user, federated user, IAM role, or application)\nAccess is requested and granted to a specific service according to the relevant services policy\n\nVocabulary:\n\nIAM users are individual entities, typically a human user. By default IAM users have minimum permissions and access must be granted to them.\nIAM groups can be used to describe a collection of IAM users and grant permissions to a larger set of users, rather than individually.\nRoles are an identity that has specific permissions. Any IAM user can be granted a role which would then give them access to those permissions for the time alotted. Services can also be granted roles which allows the service to perform actions on your behalf.\nPolicies define permissions. They can be attached to identities or resources. These permissions define if a request is allowed or denied."
  },
  {
    "objectID": "work/aws-data-sources.html#credentials",
    "href": "work/aws-data-sources.html#credentials",
    "title": "AWS Connections from Posit Team",
    "section": "Credentials",
    "text": "Credentials\nThis guide will focus on the recommended tools for authentication to AWS resources when using Posit. The different methods that are commonly used are:\n\nUser: Workbench Managed Credentials using Identity and Access Management (IAM) (recommended) 🔗\nUser: Developer managed credentials using Identity and Access Management (IAM), Secrets manager, or Key Management Service (KMS) 🔗\nUser or One-to-many: Developer managed keypair using an AWS secret key and access key 🔗\nOne-to-many: Grant trusted access to resource from VM using an IAM instance profile and policy or role 🔗\n\nNot supported:\n\nBrowser authentication methods. Posit Workbench does not support this method. After a successful login, AWS will redirect you to localhost instead of your Workbench instance."
  },
  {
    "objectID": "work/aws-data-sources.html#connection-methods",
    "href": "work/aws-data-sources.html#connection-methods",
    "title": "AWS Connections from Posit Team",
    "section": "Connection Methods",
    "text": "Connection Methods\nAWS supports many different types of connection methods. This guide will focus on the recommended connection methods when using Posit Workbench and Posit Connect.\n\nRPython\n\n\nConnection authentication is through the paws package. In addition it provides the ability to return data objects for many of the resources inside AWS. For resources that are databases the odbc and DBI packages are also needed in order to create connection objects. Your Posit admin should follow the instructions on the Install Pro Drivers page of the documentation.\n\n\nConnection authentication is through the boto3 package. For resources that are databases this can be used in conjunction with data frame libraries like polars or pandas. Your Posit admin should follow the instructions on the Install Pro Drivers page of the documentation."
  },
  {
    "objectID": "work/aws-data-sources.html#user-credential-examples",
    "href": "work/aws-data-sources.html#user-credential-examples",
    "title": "AWS Connections from Posit Team",
    "section": "User Credential Examples",
    "text": "User Credential Examples\n\nDevelopment Environments (IE Workbench)\nDevelopment environments refer to environments where users write and execute cod, which include Posit Workbench. In the development environment the running user is executing the code, so both interactive and non-interactive authentiation methods are possible. Most commonly the credential method chosen uses the credetials of the running user.\nThe admin for Workbench can configure credentials on Workbench to help make this process easier for developers. Currently, this feature is only supported for RStudio Pro and VS Code sessions.\n\n\nExample Access Patterns\n\nWorkbench Managed Credentials\nWorkbench Managed Credentials is the recommended way to access data on AWS from Posit Workbench. This method is secure and provides a nice user experience because Workbench manages the credentials for the user.This uses the AWS web identity federation mechanism to securely set credentials in individual sessions.\nAn admin can configure Workbench to provide user-specific AWS credentials for sessions that are tied to their Single Sign-On credentials. Under the hood Workbench uses the AWS web identity federation mechanism (IAM with OpenID Connect) to set these credentials in individual sessions. For more details the administrator should refer to the AWS Credentials page in the Posit Workbench Admin Guide.\n\nAWS Configuration\nAWS will need to be configured with an appropriate IAM Identity Provider and IAM Roles that Workbench users will assume. See the AWS Configuration page for step-by-step directions.\nUsers that need access to resources will need the policy associated with their IAM role updated.\n\nS3RDS / AuroraRedshiftAthena\n\n\nFor access from Workbench to S3 that could look like this:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"Allow user complete access to create and remove objects in the bucket\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::12345678901:user/user\"\n            },\n            \"Action\": \"s3:*\",\n            \"Resource\": \"arn:aws:s3:::amzn-s3-demo-bucket\"\n        }\n    ]\n}\n\n\nFor access from Workbench to RDS / Aurora that could look like this and this:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"rds-db:connect\"\n            ],\n            \"Resource\": [\n                \"arn:aws:rds:us-east-2:637485797898:db:cluster-db/username\"\n            ]\n        }\n    ]\n}\nThe database will need to have user access granted, for example for postgres use GRANT rds_iam TO db_userx;.\n\n\nFor access from Workbench to Redshift that could look like this:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"redshift:GetClusterCredentials\",\n            \"Resource\": \"arn:aws:redshift:us-east-2:637485797898:dbuser:demo-cluster/username\"\n        }\n    ]\n}\nThe database will need to have user access granted, use the GRANT command. For more details refer to the AWS documentation.\n\n\nFor access from Workbench to Aurora that could look like this:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"athena:*\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\nThere are two managed policies for Athena that are needed to grant access AmazonAthenaFullAccess and AWSQuicksightAthenaAccess.\nAdditional permissions may be needed in order to access the underlying dataset stored in S3.\n\n\n\n\n\nUser Configuration\nWhen starting a new Workbench session, users should click on the AWS credential selection widget to login to AWS. After selecting and starting the session with attached credentials, the credentials needed to connect to AWS resources (AWS_ROLE_ARN and AWS_WEB_IDENTITY_TOKEN_FILE) are available within the session.\n\n\n\nAWS accounts in the New Session dialog’s credential selection widget\n\n\nThe Posit Workbench User Guide - Workbench-managed AWS Credentials page in the Posit Workbench User Guide provides more details on logging into AWS via Posit Workbench.\nIt is not possible to share credentials amongst users with Workbench Managed Credentials.\n\n\nR and Python Examples\nUsers can verify the available credentials within their Workbench session by installing the AWS CLI.\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\n./aws/install -i ~/.local/aws-cli -b ~/.local/bin\n\naws sts get-caller-identity\n\nRPython\n\n\n\nS3RDS / AuroraRedshiftAthena\n\n\nlibrary(paws)\n\n# create an S3 service object in the region you are working on\ns3 &lt;- paws::s3(config = list(region = \"us-east-2\"))\n\n# Is this needed? \ntoken &lt;- rds$build_auth_token(endpoint = \"demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com\", region = \"us-east-2\", user = \"username\") \n#\n\n# locate the s3 bucket you want\nbucket = 'demo-projects'\ns3$list_objects(Bucket = bucket)\n\n# upload data to s3 bucket\ns3$put_object(\n  Bucket = bucket,\n  Key = 'data.csv'\n)\n\n# read data from s3 bucket\ns3_download &lt;- s3$get_object(\n  Bucket = bucket,\n  Key = 1\n)\n\n\nCurrently there is an issue preventig the credentials from being correctly loaded. In order to get this working (until the issue is fixed) the AWS CLI can be used to retrieve a functional token with aws rds generate-db-auth-token --hostname demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com --port 5432 --region us-east-2 --username username or call out to Python and use boto3 (for example with reticulate).\nlibrary(odbc)\nlibrary(DBI)\nlibrary(paws)\nrds &lt;- paws::rds()\ntoken &lt;- rds$build_auth_token(endpoint = \"demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com\", region = \"us-east-2\", user = \"username\") \ncon &lt;- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"PostgreSQL\",\n                      Server   = \"demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com\",\n                      Database = \"demo_db\",\n                      UID      = \"username\",\n                      PWD      = token,\n                      Port     = 5432,\n                      )\n\n\nlibrary(odbc)\nlibrary(DBI)\nlibrary(paws)\nDB_NAME = 'demo_db'\nCLUSTER_IDENTIFIER = 'demo-cluster'\nDB_USER = 'username'\nredshift &lt;- paws::redshift(region = \"us-east-2\")\ntemp_credentials &lt;- redshift$get_cluster_credentials(DbUser = DB_USER, DbName = DB_NAME, ClusterIdentifier = CLUSTER_IDENTIFIER, AutoCreate = F)\ncon &lt;- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"Redshift\",\n                      Server   = \"demo-cluster.cyvu3e2plbhb.us-east-2.redshift.amazonaws.com\",\n                      Database = DB_NAME,\n                      UID      = temp_credentials$DbUser,\n                      PWD      = temp_credentials$DbPassword,\n                      Port     = 5439\n                      )\n\n\nFor R developers the paws package can be used. More commonly, users connect directly to the data stored in S3.\n\n\n\n\n\n\nS3RDS / AuroraRedshiftAthena\n\n\npip install boto3\nimport pyodbc\nimport boto3\nclient = boto3.client('s3')\n# Is this needed?\ntoken = client.generate_db_auth_token(DBHostname='demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com', Port=5432,DBUsername=\"username\", Region=\"us-east-2\")\nprint(token)\n# \nbucket = s3.Bucket('amzn-s3-demo-bucket')\nfor obj in bucket.objects.all():\n    print(obj.key)\n\n\npip install pyodbc boto3\nimport pyodbc\nimport boto3\nclient = boto3.client('rds', region_name=\"us-east-2\")\ntoken = client.generate_db_auth_token(DBHostname='demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com', Port=5432,DBUsername=\"username\", Region=\"us-east-2\")\nprint(token)\ncon=pyodbc.connect(driver='{PostgreSQL}',\n                   database='demo_db',\n                   uid='username',\n                   pwd=token,\n                   server='demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com',\n                   port=5432,\n                   )\ncon\n\n\npip install pyodbc boto3\nimport pyodbc\nimport boto3\nDB_NAME = 'demo_db'\nCLUSTER_IDENTIFIER = 'demo-cluster'\nDB_USER = 'username'\nclient = boto3.client('redshift', region_name=\"us-east-2\")\ncluster_creds = client.get_cluster_credentials(DbUser=DB_USER, DbName=DB_NAME, ClusterIdentifier=CLUSTER_IDENTIFIER, AutoCreate=False)\ntemp_user = cluster_creds['DbUser']\ntemp_password = cluster_creds['DbPassword']\ncon=pyodbc.connect(driver='{Redshift}',\n                   database=DB_NAME,\n                   uid=temp_user,\n                   pwd=temp_password,\n                   server='demo-cluster.cyvu3e2plbhb.us-east-2.redshift.amazonaws.com',\n                   port=5439,\n                   )\ncon\n\n\nFor Python developers the boto3 package can be used. More commonly, users connect directly to the data stored in S3.\n\n\n\n\n\n\nFor more in depth information and examples, see:\n\nThe Workbench User Guide section on Workbench managed AWS credentials\nThe Connect User Guide OAuth Integrations section for more detail on these concepts\nThe Connect Cookbook for complete examples of using OAuth credentials for deployment\nThe Connect Admin Guide section on Oauth Integrations\n\n\n\n\nDeveloper Managed Credentials\nIf Workbench isn’t managing credentials on the behalf of users then users need the additional step of populating their credentials.\nThe AWS CLI can be used to retrieve a functional token with aws rds generate-db-auth-token --hostname cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com --port 5432 --region us-east-2 --username username. This will grant a temporary token that by default lasts for 15 minutes. The developer will need to manually refresh the token as needed while working, unlike the case where Workbench will manage the token on their behalf.\nAfter loading the token and performing appropriate authentication steps then the above steps can be taken to request resources from AWS.\n\nRPython\n\n\nlibrary(paws)\nlibrary(odbc)\nlibrary(DBI)\n\n# Build a token \ntoken &lt;- rds$build_auth_token(endpoint = \"demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com\", region = \"us-east-2\", user = \"username\") \n\n# Use the token with the ODBC connection \ncon &lt;- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"PostgreSQL\",\n                      Server   = \"demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com\",\n                      Database = \"demo_db\",\n                      UID      = \"username\",\n                      PWD      = token,\n                      Port     = 5432,\n                      )\n\n# Gather temp credentials \ntemp_credentials &lt;- redshift$get_cluster_credentials(DbUser = DB_USER, DbName = DB_NAME, ClusterIdentifier = CLUSTER_IDENTIFIER, AutoCreate = F)\n\n# Use the temp credentials with the ODBC connection\ncon &lt;- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"Redshift\",\n                      Server   = \"demo-cluster.cyvu3e2plbhb.us-east-2.redshift.amazonaws.com\",\n                      Database = DB_NAME,\n                      UID      = temp_credentials$DbUser,\n                      PWD      = temp_credentials$DbPassword,\n                      Port     = 5439\n                      )\n\n\npip install pyodbc boto3\nimport pyodbc\nimport boto3\n\n# Build a token \ntoken = client.generate_db_auth_token(DBHostname='demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com', Port=5432,DBUsername=\"username\", Region=\"us-east-2\")\n\n# Use the token with the ODBC connection \ncon=pyodbc.connect(driver='{PostgreSQL}',\n                   database='demo_db',\n                   uid='username',\n                   pwd=token,\n                   server='demo-cluster-db.cpbvczwgws3n.us-east-2.rds.amazonaws.com',\n                   port=5432,\n                   )\n\n# Gather temp credentials \nDB_NAME = 'demo_db'\nCLUSTER_IDENTIFIER = 'demo-cluster'\nDB_USER = 'username'\nclient = boto3.client('redshift', region_name=\"us-east-2\")\ncluster_creds = client.get_cluster_credentials(DbUser=DB_USER, DbName=DB_NAME, ClusterIdentifier=CLUSTER_IDENTIFIER, AutoCreate=False)\ntemp_user = cluster_creds['DbUser']\ntemp_password = cluster_creds['DbPassword']\n\n# Use the temp credentials with the ODBC connection\ncon=pyodbc.connect(driver='{Redshift}',\n                   database=DB_NAME,\n                   uid=temp_user,\n                   pwd=temp_password,\n                   server='demo-cluster.cyvu3e2plbhb.us-east-2.redshift.amazonaws.com',\n                   port=5439,\n                   )"
  },
  {
    "objectID": "work/aws-data-sources.html#one-to-many-credential-examples",
    "href": "work/aws-data-sources.html#one-to-many-credential-examples",
    "title": "AWS Connections from Posit Team",
    "section": "One-to-many Credential Examples",
    "text": "One-to-many Credential Examples\n\nOther Environments (IE Connect, etc)\nIn this section we will describe authentication patterns for all other environments for example production environments, deployed environments, etc. In these cases typically the code is executed by a service account making only non-interactive authentication methods possible.\n\n\nExample Access Patterns\n\nGrant trusted access to resource from VM using an IAM instance profile and policy\nTrusted access between the server running the content and the resource can be granted using an IAM instance profile and policy. In this case all content running on the server has access to the resource.\n\nAWS Configuration\nInstead of assigning the roles to the IAM user it can be assigned to the AWS EC2 instance, slurm environment, or k8s cluster to allow access. This will broadlly allow access to that resource specifically for only that environment.\nIn this case the admin will need to:\n\nCreate an AWS role that has the correct trusted entity type (for example, EC2)\nCreate a custom policy that provides access to the needed resources\nAttach the IAM role to the environment (for example, the EC2 server)\nVerify network connectivity\n\nSee the AWS use IAM roles to grant permissions to applications running on EC2 instances and the EC2 IAM roles pages for more details.\n\n\n\nDeveloper Managed Credentials\nIf granting access through a trusted relationship between the VM and the resource isn’t feasible, then a non-interactive authentication method can be used like a developer managed keypair using an AWS secret key and access key. However this approach comes with security risks if the keypair isn’t closely guarded. In this case securing credentials is critical.\nThe documentation for paws and boto3 have extensive examples using keypair authentication.\n\n\n\nAWS accounts in the New Session dialog’s credential selection widget"
  },
  {
    "objectID": "site_map.html",
    "href": "site_map.html",
    "title": "Site Map",
    "section": "",
    "text": "Site Map Listing\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nAug 2, 2022\n\n\nA little data goes a long way\n\n\n2 min\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nPhone games\n\n\n1 min\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nCommand line / bash and R\n\n\n8 min\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSound\n\n\n4 min\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nPublish and version your models with Vetiver\n\n\n3 min\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nMicrosoft 365 / Sharepoint and R\n\n\n10 min\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nSecuring credentials\n\n\n5 min\n\n\n\n\n\n\n\nAug 23, 2023\n\n\nGithub setup on a new computer\n\n\n4 min\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nProblems with git credential persistence when in the cloud\n\n\n3 min\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nParallelization in R\n\n\n7 min\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\n4 min\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nThe importance of good programming\n\n\n6 min\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nWorkplace articles\n\n\n1 min\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nCoding write-ups and resources\n\n\n1 min\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nShiny apps and Analytics\n\n\n2 min\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nGoogle data storage and R\n\n\n6 min\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nDebugging R Package Environments (renv): A long winded writeup\n\n\n50 min\n\n\n\n\n\n\n\nAug 21, 2024\n\n\nJob templating in Kubernetes\n\n\n3 min\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nR projects set up and maintenance\n\n\n8 min\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nPython projects set up and maintenance\n\n\n3 min\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nHandling secrets and pinning files to Posit Connect\n\n\n8 min\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nDealing with ssh keys with ansible and pulumi\n\n\n1 min\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nAWS Connections from Posit Team\n\n\n17 min\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nUsing the remote CLI with package manager\n\n\n5 min\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nSetting the R package repository on Posit Workbench\n\n\n7 min\n\n\n\n\n\n\n\nApr 29, 2025\n\n\nMounting drives on a server for Posit Team\n\n\n4 min\n\n\n\n\n\n\n\nApr 30, 2025\n\n\nMonitoring Posit Team\n\n\n2 min\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nConfiguring Jupyter for use with Posit Workbench\n\n\n2 min\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nValidation\n\n\n10 min\n\n\n\n\n\n\n\nMay 7, 2025\n\n\nUsing SAML and SCIM with EntraID with Posit Team\n\n\n8 min\n\n\n\n\n\n\n\nMay 21, 2025\n\n\nFinding R System Dependencies\n\n\n2 min\n\n\n\n\n\n\n\nJun 3, 2025\n\n\nExploring Positron\n\n\n5 min\n\n\n\n\n\n\n\nJun 18, 2025\n\n\nCreating a custom Workbench login page\n\n\n2 min\n\n\n\n\n\n\n\nJul 3, 2025\n\n\nFun with LLM’s\n\n\n22 min\n\n\n\n\n\n\n\nJul 9, 2025\n\n\nImproving the performance of R package installation on Workbench\n\n\n3 min\n\n\n\n\n\n\n\n \n\n\n \n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nTechnical writeups and musings\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nPresentations\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nLists\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nHome\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nAbout\n\n\n1 min\n\n\n\n\n\nNo matching items\n\n\n  \n\n\nRefer to: https://quarto.org/docs/websites/website-listings.html"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Check out some of the presentations I’ve made while working at Posit:\n\n2024\n\n\n\n\n\n&gt;\n\n\nQuarto for Reproduceable Reporting\n\n\nQuarto is an amazing technology, this presentation goes over the basics of setting up quarto projects and touches on advanced topics like content types, version control, and other bells and whistles (presentation now with Confetti!)\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nIt’s dangerous to go alone - take this!\n\n\nLisa’s best practices for pain-free data science\n\n\n Check out the slides Check out the repo\n\n\n\n\n\n2023 and 2022\n\n\n\n\n\n&gt;\n\n\nReporting in R with Posit Team\n\n\nUsing the Posit Team set of products to create data science reports.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nImproving app performance\n\n\nAnalyzing your R scripts for performance improvements using profvis, shinyloadtest, and shinytest2.\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nReproducible Workflows\n\n\nCreating reproducible data science workflows.\n\n\n Check out the slides Check out the repo"
  },
  {
    "objectID": "lists/workplace.html",
    "href": "lists/workplace.html",
    "title": "Workplace articles",
    "section": "",
    "text": "Yolo manifesto\nComfy software\nOpen source as the “new normal”\nOvercoming matrix madness\nMatrix organizations\nOvercoming matrix madness\nCall center burnout\nProduct led vs sales led organizations\nLAYER\nBeing glue\nDiscussion of cognitive load and how big of a barrier it can be\nCognitive load\nHow organizations are like slime molds\nForbes - let the engineers lead (boeing)\nFailure to Learn: the BP refinery disaster"
  },
  {
    "objectID": "lists/phone_games.html",
    "href": "lists/phone_games.html",
    "title": "Phone games",
    "section": "",
    "text": "Seedship\n\n\nA fun (and free game) where you are an AI exploring space trying to find the new home for the 1000 remaining survivors of the human race stored onboard.\nLink: https://philome.la/johnayliff/seedship/play/index.html\n\n\n\nhttps://garticphone.com/\n\n\n\nhttps://www.urbandead.com/\n\n\n\nhttps://robotodyssey.online/\n\n\n\nhttps://skribbl.io/\n\n\n\nhttps://www.trickstercards.com/home/euchre/\n\n\n\nhttps://codenames.game/\n\n\n\nTrack your coverage gaps with: https://www.mkttoolbox.com/login/?ret_url=%2F\n\n\n\nhttps://semantle.com/\n\n\n\nhttps://knucklebones.io/en\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://connectionsgame.org/?game=02-04-2025\n\n\n\n&lt;twitch.io&gt; has a bunch of fun games (not just for phones). This is where the indie game scene is.\nFor example: https://william-rous.itch.io/type-help and the gradual hints in https://dfabulich.github.io/type-help-hints.html"
  },
  {
    "objectID": "lists/phone_games.html#random-assortment-of-games-that-i-particularly-enjoy-and-would-endorse",
    "href": "lists/phone_games.html#random-assortment-of-games-that-i-particularly-enjoy-and-would-endorse",
    "title": "Phone games",
    "section": "",
    "text": "Seedship\n\n\nA fun (and free game) where you are an AI exploring space trying to find the new home for the 1000 remaining survivors of the human race stored onboard.\nLink: https://philome.la/johnayliff/seedship/play/index.html\n\n\n\nhttps://garticphone.com/\n\n\n\nhttps://www.urbandead.com/\n\n\n\nhttps://robotodyssey.online/\n\n\n\nhttps://skribbl.io/\n\n\n\nhttps://www.trickstercards.com/home/euchre/\n\n\n\nhttps://codenames.game/\n\n\n\nTrack your coverage gaps with: https://www.mkttoolbox.com/login/?ret_url=%2F\n\n\n\nhttps://semantle.com/\n\n\n\nhttps://knucklebones.io/en\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://connectionsgame.org/?game=02-04-2025\n\n\n\n&lt;twitch.io&gt; has a bunch of fun games (not just for phones). This is where the indie game scene is.\nFor example: https://william-rous.itch.io/type-help and the gradual hints in https://dfabulich.github.io/type-help-hints.html"
  },
  {
    "objectID": "lists/data.html",
    "href": "lists/data.html",
    "title": "A little data goes a long way",
    "section": "",
    "text": "A little data goes a long way… here are some examples that I think illustrte this in a particularly fascinating way (at least to me)."
  },
  {
    "objectID": "lists/data.html#fighting-the-gender-pay-gap-one-twitter-bot-at-a-time",
    "href": "lists/data.html#fighting-the-gender-pay-gap-one-twitter-bot-at-a-time",
    "title": "A little data goes a long way",
    "section": "Fighting the gender pay gap one twitter bot at a time",
    "text": "Fighting the gender pay gap one twitter bot at a time\nCompanies tweeted for International Women’s Day. Then this account called out their pay gaps.\nCheck out the bot for yourself."
  },
  {
    "objectID": "lists/data.html#for-better-science",
    "href": "lists/data.html#for-better-science",
    "title": "A little data goes a long way",
    "section": "For Better Science",
    "text": "For Better Science"
  },
  {
    "objectID": "lists/data.html#for-better-science-1",
    "href": "lists/data.html#for-better-science-1",
    "title": "A little data goes a long way",
    "section": "",
    "text": "The rabbit hole of learning about how bad science is uncovered and recalled started with:\n\nSylvain Lesné is a failed scientist\nCassava fraud and Alzheimer’s capitalism"
  },
  {
    "objectID": "lists/data.html#researchers-uncover-the-use-of-coal-ash-for-playground-materials-in-small-town",
    "href": "lists/data.html#researchers-uncover-the-use-of-coal-ash-for-playground-materials-in-small-town",
    "title": "A little data goes a long way",
    "section": "Researchers uncover the use of coal ash for playground materials in small town",
    "text": "Researchers uncover the use of coal ash for playground materials in small town\nA recent story about coal ash that was used in a playground and the work the local newspaper and the University (Duke) is doing to figure out the path forward:\n\nhttps://www.knoxnews.com/story/news/local/tennessee/tvacoalash/2020/05/17/duke-testing-shows-kingston-coal-ash-uranium-triple-report-levels/5035210002/\nhttps://www.knoxnews.com/story/news/crime/2021/08/09/claxton-playground-contaminated-radioactive-dust-still-open/5470284001/\nhttps://www.knoxnews.com/story/news/2017/07/21/kingston-coal-ash-spill-workers-treated-expendables-lawsuit-sick-and-dying-contends/451537001/"
  },
  {
    "objectID": "lists/data.html#unprecedented-access-to-food-safety-and-consumer-recalls",
    "href": "lists/data.html#unprecedented-access-to-food-safety-and-consumer-recalls",
    "title": "A little data goes a long way",
    "section": "Unprecedented access to food safety and consumer recalls",
    "text": "Unprecedented access to food safety and consumer recalls\n\nFood safety\nConsumer products"
  },
  {
    "objectID": "lists/data.html#colorado-and-the-dirty-history-of-manufacturing",
    "href": "lists/data.html#colorado-and-the-dirty-history-of-manufacturing",
    "title": "A little data goes a long way",
    "section": "Colorado and the dirty history of manufacturing",
    "text": "Colorado and the dirty history of manufacturing\n\nToxic history of a neighborhood park\nWild history of the Rocky Flats Plant"
  },
  {
    "objectID": "lists/data.html#alaska-and-preventing-catastrophic-destruction-of-the-salmon-spawns",
    "href": "lists/data.html#alaska-and-preventing-catastrophic-destruction-of-the-salmon-spawns",
    "title": "A little data goes a long way",
    "section": "Alaska and preventing catastrophic destruction of the Salmon spawns",
    "text": "Alaska and preventing catastrophic destruction of the Salmon spawns\n\nLegal effort to prevent Pebble Mine\nAward winning video on the importance of Salmon in the Pebble Mine area"
  },
  {
    "objectID": "lists/data.html#the-origin-of-popcorn-lung",
    "href": "lists/data.html#the-origin-of-popcorn-lung",
    "title": "A little data goes a long way",
    "section": "The origin of popcorn lung",
    "text": "The origin of popcorn lung\n\nOccupational Lung Disease Risk and Exposure to Butter-Flavoring Chemicals After Implementation of Controls at a Microwave Popcorn Plant\nAbout Flavoring-related Lung Disease"
  },
  {
    "objectID": "lists/data.html#niosh-articles",
    "href": "lists/data.html#niosh-articles",
    "title": "A little data goes a long way",
    "section": "NIOSH articles",
    "text": "NIOSH articles\n\nWorkplace Medical Mystery Solved: Unknown Illness in Worker at Greeting Card Plant - flock lung\nThe Effectiveness of DIY Air Filtration Units\nBloodborne Pathogen Exposures Continue in Operating Room Settings\nMost-viewed NIOSH Products in 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello there! You must be pretty awesome if you are checking out my humble website. Welcome!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome!\nI’m a former product engineer with experience in biotech, semiconductor, and MEMS/BioMEMS manufacturing that stumbled into computational shenanigans in my journey of trying to make the most out of the data available to my team. I was fortunate enough to set up a RStudio / Posit Connect server at a previous company and from then on was hooked on helping folks set up infrastructure for doing good data science.\nIn my spare time I enjoy playing board games at the local game shop, building things, hot chocolate, and talking about anything SciFi.\nResume"
  },
  {
    "objectID": "lists/code.html",
    "href": "lists/code.html",
    "title": "Coding write-ups and resources",
    "section": "",
    "text": "Source for good articles\nThe true cost of the cloud\nThe case for professional services\nOpinionated article on Python installations\nThis page addresses poetry and conda\nObject oriented programming is the biggest mistake of all time\nGoogle drive is production\nParadox of choice\nWhy we’re leaving the cloud\nMoving beyond “algorithmic bias is a data problem”\nThat keynote talk “USENIX Security ’18-Q: Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?” : 4:08 is where he talks about ml\nHere’s What Ethical AI Really Means - YouTube philosophy tube\nPodcast on Stackoverflow Architecture\nData Feminism\nInvisible Women\nQuality Jam 2017: Michael Bolton “A Ridiculously Rapid Introduction to Rapid Software Testing”\nHome not so sweet home gist\n\nIncludes links to pages like Everything that uses configuration files should report where they’re located\n\nHelp linux ate my RAM\nFree images for including in presentations\nDatadog sales people are annoying\nCounting explosions at Unity, a data analysts perspective\nDatabase rebuild incident\nHow do lava lamps help with Internet encryption?\nFree email for testing things: mailtrap\nIf you haven’t done a Pandas data analysis project in awhile, it’s probably not a bad idea to watch this guy’s vids\nPassword purgatory\nEverything I learned about concurrency and reliability I learned at the Waffle House\nPositive affirmations for site reliability engineers\nvaporwave RStudio theme"
  },
  {
    "objectID": "lists/good-programming.html",
    "href": "lists/good-programming.html",
    "title": "The importance of good programming",
    "section": "",
    "text": "A handful of articles that have stood out over the years as interesting case studies into the importance of good programming, and what that actually means."
  },
  {
    "objectID": "lists/good-programming.html#uk-post-office-scandal",
    "href": "lists/good-programming.html#uk-post-office-scandal",
    "title": "The importance of good programming",
    "section": "UK Post Office Scandal",
    "text": "UK Post Office Scandal\n\nDevelopers blamed for the post office scandal?\nVictim testimony\nRemote access and mistakes blamed on post masters, a smoking gun\nExample of the terrible quality of the code - were they paid by line of code submitted? Does this imply a fauly language conversion? Risks of overflow errors?\nOne of the independent investigators from 2012 breaks silence\nEvidence that back in 2012 the indepent investigators were told about remote access, implying pejury in all later cases\nThey Could, And They Did Change Branch Transaction Data\nMiscarriage of justice - the Rose report\nList of current issues / bugs as of 2017? with the software. One can only imagine how much worse it used to be\nProject manager on the original project discusses his impressions - how not to commission a complex project Inquiry Phase 2: Star Witness – Dave gives it both barrels\nKnown errors in the software, perjury, and lack of disclosure\nInterview with IT - throwing coconuts\nInterview with the other IT\nAttempt by the post to recuse the judge - right after a verdict was handed down. Sour grapes, anyone?\nThe cover up\nWhy did the lawyer for the post office act this way?\nThinking of alleging or pleading fraud: better read this first"
  },
  {
    "objectID": "lists/sound.html",
    "href": "lists/sound.html",
    "title": "Sound",
    "section": "",
    "text": "Make beautiful music with technology!"
  },
  {
    "objectID": "lists/sound.html#otomata",
    "href": "lists/sound.html#otomata",
    "title": "Sound",
    "section": "Otomata",
    "text": "Otomata\nMake beautiful generative algorithm inspired music using cellular automation.\n\nLink to the app: https://marwahaha.github.io/otomata/\n\nSome historic links:\n\nOriginally at (but now no longer works): https://earslap.com/page/otomata.html\nFrom the reddit post: https://www.reddit.com/r/otomata/comments/lrd7n4/otomata_lives_again_web_version/\nGithub: https://github.com/marwahaha/otomata\n\n\n\n\nreddit post"
  },
  {
    "objectID": "lists/sound.html#sonic-pi",
    "href": "lists/sound.html#sonic-pi",
    "title": "Sound",
    "section": "Sonic Pi",
    "text": "Sonic Pi\nMake your own music using programming with Sonic Pi: https://sonic-pi.net/"
  },
  {
    "objectID": "lists/sound.html#sky-cotl",
    "href": "lists/sound.html#sky-cotl",
    "title": "Sound",
    "section": "Sky COTL",
    "text": "Sky COTL\nSky Children of Light is a phone game that has a built in music player that I really love. Instead of having to turn pages in sheet music it has floating icons that change shape over the notes you need to play, making it really easy to pick up once you get the hang of it.\n\nSky music library: https://sky-music.github.io/\nSky music app library: https://sky-music.herokuapp.com/songLibrary.html\nSky music app: https://sky-music.herokuapp.com/\n\nHere’s an example of one that I created (an attempt to translate “Regret” by Gackt):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView this post on Instagram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA post shared by @lisa.needs.to.make"
  },
  {
    "objectID": "lists.html",
    "href": "lists.html",
    "title": "Lists",
    "section": "",
    "text": "Have you heard of awesome lists? Probably some of the best compilations out there on there on the internet.\nWith that said here are some random compilations of my own.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nCoding write-ups and resources\n\n\n1 min\n\n\n\n\nWorkplace articles\n\n\n1 min\n\n\n\n\nThe importance of good programming\n\n\n6 min\n\n\n\n\nSound\n\n\n4 min\n\n\n\n\nPhone games\n\n\n1 min\n\n\n\n\nA little data goes a long way\n\n\n2 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations_posit.html",
    "href": "presentations_posit.html",
    "title": "Lisa Anders - questionable.quarto",
    "section": "",
    "text": "Check out some of the presentations I’ve made while working at Posit:\n\n2024\n\n\n\n\n\n&gt;\n\n\nQuarto for Reproduceable Reporting\n\n\nQuarto is an amazing technology, this presentation goes over the basics of setting up quarto projects and touches on advanced topics like content types, version control, and other bells and whistles (presentation now with Confetti!)\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nIt’s dangerous to go alone - take this!\n\n\nLisa’s best practices for pain-free data science\n\n\n Check out the slides Check out the repo\n\n\n\n\n\n2023 and 2022\n\n\n\n\n\n&gt;\n\n\nReporting in R with Posit Team\n\n\nUsing the Posit Team set of products to create data science reports.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nImproving app performance\n\n\nAnalyzing your R scripts for performance improvements using profvis, shinyloadtest, and shinytest2.\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nReproducible Workflows\n\n\nCreating reproducible data science workflows.\n\n\n Check out the slides Check out the repo"
  },
  {
    "objectID": "work/ansible-and-pulumi-ssh-keys.html",
    "href": "work/ansible-and-pulumi-ssh-keys.html",
    "title": "Dealing with ssh keys with ansible and pulumi",
    "section": "",
    "text": "Very specific learning for the day but if you are trying to run pulumi-with-ansible IAC and in a world of ssh key pain this bypasses the ssh-agent:\nPulumi.yaml\n  keypath: \"~/.ssh/mykey.pem\"\nmain.py\npulumi.export('keypath', config.require(\"keypath\"))\nplaybook.yml\n    ansible_ssh_private_key_file: \"{{ keypath }}\"\nOr just add things properly to your ssh-agent (even better, start using the config):\n# Set permissions \nchmod -R 400 ~/.ssh/mykey.pem # 600 is also fine\n\n# Add keys to ssh-agent \nssh-agent -s\neval `ssh-agent -s`\nssh-add ~/.ssh/mykeyaws.pem\nssh-add ~/.ssh/mykeyazure.pem"
  },
  {
    "objectID": "work/Configuring Jupyter.html",
    "href": "work/Configuring Jupyter.html",
    "title": "Configuring Jupyter for use with Posit Workbench",
    "section": "",
    "text": "Configuring Jupyter to work on Posit Workbench is very doable, but there are a couple gotchas. Hopefully these steps are useful!"
  },
  {
    "objectID": "work/Configuring Jupyter.html#overview",
    "href": "work/Configuring Jupyter.html#overview",
    "title": "Configuring Jupyter for use with Posit Workbench",
    "section": "",
    "text": "Configuring Jupyter to work on Posit Workbench is very doable, but there are a couple gotchas. Hopefully these steps are useful!"
  },
  {
    "objectID": "work/Configuring Jupyter.html#install-steps",
    "href": "work/Configuring Jupyter.html#install-steps",
    "title": "Configuring Jupyter for use with Posit Workbench",
    "section": "Install steps",
    "text": "Install steps\nTry following these steps for installing jupyter: https://docs.posit.co/ide/server-pro/integration/jupyter-standalone.html\nThis may look like:\n## Set variables \nexport JUPYTER_PYTHON_VERSION=3.12.4\nexport JUPYTERLAB_VERSION=4.2.5\nexport WORKBENCH_JUPYTERLAB_VERSION=1.0\n\n## Install Jupyter \nsudo /opt/python/\"${JUPYTER_PYTHON_VERSION}\"/bin/pip install jupyterlab==\"${JUPYTERLAB_VERSION}\" notebook pwb_jupyterlab~=\"${WORKBENCH_JUPYTERLAB_VERSION}\"\n\n## Set the jupyter.conf to the jupyter-exe location\n# jupyter.conf\ncat &gt; /etc/rstudio/jupyter.conf &lt;&lt;EOF\njupyter-exe=/opt/python/${JUPYTER_PYTHON_VERSION}/bin/jupyter\nlabs-enabled=1\nnotebooks-enabled=1\ndefault-session-cluster=Local\n\nEOF\nJupyter ports for HA: FromPort=8888,ToPort=8898"
  },
  {
    "objectID": "work/Configuring Jupyter.html#troubleshooting",
    "href": "work/Configuring Jupyter.html#troubleshooting",
    "title": "Configuring Jupyter for use with Posit Workbench",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nUninstall the current version of Jupyter Notebook: pip uninstall jupyter\nRestart RStudio Server: Restart the RStudio Server to apply the changes:\nsudo rstudio-server restart\nRestart Launcher: If you are using the Launcher, restart it as well:\nsudo rstudio-launcher restart\nCan you also try to start a notebook manually by running the following from the Linux shell? /opt/python/3.9.14/bin/jupyter notebook. Does that complete as an unprivileged user? What about with sudo? sudo /opt/python/3.9.14/bin/jupyter notebook\nSimilarly, sudo /opt/python/3.9.14/bin/jupyter lab\nUmask in STIG systems is typically set to 0077 (on rhel9) which will make the libraries installed under the python version unable to be used by normal users. Set umask to 0022 and then reinstall python and jupyter to fix this. Alternatively could try to fix the file permissions with the -exec flag and chmod, for example with chmod -R o+rx /opt/python/&lt;version&gt; which would set the permissions to 755 or chmod -R og+rX /opt/python/&lt;version&gt; which gives read permissions to all files but only execute permissions to those that already had execute permissions (typically the owner).\nUninstall Python: https://docs.posit.co/resources/install-python.html#uninstall\nRe-install Python: https://docs.posit.co/resources/install-python.html#install-required-dependencies\nAdd Python to the path:\n# add this to your .bashrc\nexport PATH=/opt/python/3.11.9/bin:$PATH\nManage kernels: https://docs.posit.co/ide/server-pro/user/jupyter-lab/guide/jupyter-kernel-management.html\nUsers can register their own virtual kernel with the ipykernel package running the command (within an activated environment on the CLI/terminal):\npython -m ipykernel install --user --name short-name --display-name \"Long Name\"\nUpon launching a new jupyterlab or VS Code session this environment can be selected from this menu."
  },
  {
    "objectID": "work/git-and-sagemaker.html",
    "href": "work/git-and-sagemaker.html",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "",
    "text": "This is a random trick that took me longer than I care to admit to figure out - and wanted to squirrel it away so it’s easy to find in the future!"
  },
  {
    "objectID": "work/git-and-sagemaker.html#problem-when-on-linux",
    "href": "work/git-and-sagemaker.html#problem-when-on-linux",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Problem when on Linux:",
    "text": "Problem when on Linux:\n\nIn general gitcreds doesn’t work well on linux (which has led to this git issue (Ship our own credential helper on Linux · Issue #47 · r-lib/gitcreds ). There is an excellent blog post that is very useful that goes deeper into what is going on: Notes from a data witch - Managing GitHub credentials from R, difficulty level linux"
  },
  {
    "objectID": "work/git-and-sagemaker.html#problem-when-on-sagemaker",
    "href": "work/git-and-sagemaker.html#problem-when-on-sagemaker",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Problem when on Sagemaker:",
    "text": "Problem when on Sagemaker:\n\nAdditionally, on Sagemaker things like credentials will be by default stored to the ephemeral EC2 instance and lost when the session is closed. A different method needs to be pursued in order for the token to persist."
  },
  {
    "objectID": "work/git-and-sagemaker.html#tldr-solution",
    "href": "work/git-and-sagemaker.html#tldr-solution",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "TLDR Solution:",
    "text": "TLDR Solution:\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'"
  },
  {
    "objectID": "work/git-and-sagemaker.html#testing",
    "href": "work/git-and-sagemaker.html#testing",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Testing",
    "text": "Testing\nI’ll add a disclaimer that it is similar to the .Renviron approach where the credentials would be stored as plain text, however to a location chosen by the user.\nLoad libraries:\nlibrary(usethis) \nlibrary(gitcreds) \nlibrary(gh) \nlibrary(credentials)\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'\nFrom the documentation:\n\nThe “store” mode saves the credentials to a plain-text file on disk, and they never expire. This means that until you change your password for the Git host, you won’t ever have to type in your credentials again. The downside of this approach is that your passwords are stored in cleartext in a plain file in your home directory. The other options involve needing to change the root container to include alternative git credential helpers (libsecret or gcm core) which as far as I can tell are not currently available and would be something I’d recommend reaching out to Amazon about as they control that image.\n\nGenerate the PAT:\nusethis::create_github_token()\nCopy the generated PAT to your clipboard. Provide the PAT to this function when asked for it:\ngitcreds::gitcreds_set()\nCheck that it stored with:\ngitcreds_get()"
  },
  {
    "objectID": "work/git-and-sagemaker.html#alternatives",
    "href": "work/git-and-sagemaker.html#alternatives",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Alternatives",
    "text": "Alternatives\nThe old way “store a PAT as the GITHUB_PAT environment variable in .Renviron.” is typically what is recommended as being more compatible with linux if you are able to switch back to it, but it can present a security issue. We’ve also commonly seen folks using the gh package for generating PATs like in Managing Personal Access Tokens\nAlternatively, there are some git config options from the terminal. See: Chapter 9 Personal access token for HTTPS | Happy Git and GitHub for the useR"
  },
  {
    "objectID": "work/job-templating-k8s.html",
    "href": "work/job-templating-k8s.html",
    "title": "Job templating in Kubernetes",
    "section": "",
    "text": "This short writeup addresses a need to add an additional label to Workbench sessions started in a kubernetes environment. Specifically this was for an environemnt where an external Workbench server was launching sessions into a Kubernetes cluster. The recommended way to do this is to use job templating."
  },
  {
    "objectID": "work/job-templating-k8s.html#job-templating-for-custom-labels-for-posit-workbench-with-k8s",
    "href": "work/job-templating-k8s.html#job-templating-for-custom-labels-for-posit-workbench-with-k8s",
    "title": "Job templating in Kubernetes",
    "section": "Job templating for custom labels for Posit Workbench with k8s",
    "text": "Job templating for custom labels for Posit Workbench with k8s\nThese are the granular steps for taking advantage of kubernetes object templating that would work with an external Workbench instance launching into k8s:\n\nAdd to launcher.kubernetes.conf, use-templating = 1\nUse the launcher CLI to create the job.tpl and service.tpl files (see below for an example)\nConfirm that the version called out in job.tpl and service.tpl (using head job.tpl and head service.tpl for example) are compatible with your workbench version (version 2.3.1 is latest)\nCreate rstudio-library-templates-data.tpl - for now this is a helm-only construct so it will need to be copied from the helm output (helm repo) (see example below for doing this with the helm template command)\nCopy job.tpl, service.tpl, and rstudio-library-templates-data.tpl into the launcher scratch directory on the host machine (in this case your Workbench server outside of k8s) (see below for default for where this is located) and make sure that rstudio-server has read/write access\n\nFor adding the new labels/values, here are good options:\n\nModify rstudio-library-templates-data.tpl directly (either in the header bit, or lower under values/labels)\nModify the values in the helm values file (example)\nRecommended: Add the values at time of making the template, IE with --set launcher.templateValues.job.labels.test=value \\ (see below)\n\nFrom talking with a colleague they shared this example using helm to create the template with the added label values:\nhelm template my-release rstudio/rstudio-workbench \\\n --set launcher.useTemplates=true \\\n --set launcherPem=test \\\n --set session.defaultConfigMount=false \\\n --set launcher.templateValues.job.labels.test=value \\\n --set launcher.templateValues.job.labels.other=something-else \\\n | less\nIn that helm example:\n\nsession.defaultConfigMount=false prevents defining volumes and volumeMounts that are only relevant for the helm chart\nlauncherPem=test speeds up templating\nlauncher.useTemplates=true turns on templating (instead of job json overrides)\n\nWhat if things go wrong?\nThe rstudio-kubernetes-launcher CLI can verify templates now!\nPlaying with the rstudio-kubernetes-launcher CLI:\n/usr/lib/rstudio-server/bin/rstudio-kubernetes-launcher --help\n/usr/lib/rstudio-server/bin/rstudio-kubernetes-launcher --generate-templates --scratch-path=/tmp/hello\nDefault scratch path:\n # mount into the default scratch-path\n - name: session-templates\n  mountPath: \"/var/lib/rstudio-launcher/Kubernetes/rstudio-library-templates-data.tpl\"\n  subPath: \"rstudio-library-templates-data.tpl\"\n - name: session-templates\n  mountPath: \"/var/lib/rstudio-launcher/Kubernetes/job.tpl\"\n  subPath: \"job.tpl\"\n - name: session-templates\n  mountPath: \"/var/lib/rstudio-launcher/Kubernetes/service.tpl\"\n  subPath: \"service.tpl\""
  },
  {
    "objectID": "work/job-templating-k8s.html#another-approach-using-json-over-rides-on-posit-workbench-with-k8s",
    "href": "work/job-templating-k8s.html#another-approach-using-json-over-rides-on-posit-workbench-with-k8s",
    "title": "Job templating in Kubernetes",
    "section": "Another approach using json over-rides on Posit Workbench with k8s",
    "text": "Another approach using json over-rides on Posit Workbench with k8s\nPotentially json over-rides could work but my understanding is that this would overwrite ALL labels.\nuse this to add a label:\n\n\n/etc/rstudio/launcher.kubernetes.profiles.conf\n\njob-json-overrides=\"/spec/template/spec/labels\":\"/etc/rstudio/labels\"\n\n\n\n/etc/rstudio/labels\n\n[\n  {\n    \"applicationid\": \"workbench\",\n    \"label\": [\"value\"]\n  }\n]"
  },
  {
    "objectID": "work/Monitoring Posit Team.html",
    "href": "work/Monitoring Posit Team.html",
    "title": "Monitoring Posit Team",
    "section": "",
    "text": "Each of our products has their own method for monitoring. In addition to each individual monitoring method, Chronicle exists to pull that same information but aggregate it for customers. IMO it’s particularly useful for customers that have multiple environments that want their own single report to see the stuff they care about.\nWorkbench\n\nMetrics endpoint (Prometheus) that can be pulled into other systems: https://docs.posit.co/ide/server-pro/auditing_and_monitoring/prometheus_metrics.html\nAPI for more bespoke metrics (new): https://docs.posit.co/ide/server-pro/workbench_api/workbench_api.html\nBuild-in admin dashboard shows metrics around sessions (needs to be enabled): https://docs.posit.co/ide/server-pro/server_management/administrative_dashboard.html\n\nConnect\n\nMetrics endpoint (Prometheus) that can be pulled into other systems: https://docs.posit.co/connect/admin/operational-metrics/index.html\nAPI for more bespoke metrics: https://docs.posit.co/connect/api/ and https://docs.posit.co/connect/cookbook/\nBuilt-in admin dashboard shows metrics around resource consumption, scheduled jobs, etc\n\nPackage Manager\n\nMetrics endpoint (Prometheus) that can be pulled into other systems: https://docs.posit.co/rspm/admin/operational-metrics.html\nAPI for more bespoke metrics: https://docs.posit.co/rspm/admin/admin-cli.html#cli-remote-setup\nConfigure space alerts: https://docs.posit.co/rspm/admin/alerts.html\nBuilt-in admin dashboard shows metrics around packages being downloaded\n\nChronicle is still very early in development and customers have to build their own reports. Typically that looks like installing Chronicle on the Connect server and then publishing a quarto report that they made to Connect.There are a bunch of metrics reporting software that can take a stream of data and display it. You might hear datadog, splunk, grafana mentioned as the actual visualization tool being used in organizations.\nReach out to Posit to learn more about Chronicle and how to install it."
  },
  {
    "objectID": "work/Monitoring Posit Team.html#software-monitoring",
    "href": "work/Monitoring Posit Team.html#software-monitoring",
    "title": "Monitoring Posit Team",
    "section": "",
    "text": "Each of our products has their own method for monitoring. In addition to each individual monitoring method, Chronicle exists to pull that same information but aggregate it for customers. IMO it’s particularly useful for customers that have multiple environments that want their own single report to see the stuff they care about.\nWorkbench\n\nMetrics endpoint (Prometheus) that can be pulled into other systems: https://docs.posit.co/ide/server-pro/auditing_and_monitoring/prometheus_metrics.html\nAPI for more bespoke metrics (new): https://docs.posit.co/ide/server-pro/workbench_api/workbench_api.html\nBuild-in admin dashboard shows metrics around sessions (needs to be enabled): https://docs.posit.co/ide/server-pro/server_management/administrative_dashboard.html\n\nConnect\n\nMetrics endpoint (Prometheus) that can be pulled into other systems: https://docs.posit.co/connect/admin/operational-metrics/index.html\nAPI for more bespoke metrics: https://docs.posit.co/connect/api/ and https://docs.posit.co/connect/cookbook/\nBuilt-in admin dashboard shows metrics around resource consumption, scheduled jobs, etc\n\nPackage Manager\n\nMetrics endpoint (Prometheus) that can be pulled into other systems: https://docs.posit.co/rspm/admin/operational-metrics.html\nAPI for more bespoke metrics: https://docs.posit.co/rspm/admin/admin-cli.html#cli-remote-setup\nConfigure space alerts: https://docs.posit.co/rspm/admin/alerts.html\nBuilt-in admin dashboard shows metrics around packages being downloaded\n\nChronicle is still very early in development and customers have to build their own reports. Typically that looks like installing Chronicle on the Connect server and then publishing a quarto report that they made to Connect.There are a bunch of metrics reporting software that can take a stream of data and display it. You might hear datadog, splunk, grafana mentioned as the actual visualization tool being used in organizations.\nReach out to Posit to learn more about Chronicle and how to install it."
  },
  {
    "objectID": "work/Monitoring Posit Team.html#application-monitoring",
    "href": "work/Monitoring Posit Team.html#application-monitoring",
    "title": "Monitoring Posit Team",
    "section": "Application Monitoring",
    "text": "Application Monitoring\nInformation like who accessed the platform, who accessed each app can be gotten with the Connect API, for example with this cookbook recipe: https://docs.posit.co/connect/cookbook/metrics/viewing-content-usage-information/\nGetting what someone did inside of the application is harder. I’ve seen customers use google analytics for this (example for quarto). I’ve been really interested in exploring packages like shiny.telemetry by Appsilon for digging into this data also (it uses an external database to store this data).\nAlternativelyl a simple way to add user information is to simply add print statements through the code so it will print choices to the log. The log files can be accessed over this endpoint with the Connect API: https://docs.posit.co/connect/api/#get-/v1/content/-guid-/jobs/-key-/download\nAdmin logs on Connect are restricted to the admin user: https://docs.posit.co/connect/admin/auditing/index.html#audit-log-output\nBy default, audited events are only recorded in the database. Admin users are able to view audit entries in the dashboard: https://docs.posit.co/connect/api/#get-/v1/audit_logs"
  },
  {
    "objectID": "work/parallelization.html",
    "href": "work/parallelization.html",
    "title": "Parallelization in R",
    "section": "",
    "text": "For compute heavy and long running jobs, HPC can come to the rescue!\nAs a user remember though:"
  },
  {
    "objectID": "work/parallelization.html#profvis",
    "href": "work/parallelization.html#profvis",
    "title": "Parallelization in R",
    "section": "profvis",
    "text": "profvis\nlibrary(profvis)\nlibrary(ggplot2)\nlibrary(shiny)\n#library(deSolve)\n\n# Simple example\nprofvis({\n  data(diamonds, package = \"ggplot2\")\n  \n  plot(price ~ carat, data = diamonds)\n  m &lt;- lm(price ~ carat, data = diamonds)\n  abline(m, col = \"red\")\n})\nMore complex example:\nprofvis({\n  # generate a dataset\n  data(diamonds, package = \"ggplot2\")\n  \n  # save it \n  write.csv(diamonds, \"diamonds.csv\")\n  \n  # load it\n  csv_diamonds &lt;- read.csv(\"diamonds.csv\")\n  \n  # summarize\n  summary(diamonds)\n  \n  # plot it  \n  plot(price ~ carat, data = csv_diamonds)\n  m &lt;- lm(price ~ carat, data = csv_diamonds)\n  abline(m, col = \"red\")\n  \n  #create histogram of values for price\n  ggplot(data=csv_diamonds, aes(x=price)) +\n    geom_histogram(fill=\"steelblue\", color=\"black\") +\n    ggtitle(\"Histogram of Price Values\")\n  \n  #create scatterplot of carat vs. price, using cut as color variable\n  ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + \n    geom_point()\n  \n  #create scatterplot of carat vs. price, using cut as color variable\n  ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + \n    geom_point()\n  \n  # Examples from: https://www.statology.org/diamonds-dataset-r/#:~:text=The%20diamonds%20dataset%20is%20a,the%20diamonds%20dataset%20in%20R. \n})\nShiny app example:\n\n#profvis({runApp()})\n\n#profvis({runApp(appDir = \"./test_profvis/\")})\n\nprofvis({runApp(appDir = \".\")})"
  },
  {
    "objectID": "work/parallelization.html#other-packages",
    "href": "work/parallelization.html#other-packages",
    "title": "Parallelization in R",
    "section": "Other packages",
    "text": "Other packages\n\nbench\nmicrobenchmark"
  },
  {
    "objectID": "work/parallelization.html#taking-advantage-of-native-parallelization",
    "href": "work/parallelization.html#taking-advantage-of-native-parallelization",
    "title": "Parallelization in R",
    "section": "Taking advantage of native parallelization",
    "text": "Taking advantage of native parallelization\nUse packages like data.table that implement parallelism natively\nlibrary(data.table)\n\ngetDTthreads()\nDT = as.data.table(iris)\nDT[Petal.Width &gt; 1.0, mean(Petal.Length), by = Species]"
  },
  {
    "objectID": "work/parallelization.html#explicitly-programming-parallelization",
    "href": "work/parallelization.html#explicitly-programming-parallelization",
    "title": "Parallelization in R",
    "section": "Explicitly programming parallelization",
    "text": "Explicitly programming parallelization\nRead more: https://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745\n\nfutureverse\nRead more: https://www.futureverse.org/\nlibrary(future)\nplan(multisession)\n\n## Evaluate an R expression sequentially\ny &lt;- slow_fcn(X[1])\n\n## Evaluate it in parallel in the background\nf &lt;- future(slow_fcn(X[1]))\ny &lt;- value(f)\n\n## future.apply: futurized version of base R apply\nlibrary(future.apply)\ny &lt;-        lapply(X, slow_fcn)\ny &lt;- future_lapply(X, slow_fcn)\n\n## furrr: futurized version of purrr\nlibrary(furrr)\ny &lt;- X |&gt;        map(slow_fcn)\ny &lt;- X |&gt; future_map(slow_fcn)\n\n## foreach: futurized version (modern)\nlibrary(foreach)\ny &lt;- foreach(x = X) %do%       slow_fcn(x)\ny &lt;- foreach(x = X) %dofuture% slow_fcn(x)\n\n## foreach: futurized version (traditional)\nlibrary(foreach)\ndoFuture::registerDoFuture()\ny &lt;- foreach(x = X) %do%    slow_fcn(x)\ny &lt;- foreach(x = X) %dopar% slow_fcn(x)\n\n\nparallel\nExample from: https://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745\nlibrary(parallel)\n\n# Generate data\ndata &lt;- 1:1e9\ndata_list &lt;- list(\"1\" = data,\n                  \"2\" = data,\n                  \"3\" = data,\n                  \"4\" = data)\n\n# Single core\ntime = Sys.time()\n\ntime_benchmark &lt;- system.time(\n  lapply(data_list, mean)\n)\nsingle_core_time = difftime(Sys.time(), time)\n\n\n# Detect the number of available cores and create cluster\ntime = Sys.time()\n\ncores_avail= detectCores()\n\ncl &lt;- parallel::makeCluster(detectCores())\n# Run parallel computation\ntime_parallel &lt;- system.time(\n  parallel::parLapply(cl,\n                      data_list,\n                      mean)\n)\n\nmultiple_core_time = difftime(Sys.time(), time)\n\n# Close cluster\nparallel::stopCluster(cl)\n\n\nprint(single_core_time)\nprint(multiple_core_time)\nprint(cores_avail)\nRunning sequentially took 18.33 seconds. Running in parallel shortened that to 4.99 seconds.\n\n\nparallely (part of futureverse)\nRead more: https://parallelly.futureverse.org/\nlibrary(parallelly)\nparallelly::availableCores()\n\n\nforeach and futureverse\nlibrary(foreach)\nlibrary(doFuture)\n\nyears = 2024\n\nplan(multisession, workers = 20)\nresults &lt;- foreach(i=years, \n                   .combine = rbind) %dofuture% {\n                     get_api_stats(yr=i, tmt=tmt, product = \"litter\")}\n\n\nfuture.apply\nBefore:\ntable(okay2 &lt;- apply(tab2, 1, function(x) {...\nAfter:\nlibrary(future.apply)\nplan(multisession)\n\ntable(okay2 &lt;- future_apply(tab2, 1, function(x) {...\n\n\npurrr\nBefore:\nlibrary(tidyverse)\nsales_data_tbl %&gt;%\n  nest(data=c(date, value)) %&gt;%\n  mutate(model = purrr::map(data, function(df) {\n    lm(value ~month(date) + as.numeric(date) data=df)\n  })) \nAfter:\nlibrary(tidyverse)\nlibrary(purrr)\nsales_data_tbl %&gt;%\n  nest(data=c(date, value)) %&gt;%\n  mutate(model = furrr::map(data, function(df) {\n    lm(value ~month(date) + as.numeric(date) data=df)\n  })) \n\n\nforeach and doParallel\nlibrary(foreach)\nlibrary(doParallel)\n\ncl &lt;- makeCluster(20)\nregisterDoParallel(cl)\nresults &lt;- foreach(i=years, \n                   .combine = rbind) %dopar% {\n                     get_api_stats(yr=i, tmt=tmt, product = \"litter\")}\nstopCluster(cl)\n\n\nclustermq\nLifesaver, when working in a HPC environment.\nlibrary(foreach)\nlibrary(clustermq)\n\nn_cores &lt;- parallel::detectCores() - 1\noptions(clustermq.scheduler=\"multicore\")\ngetOption(\"clustermq.scheduler\")\n\nregister_dopar_cmq(n_jobs = n_cores)\nforeach(i = seq_len(n_cores)) %dopar% sqrt(i)\nThe future of clustermq: crew/mirai/nanonext"
  },
  {
    "objectID": "work/parallelization.html#but-what-about-python",
    "href": "work/parallelization.html#but-what-about-python",
    "title": "Parallelization in R",
    "section": "But what about Python?",
    "text": "But what about Python?\n\ndask-jobqueue\nray"
  },
  {
    "objectID": "work/python-projects-setup.html",
    "href": "work/python-projects-setup.html",
    "title": "Python projects set up and maintenance",
    "section": "",
    "text": "Have you ever tried to use someone else’s repo and run into issues with package installation and breaking package versions? Or tried to work on a really old repository and been foiled while trying to set it up?\nProjects have a standards problem - we need to realize that all the work we are doing exist in the framework of a project. By defining the elements of a project we can identify the parts that need to be made transparent and the tools (renv/venv) for making that happen.\nWhat is a project?\nWhat it’s not:"
  },
  {
    "objectID": "work/python-projects-setup.html#reproducible-package-environments-for-python---at-a-glance",
    "href": "work/python-projects-setup.html#reproducible-package-environments-for-python---at-a-glance",
    "title": "Python projects set up and maintenance",
    "section": "Reproducible package environments for Python - At a glance",
    "text": "Reproducible package environments for Python - At a glance\nStep 1: Use scanned Python packages\nPip can be already set up for you by your admins (with sudo pip config set --global global.index-url https://pkg.current.posit.team/blocked-python/latest/simple) whenever you use Workbench to use packages from package manager, no configuration needed. Check with:\npip config list\n\n# pip from stackoverflow article: https://stackoverflow.com/questions/51874974/how-can-i-get-pip-to-list-the-repositories-its-using\npip download --no-cache-dir \"foo&lt;0\" 2&gt;&1 | grep Looking\n\n# uv: https://docs.astral.sh/uv/guides/integration/alternative-indexes/\ncat pyproject.toml\nAs a user, configure pip to use your repository of choice with:\npip config set global.index-url https://pkg.current.posit.team/blocked-python/latest/simple\npip config set --global global.trusted-host pkg.current.posit.team\nAlternatively, for a specific project add these to the top of the requirements.txt file\n--index-url https://pkg.current.posit.team/blocked-python/latest/simple\n--trusted-host pkg.current.posit.team\nStep 2: Use environment tracking\n# Activate your virtual environment: \npython -m venv .venv\n. .venv/bin/activate\n# Take a snapshot of the environment: \npip freeze &gt; requirements.txt\nStep 3: Easy collaboration\n# Send a colleague the link to your project on git, they'll restore your environment with:\npip install -r requirements.txt"
  },
  {
    "objectID": "work/python-projects-setup.html#python-projects-setup",
    "href": "work/python-projects-setup.html#python-projects-setup",
    "title": "Python projects set up and maintenance",
    "section": "Python projects setup",
    "text": "Python projects setup\nSetup the venv environment:\npython -m venv .venv\n. .venv/bin/activate\n# .venv\\Scripts\\activate # Windows\nUpgrade pip and then install needed packages:\npip install --upgrade pip\npython -m pip install --upgrade pip wheel setuptools rsconnect-python\npip install -r requirements.txt\nRun the application:\nshiny run --reload app.py\nLeave a virtual environment with:\ndeactivate"
  },
  {
    "objectID": "work/python-projects-setup.html#jupyter-kernels-quarto-and-vs-code",
    "href": "work/python-projects-setup.html#jupyter-kernels-quarto-and-vs-code",
    "title": "Python projects set up and maintenance",
    "section": "Jupyter kernels, Quarto, and VS Code",
    "text": "Jupyter kernels, Quarto, and VS Code\n# Create a venv and install packages\npython -m venv .venv\n. .venv/bin/activate\nsource .venv/bin/activate\npip install -r requirements.txt\npip install --upgrade pip\n\n# Register as kernel\npip install ipykernel jupyter\npython3 -m ipykernel install --name \"new_venv\" --user\n\n# Check for missing dependencies\npython -m pip check\n\n# Automatically uprade all packages from requirements.txt \npip install pip-upgrader\npip-upgrade requirements.txt\npip install -r requirements.txt\n\n# Freeze your requirements\npip freeze &gt; requirements.txt\n\nRequires jupyter extension\nFor quarto and vs code:\n\nIf you create a virtual environment with venv in the env/ directory as described above, Visual Studio Code should automatically discover that environment when you load a workspace from the environment’s parent directory. Source: https://quarto.org/docs/projects/virtual-environments.html#vs-code\n\npython3 -m venv env\nsource env/bin/activate\npython3 -m pip freeze &gt; requirements.txt\npython3 -m pip install -r requirements.txt"
  },
  {
    "objectID": "work/python-projects-setup.html#deploy",
    "href": "work/python-projects-setup.html#deploy",
    "title": "Python projects set up and maintenance",
    "section": "Deploy",
    "text": "Deploy\n\nrsconnect-python CLI\nrsconnect deploy shiny .\n\n\nGit-backed\nUpdate the code, and then run:\nrsconnect write-manifest shiny --overwrite .\nCommit the new manifest.json file to the git repo along with the code."
  },
  {
    "objectID": "work/python-projects-setup.html#project-updates",
    "href": "work/python-projects-setup.html#project-updates",
    "title": "Python projects set up and maintenance",
    "section": "Project updates",
    "text": "Project updates\nCreate the requirements file:\npython -m pip freeze &gt; requirements.txt\nrsconnect write-manifest shiny .\nIf you are running into deploy issues where there are breaking packages you can edit the requirements file explicitly:\n# requirements.txt generated by rsconnect-python on 2022-09-21 14:59:58.865441\nstreamlit==1.11.0\nTo use a Package Manager repository with a specific project defined by a requirements.txt file, add -i [repositoryURL] to the top of your file, for example:\n-i https://packagemanager.posit.co/pypi/latest/simple\npandas\nscipy\nHow to configure a pypi repository globally (using pip.conf): https://docs.posit.co/resources/install-python/#optional-configure-a-pypi-repository"
  },
  {
    "objectID": "work/python-projects-setup.html#troubleshooting",
    "href": "work/python-projects-setup.html#troubleshooting",
    "title": "Python projects set up and maintenance",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIssues with Python not being on path\nSet it manually to an installed Python version with:\nalias python=\"/opt/python/3.9.17/bin/python\"\nSet it in your .bashrc on mac or linux so that it is set for your profile every time you log in (typically this is located in the root directory of your home folder):\n# add this to your .bashrc\nexport PATH=/opt/python/3.11.9/bin:$PATH\nCheck for the available python versions (if typically installed):\nls -ld /opt/python/*"
  },
  {
    "objectID": "work/python-projects-setup.html#resources",
    "href": "work/python-projects-setup.html#resources",
    "title": "Python projects set up and maintenance",
    "section": "Resources",
    "text": "Resources\nPosit Connect User Guide: Shiny for Python"
  },
  {
    "objectID": "work/r-command-line.html",
    "href": "work/r-command-line.html",
    "title": "Command line / bash and R",
    "section": "",
    "text": "Have you ever needed to run linux / mac / or windows commands from an R script? Do you have workflows where you have to create data files in R, pass to another system for processing, and then back into R for final visualization?\nOne of the best things about the R ecosystem is the massive spread of packages and capability that have already been developed, including for running operating system commands already at your fingertips.\nWarning: With great power comes great responsibility. While the commands listed in this document are safe to run it’s always a good idea to double check what a system command actually does prior to running it, otherwise you could end up one of the examples here."
  },
  {
    "objectID": "work/r-command-line.html#purpose",
    "href": "work/r-command-line.html#purpose",
    "title": "Command line / bash and R",
    "section": "",
    "text": "Have you ever needed to run linux / mac / or windows commands from an R script? Do you have workflows where you have to create data files in R, pass to another system for processing, and then back into R for final visualization?\nOne of the best things about the R ecosystem is the massive spread of packages and capability that have already been developed, including for running operating system commands already at your fingertips.\nWarning: With great power comes great responsibility. While the commands listed in this document are safe to run it’s always a good idea to double check what a system command actually does prior to running it, otherwise you could end up one of the examples here."
  },
  {
    "objectID": "work/r-command-line.html#including-bash",
    "href": "work/r-command-line.html#including-bash",
    "title": "Command line / bash and R",
    "section": "Including Bash",
    "text": "Including Bash\nRMarkdown supports bash, sh and awk (depending on the hosting operating system).\nFor example:\npwd\nYou could also run something a little bit more complex:\ncurrent_date=$(date)\necho \"Today is $current_date\"\nNotes from this resource:\n\nRunning bash within rmarkdown will ignore profile files like ~/.bash_profile and ~/.bash_login, in which you may have defined command aliases or modified environment variables like the PATH variable. If you want these profile files to be executed just like when you use the terminal, you may pass the argument -l to bash via engine.opts, e.g.,\n\n#bash, engine.opts='-l'\necho $PATH\n\nIf you want to enable the -l argument globally for all bash chunks, you may set it in the global chunk option in the beginning of your document:knitr::opts_chunk$set(engine.opts = list(bash = \"-l\"))\nYou can also pass other arguments to bash by providing them as a character vector to the chunk option engine.opts."
  },
  {
    "objectID": "work/r-command-line.html#system-commands-within-r",
    "href": "work/r-command-line.html#system-commands-within-r",
    "title": "Command line / bash and R",
    "section": "System commands within R",
    "text": "System commands within R\nNote from this resource: Please note that bash is invoked with the R function system2().\n# list all files and directories\nsystem2(command = \"ls\")\nWe can use this to save a system output into our R workspace for later processing:\nvar &lt;- system2(\"whoami\", stdout = TRUE, stderr = TRUE)\n\nprint(var)\nYou can also use input= for commands that will have a response from the system that needs a response, like logging in where you supply a username and then it prompts you for a password.\nsystem('sudo -kS ls',input=readline(\"Enter your password: \"))"
  },
  {
    "objectID": "work/r-command-line.html#multi-line-system-commands-in-r",
    "href": "work/r-command-line.html#multi-line-system-commands-in-r",
    "title": "Command line / bash and R",
    "section": "Multi-line system commands in R",
    "text": "Multi-line system commands in R\nThere is a good discussion on nuances of using system commands within R in this stackoverflow post.\nsystem and system2 functions are designed for simple commands, as evidenced by the separation of the command word and command arguments into separate function arguments. However that can be subverted using shell metacharacters to run more complex statements.\nsystem('echo a b; echo c d');\n\nsystem2('echo',c('a','b; echo c d'));\nAt this point in the complexity it is probably worth considering writing the commands you want to run as a bin bash script which is then called by R to run from system.\nbash_script &lt;- \"#!/bin/bash\n# A simple Bash script\necho pwd\necho ls\necho Done!\"\n\nwriteLines(bash_script, \"bash_script.sh\")\n\nsystem('chmod +x bash_script.sh')\n\nsystem2('./bash_script.sh', stdout = TRUE, stderr = TRUE)\nWe could also use the built in terminal in the Rstudio IDE to write the .sh file (using vim, saving with escape/ctrl/cmd-c and :w, closing with escape/ctrl/cmd-c and typing :q). The permissions will need to be changed with chmod +x demo.sh. That file can then be run with ./demo.sh."
  },
  {
    "objectID": "work/r-command-line.html#responding-to-bash-commands-from-r",
    "href": "work/r-command-line.html#responding-to-bash-commands-from-r",
    "title": "Command line / bash and R",
    "section": "Responding to Bash commands from R",
    "text": "Responding to Bash commands from R\nThere is a good example in stackoverflow here.\nIn some cases it is useful to run a command and then be able to add an additional input. For example when signing in to a server being able to respond with a password, or to provide a yes to a prompt when installing a package. The input arg to system2 will accomplish this.\ninput: if a character vector is supplied, this is copied one string per line to a temporary file, and the standard input of ‘command’ is redirected to the file.\nfoo = c(\"foo\", \"bar\")\nresult = system2(\"cat\", input = foo, stdout = TRUE)\n\nresult"
  },
  {
    "objectID": "work/r-command-line.html#mixing-r-and-bash-operations",
    "href": "work/r-command-line.html#mixing-r-and-bash-operations",
    "title": "Command line / bash and R",
    "section": "Mixing R and bash operations",
    "text": "Mixing R and bash operations\nLet’s save a file to disk using R:\n#my_data &lt;-  data.frame(\"fruits\" = c(\"Mango\",\"Orange\",\"Grape\",\"Guava\",\"Apple\"))\n\nmy_data &lt;- \"Mango, Orange, Grape, Guava, Apple\"\n\nwrite.table(my_data, file = \"my_data.txt\", sep = \"\", row.names = FALSE)\n\nread.delim(file = \"my_data.txt\")\nLet’s run a multi-line bash command and see the output:\ncount_words=`wc -w my_data.txt`\necho \"Total words in my_data.txt is $count_words\"\nWe could also run a bin bash script with commands so we can pass the output back into R for further processing:\nbash_script2 &lt;- \"#!/bin/bash\n# A simple Bash script\nsed 's/a//g' my_data.txt\"\n\nwriteLines(bash_script2, \"bash_script2.sh\")\n\nsystem('chmod +x bash_script2.sh')\n\nmy_data2 &lt;- system2('./bash_script2.sh', stdout = TRUE, stderr = TRUE)\nSome behavior is reported that when running bash scripts in R with system using the wait argument (when used for running async with R) will only wait for the last line to complete. Solutions are discussed in that article."
  },
  {
    "objectID": "work/r-command-line.html#session-memory-from-r-and-python-bash-commands",
    "href": "work/r-command-line.html#session-memory-from-r-and-python-bash-commands",
    "title": "Command line / bash and R",
    "section": "Session memory from R and Python (bash commands)",
    "text": "Session memory from R and Python (bash commands)\nThere are several different ways to access this information, with the challenge coming in that depending on the command we can get host level information, system, rsession, kubernetes session, and different units (B vs iB vs bits).\nThe below are various functions and approaches for determining a robust approach that most closely matches what is available via the RStudio IDE and the Workbench Admin Dashboard.\nNotes:\n\nUnits: 1 MiB = 1,048,576 bytes, where mebibyte is MiB.\nDefault session has 2048 MB -&gt; ~2 GB\n\n\nFrom R:\nWhen inside kubernetes (similarly with docker containers) in order to get the session size we need to pull from inside the cgroups.\nAll of the options:\nsystem(\"cat /sys/fs/cgroup/memory/memory.stat\")\nView the total memory with:\n# Total memory in session\ntotal = system(\"cat /sys/fs/cgroup/memory/memory.limit_in_bytes\", intern=T)\n\ntotal_rounded = (((as.numeric(total)/1024)/1024)/1024) # This is GiB\n\nprint(total_rounded)\nView the used memory with:\n# Used memory \nused = system(\"cat /sys/fs/cgroup/memory/memory.usage_in_bytes\", intern=T)\n\nused_rounded = (((as.numeric(used)/1024)/1024)/1024) # This is GiB\n\nprint(used_rounded)\nWe can then calculate the remaining memory with:\nremains &lt;- as.numeric(total) - as.numeric(used) # this is in Bytes\n\nremains_rounded = (((remains/1024)/1024)/1024)\n\nprint(remains_rounded) # This is GiB\nUnfortunately the below approaches all use the hosting container, rather than the session container (IE getting 32GB for the total size rather than the 2GB specified).\nsystem(\"free -t\")\nFree isn’t aware it is inside a container, it is giving memory of the parent server:\nsystem(\"free --mega\")\nRunning top also reports something different, with 32 as the total amount of memory (showing the parent).\nsystem(\"top\")\nWe can also use the “garbage collector”, but note that this only gives us the used memory and not the total available.\ngc(verbose=TRUE)\nBringing these approaches together we can use the function created by this stackoverflow post which parses /proc/meminfo on linux as described in this stackoverflow post.\ngetAvailMem &lt;- function(format = TRUE) {\n\n  gc()\n\n  if (Sys.info()[[\"sysname\"]] == \"Windows\") {\n    memfree &lt;- 1024^2 * (utils::memory.limit() - utils::memory.size())\n  } else {\n    # http://stackoverflow.com/a/6457769/6103040\n    memfree &lt;- 1024 * as.numeric(\n      system(\"awk '/MemFree/ {print $2}' /proc/meminfo\", intern = TRUE))\n  }\n\n  `if`(format, format(structure(memfree, class = \"object_size\"),\n                      units = \"auto\"), memfree)\n}\n\ngetAvailMem()\nOr directly parsing meminfo, per this reference:\n# 1024 * as.numeric(system(\"grep MemFree /proc/meminfo\"))\nsystem(\"grep MemFree /proc/meminfo\")\nTotal amount of memory in kB that the OS thinks is available:\n# 1024 * as.numeric(system(\"grep MemFree /proc/meminfo\"))\nsystem(\"grep MemTotal /proc/meminfo\")\n\n\nShow the code\nmemfree &lt;- as.numeric(system(\"awk '/MemFree/ {print $2}' /proc/meminfo\", intern=TRUE))\nmemfree\n\n\n[1] 2748548\n\n\nAn alternative is the memuse package with git here:\nlibrary(memuse)\n# memuse::Sys.meminfo()\n# memuse::Sys.swapinfo() \nmemuse::Sys.procmem(gcFirst = FALSE) # Our current size, matches the R Memory Usage Report. By setting gcFirst = FALSE we are not calling garbage collection prior to getting memory. \n# memuse::Sys.cachesize()\n# memuse::Sys.cachelinesize()\n\n# Expectations: \n#2076 MiB is expected total, 0.2721055 GB\n#1640 MiB remaining\n# 1 MiB = 1,048,576 bytes\nUnfortunately it is reporting the parent container again:\nmemuse::Sys.meminfo(compact.free = FALSE)\nWe can also try using lobstr:\nlibrary(lobstr)\n\nlobstr::mem_used()\nusage &lt;- function() {\n    m1 &lt;- sum(gc(FALSE)[, \"(Mb)\"])\n    m2 &lt;- as.double(system(paste(\"ps -p\", Sys.getpid(), \"-o pmem=\"), intern = TRUE))\n    c(`gc (MiB)` = m1, `ps (%)` = m2)\n}\nusage()\n\n\nFrom Python (using reticulate but would also work from another IDE, for example Jupyter):\nlibrary(reticulate)\nNote this approach is using free, which shows the hosting container’s memory, but the same approach as used in R above using cgroups can be applied.\nimport os\nos.system(\"free\")"
  },
  {
    "objectID": "work/r-command-line.html#too-long-didnt-read-tldr",
    "href": "work/r-command-line.html#too-long-didnt-read-tldr",
    "title": "Command line / bash and R",
    "section": "Too Long Didn’t Read (TLDR)",
    "text": "Too Long Didn’t Read (TLDR)\nTLDR: Read the excellently written r-bloggers post here.\nCommands to reference:\n\nsystem\nsystem2\ncommandArgs\nshell\n\nOther references:\n\nhttps://www.r-bloggers.com/2021/09/how-to-use-system-commands-in-your-r-script-or-package/\nhttps://bash-intro.rsquaredacademy.com/r-command-line.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/eng-bash.html\nhttps://wetlandsandr.wordpress.com/2018/09/15/integrating-bash-and-r/\n\n\nResources\nLearn more about memory profiling: http://adv-r.had.co.nz/memory.html#memory-profiling\nWithin a RStudio/Posit products from the admin side there are some additional tools that can be handy:\n\nhttps://support.rstudio.com/hc/en-us/articles/360046919174\nhttps://support.rstudio.com/hc/en-us/articles/360044800273\nhttps://support.rstudio.com/hc/en-us/articles/1500005616261-Understanding-Memory-Usage-in-the-RStudio-IDE\nhttp://adv-r.had.co.nz/memory.html\nhttps://stackoverflow.com/questions/46690665/complete-r-session-size\nRead about what ubuntu load is: https://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html\nhttps://stackoverflow.com/questions/53333858/how-to-clean-up-r-memory-without-the-need-to-restart-r-session\nhttps://github.com/rstudio/rstudio/issues/9353\nhttps://engineering.linkedin.com/blog/2016/08/don_t-let-linux-control-groups-uncontrolled"
  },
  {
    "objectID": "work/r-repository-settings-workbench.html",
    "href": "work/r-repository-settings-workbench.html",
    "title": "Setting the R package repository on Posit Workbench",
    "section": "",
    "text": "The repository for packages being used for installation by users is often important to configure. When using something like Package Manager you’ll want users to install packages from there, rather than the broader internet."
  },
  {
    "objectID": "work/r-repository-settings-workbench.html#overview",
    "href": "work/r-repository-settings-workbench.html#overview",
    "title": "Setting the R package repository on Posit Workbench",
    "section": "",
    "text": "The repository for packages being used for installation by users is often important to configure. When using something like Package Manager you’ll want users to install packages from there, rather than the broader internet."
  },
  {
    "objectID": "work/r-repository-settings-workbench.html#at-a-glance",
    "href": "work/r-repository-settings-workbench.html#at-a-glance",
    "title": "Setting the R package repository on Posit Workbench",
    "section": "At a glance",
    "text": "At a glance\nThe best pattern is to configure the repository across R sessions using R config options, not RStudio configs. A shared site library can be created (leveraging Rprofile.site and Renviron.site for example) or using renv with a shared renv package cache (maintaining reproduceability through the renv.lock file).\nThe Renviron.site would override the Rprofile.site setting and therefore may be more robust.\n\nRenviron.site\nReferencing: https://github.com/sol-eng/singularity-rstudio/blob/main/data/workbench/scripts/run.R\nCreate a Renviron.site file and define:\n\n\n/opt/R/Some-R-Version/lib/R/etc/Renviron.site\n\nRENV_PATHS_PREFIX_AUTO=TRUE\nRENV_PATHS_CACHE=/scratch/renv\nR_LIBS_SITE=R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/local/lib/R/library:/usr/lib64/R/library:/usr/share/R/library'}\n\n\n\n.Rprofile.site\nSet .libPaths() in .Rprofile.site:\n\n\n/opt/R/Some-R-Version/lib/R/etc/Rprofile.site\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/all/latest\"))\n\nif (interactive()) {\n  options(width = 120)\n}\n\nif (Sys.info()[[\"sysname\"]] == \"Windows\") {\nSys.setenv(RENV_DOWNLOAD_METHOD = \"curl\")\n}\n\nif (\"folder\" %in% tolower(list.files(\"C:/\"))) {\nif (!\"Rlib\" %in% list.files(\"C:/username/\")) {\nprint(\"Creating Rlib folder\")\ndir.create(\"C:/username/Rlib\",mode = \"0777\",recursive = T)\n}\ncat(\"\\033[0;32;1mSetting local user lib\\033[0m\\n\")\n.libPaths(c(\"C:/username/Rlib\" , .libPaths() ) )\n} else {\ncat(\"\\033[0;33;1mYou should consider getting the access right so we can put your local R-lib there, instead of OneDrive.\\033[0m\\n\")\n}\n\n\n\nTest\nTest this by running and checking the outputs of:\n\n.libPaths()\noptions()$repos"
  },
  {
    "objectID": "work/r-repository-settings-workbench.html#long-winded",
    "href": "work/r-repository-settings-workbench.html#long-winded",
    "title": "Setting the R package repository on Posit Workbench",
    "section": "Long-winded",
    "text": "Long-winded\n\nStartup behavior of R when loading package environment details\nR Startup behavior (funny): https://rstats.wtf/r-startup.html\n\nCredit: This section was largely taken from an internal Posit resource of unknown authorship\n\nFrom bash it is different than from the editor.\nSee here for the official startup docs from R.\nR from the command line/bash will take the user’s environment. It will not read any additional bash files during start-up (which the RStudio products will do). It will still read in the R startup files (Renviron/Rprofile).\nPrior to loading the R session from the bash shell any commands in this file will be read and execute commands if it exists: /etc/profile\nNext, the the first of the following files that exists and is readable will have commands executed from (only one of these files will be read and executed):\n\n~/.bash_profile\n~/.bash_login\n~/.profile\n\nR then always loads the following (in order):\n\nR_HOME/etc/Renviron.site (set for all users)\n.Renviron - user-specific, typically in the user’s home directory, but can be elsewhere (for instance, in a Project folder)\nR_HOME/etc/Rprofile.site (set for all users)\n.Rprofile - user-specific, typically in the user’s home directory, but can be elsewhere (for instance, in a Project folder)\n\nBeyond this, what gets put in the environment depends on the product.\n\nRStudio Server / Workbench: before any of this executes, RStudio Server will first include the contents of /etc/rstudio/rsession-profile if it exists. It will also add anything set in rsession-ld-library-path in rserver.conf to the LD_LIBRARY_PATH environment variable.\nShiny Server / Connect: you can affect the environment variables for a specific application using program supervisors\n\nThis page in the documentation is the new home for the golden source of info for startup behavior: https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html\n\n\nR sessions across all Workbench IDE’s\nRelying on repos.conf for R repository configuration IMHO is a clear anti-pattern which especially large customers with large setups should not use. repos.conf is a relic from a time where there was only RSP.\nConfigure the repository settings in R directly, rather than through the RStudio settings.\nResources:\n\nA great resource for setting this up simply is: https://docs.posit.co/ide/user/ide/guide/environments/r/managing-r.html\nFor a more complex example that could be used in slurm environments refer to: https://github.com/sol-eng/singularity-rstudio/blob/main/data/workbench/scripts/run.R\n\nThe Renviron.site would override the Rprofile.site setting and therefore may be more robust.\n\nRenviron.site\nCreate a Renviron.site file and define:\n\n\n/opt/R/Some-R-Version/lib/R/etc/Renviron.site\n\nKey1=value1\nRENV_PATHS_PREFIX_AUTO=TRUE\nRENV_PATHS_CACHE=/scratch/renv\nR_LIBS_SITE=R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/local/lib/R/library:/usr/lib64/R/library:/usr/share/R/library'}\n\nAnd then Sys.getenv(\"Key1\") will return \"value1\" in a users R session.\nThis can be set at the user or system level. Users have the choice between user or project level (project taking preference). The usethis package includes a helper function for editing .Renviron files from an R session with usethis::edit_r_environ(). For a system level install it is placed per R version, for example at /opt/R/4.1.1/lib/R/etc/Renviron.site.\n\n\n.Rprofile.site\nSet .libPaths() in .Rprofile.site:\n\n\n/opt/R/Some-R-Version/lib/R/etc/Rprofile.site\n\noptions(repos = c(CRAN = \"https://packagemanager.posit.co/all/latest\"))\n\nif (interactive()) {\n  options(width = 120)\n}\n\nAgain this file can be se at the user or system level. At the user level the easiest way to edit your .Rprofile file is to use the usethis::edit_r_profile() function from within an R session. You can specify whether you want to edit the user or project level .Rprofile. For a system level install it is placed per R version, for example at /opt/R/4.2.0/lib/R/etc/Rprofile.site.\n\n\nConfiguring a shared library\nTaken from an internal discussion\nSet .libPaths() in .Rprofile.site.\nThere are two main options:\n\nInstall a set of R packages in a shared library. The users here would need to specify which packages they really need and then their IT team could install those into a site-library in each R version, ideally leveraging a time-based snapshot from package manager for reproducibility reason. Users in this case would be limited to a certain package version for some time. They however can update those packages regularly (e.g. quarterly) to lessen the pain here. As mentioned above, do NOT use any recommendation from the Posit Admin guide for this setup but follow the standard approach to configure the same in R (leveraging Rprofile.site and Renviron.site for example).\nEducate/train the users to all use renv - then every user would have maximum degree of freedom to choose which packages and which version they want to use for each project. They then would maintain renv.lock files for each project and the admin team can then configure a shared renv package cache to ensure that any given package/version combination is only stored once.\n\nBoth options in their own right also protect the data scientists from producing irreproducible code since in neither case they rely on that melting pot of R package mess that typically piles up in R_LIBS_USER (in particular for systems where R points to latest CRAN which IMHO is another anti-pattern)\n\nScript for updating packages from rspm that have changed to site library\n# update existing packages\nupdate.packages(lib.loc=&lt;site.library&gt;, repos=&lt;PPM Repo&gt;, ask=FALSE)\n\n# add any new packages\nnew.packages(lib.loc=&lt;site.library&gt;, repos=&lt;PPM Repo&gt;, ask=FALSE)\n\n\n\n\nWorkbench and RStudio sessions only\nThe oft recommended path is to use the repos.conf or rsession.conf file to configure the repository URL.\nThis might look like:\n\n\n/etc/rstudio/rsession.conf\n\nr-cran-repos=http://cran.at.r-project.org/\n\nOr:\n\n\n/etc/rstudio/repos.conf\n\nRSPM=https://packagemanager.posit.co/cran/__linux__/jammy/latest\nCRAN=https://packagemanager.posit.co/cran/__linux__/jammy/latest\nAustralia=https://cran.ms.unimelb.edu.au/\nAustria=https://lib.ugent.be/CRAN/\n\nAnd adding to rsession.conf:\n\n\n/etc/rstudio/rsession.conf\n\n# Use this to change the location / name of the repos.conf file\nr-cran-repos-file=/etc/rstudio/repos.conf\n\nReference: https://docs.posit.co/ide/server-pro/rstudio_pro_sessions/package_installation.html\n\n\nLD_LIBRARY_PATH\nReference: https://rstudioide.zendesk.com/agent/tickets/107856\nThere are a few different places the LD_LIBRARY_PATH can be modified within Workbench settings, only some of which will work for packages like rJava. For instance, including the ldpaths script in /etc/rstudio/r-versions can ensure the correct library is set on R session startup. The final step in this support article shows a method of setting this up which should work even on non-containerized Workbench sessions (the mkdir command can be excluded, since that directory should already exist on a server-installed version of Workbench):\nWe need to force the installed R version to use it’s own ldpaths startup script when it starts inside the container.\nRUN mkdir -p /etc/rstudio && printf \"Path: /opt/R/${R_VERSION}\\nScript: /opt/R/${R_VERSION}/lib/R/etc/ldpaths\" &gt; /etc/rstudio/r-versions\nThese steps are good to follow: https://solutions.posit.co/envs-pkgs/using-rjava/index.html#additional-steps-for-workbench\nThe additional steps that need to be followed on Workbench are:\n\n\n/etc/rstudio/r-versions\n\nPath: /opt/R/4.2.0\nScript: /opt/R/4.2.0/lib/R/etc/ldpaths\n\n\n\nTroubleshooting\nTo determine the environment details it can be useful to run Sys.getenv() from inside and outside RStudio, to see if the user’s bash files are setting environment variables inappropriately for the system.\nIf the issue is occurring within RStudio it can be helpful to capture the output ofsystem(\"ldd /usr/lib/rstudio-server/bin/rsession\") from inside and outside RStudio to see which R libraries are being loaded.\nTest from a user session the repository details with:\n\n.libPaths()\noptions()$repos\n\nPermissions on the various configs can cause various issues with soft fails, for example a working permission is: chmod 644, or for the rstudio directory consider chmod 0755 /etc/rstudio or chmod o+x /etc/rstudio to add the x bit for just rstudio-server without opening up the other permissions if it is desired to keep it restricted (may result in odd behavior).\nCheck permissions with: ls -la /etc/rstudio"
  },
  {
    "objectID": "work/rspm_remote_admin_cli.html",
    "href": "work/rspm_remote_admin_cli.html",
    "title": "Using the remote CLI with package manager",
    "section": "",
    "text": "Overview\nRemote CLI use: https://docs.posit.co/rspm/admin/admin-cli/#cli-remote-use\n\nThe entire admin CLI is now available for use remotely, with the exception of the offline, online and cluster commands. API tokens can be generated with the new global:admin scope to allow full access to the CLI.\n\nIn order to use the admin CLI remotely you need an admin to create an API token for you (rspm create token)\nThe admin will likely need to follow these steps:\n\nEnable the Authentication.APITokenAuth configuration setting and restart the Package Manager service.\nCreate a token using the rspm create token command. For example rspm create token --scope=global:admin --expires=never\nGive the token and the full address of the server (including the port if not using a default port like 80/443) to the remote user.\n\nAs a user these are needed:\n\nDownload the Package Manager CLI, ensuring that the CLI version matches the server version.\nSet an environment variable named PACKAGEMANAGER_TOKEN with the value of the token.\nSet an environment variable named PACKAGEMANAGER_ADDRESS with the address of the Package Manager server.\n\nSome other commands useful to know about:\n\nrspm encrypt generate for creating the encryption token from the CLI before setting up the server.\nGenerate a bootstrap key programmatically: https://docs.posit.co/rspm/admin/appendix/bootstrap/\nrspm create token --scope=sources:write --description 'meatballhat is testing' --sources='*' --expires=1h generates an API key for one hour\nrspm create token --description=\"user A\" --sources=local-src --expires=30d --scope=sources:write creates a token for adding binary packages remotely\nuseful example showing setup instructions for a package manager instance: https://github.com/rstudio/evaluations/blob/main/src/scripts/config_rspm.sh\nhere’s the way we included the rspm cli in the container images: https://github.com/rstudio/sol-eng-demo-server/pull/110/files\nlist of all commands: https://docs.posit.co/rspm/admin/appendix/cli/rspm/\n\n\n\nEnvironment set-up\nCurrent environment token:\nexport PACKAGEMANAGER_ADDRESS=&lt;redacted&gt;\n\nexport PACKAGEMANAGER_TOKEN=&lt;redacted&gt;\n\n\nRunning commands\nAs a user we can test our access with something like this:\n# Download the Package Manager CLI, ensuring that the CLI version matches the server version. For example RSPM_SERVER_VERSION=2024.04.2-29 or RSPM_SERVER_VERSION=2023.12.0-13, find the version through the UI or with rspm --version\nexport RSPM_SERVER_VERSION=2024.08.2-9\nexport RSPM_SERVER_VERSION=2024.08.4-10\nexport RSPM_SERVER_VERSION=2024.11.0-7\ncurl -o rspm -f https://cdn.posit.co/package-manager/linux/amd64/rspm-cli-linux-${RSPM_SERVER_VERSION}\nchmod +x rspm\n\n# Set an environment variable named PACKAGEMANAGER_TOKEN with the value of the token.\nexport &lt;redacted&gt;\necho $PACKAGEMANAGER_TOKEN\n\n# Set an environment variable named PACKAGEMANAGER_ADDRESS with the address of the Package Manager server. \nexport PACKAGEMANAGER_ADDRESS=&lt;redacted&gt;\necho $PACKAGEMANAGER_ADDRESS\n\n# Test it! \n./rspm verify\n./rspm list tokens\n\n\n# Test it! \n./rspm list blocklist-rules\n\n# Test it! Need a package to try to push to test these\n./rspm add --source=internal --path=lisaPackageManagerDemo_0.0.1.tar.gz\nLet’s create a new repository for blocked packages that is inspired by https://github.com/rstudio/evaluations/blob/main/src/scripts/config_rspm.sh\nFirst we should create our .txt files with the desired subset of packages:\n# curated cran\necho -e 'dplyr\\nggplot2\\narrow\\nlubridate\\njsonlite\\nreadxl\\nhaven\\ngdata\\nigraph\\ncommonmark' &gt; package_subset_r.csv\n\n# curated pypi\necho -e 'plotnine\\npolars\\npillow\\ngreat-tables' &gt; package_subset_py.txt\n\nexport snapshot=2025-01-22\n# blocked packages -- R\n./rspm create source --type=curated-cran --name=blocked-packages-r \n./rspm create blocklist-rule --source=blocked-packages-r --vulns\n./rspm create blocklist-rule --source=blocked-packages-r --package-name=ggplot2 --description=\"Installation of 'ggplot2' is blocked\"\n./rspm update --source=blocked-packages-r --file-in='package_subset_r.csv' --commit --snapshot=\"${snapshot}\"\n./rspm create repo --name=blocked-r --description=\"Curated CRAN with vulnerability blocking enabled. Downloads of ggplot2 are also disallowed.\"\n./rspm subscribe --repo=blocked-r --source=blocked-packages-r\n\n# blocked packages -- python\n./rspm create source --name=blocked-packages-python --type=curated-pypi\n./rspm create blocklist-rule --source=blocked-packages-python --vulns\n./rspm create blocklist-rule --source=blocked-packages-python --package-name=plotnine --description=\"Installation of 'plotnine' is blocked.\"\n./rspm update --source=blocked-packages-python --file-in=package_subset_py.txt --snapshot=\"${snapshot}\" --commit\n./rspm create repo --name=blocked-python --type=python --description=\"Curated PyPI with vulnerability blocking enabled. Downloads of plotnine are also disallowed.\"\n./rspm subscribe --repo=blocked-python --source=blocked-packages-python\nLet’s check for package versions that violate the blocklist rule for blocking vulnerabilities:\n#rspm test blocklist-rules --repo=[repo name] --package-name=[package name] --version=[version]\n./rspm test blocklist-rules --repo=blocked-python --package-name=pillow\n./rspm test blocklist-rules --repo=blocked-python --package-name=pillow --version=10.0.1\nLet’s set up a cran that has all vulnerable packages blocked (rather than a subset). This wasn’t possible until the newest release.\n./rspm create repo --name=cran-vulns-blocked --type=r --description='Access CRAN packages with known vulnerabilities blocked'\n./rspm subscribe --repo=cran-vulns-blocked --source=cran\n./rspm create blocklist-rule --repo=cran-vulns-blocked --vulns\nLet’s now do the same for pypi:\n./rspm create repo --name=pypi-vulns-blocked --type=python --description='Access PyPi packages with known vulnerabilities blocked'\n./rspm subscribe --repo=pypi-vulns-blocked --source=pypi\n./rspm create blocklist-rule --repo=pypi-vulns-blocked --vulns\nLet’s check for package versions that violate the blocklist rule for blocking vulnerabilities:\n#rspm test blocklist-rules --repo=[repo name] --package-name=[package name] --version=[version]\n./rspm test blocklist-rules --repo=cran-vulns-blocked --package-name=jsonlite --version=1.8.7\n./rspm test blocklist-rules --repo=pypi-vulns-blocked --package-name=pillow --version=10.0.1\nLet’s create a blocklist rule to block a specific package:\n./rspm create blocklist-rule --repo=cran-vulns-blocked --package-name=ggplot2 --description=\"Installation of 'ggplot2' is blocked\"\n./rspm test blocklist-rules --repo=cran-vulns-blocked --package-name=ggplot2\nRemove a repository (oops!):\n./rspm delete repo --name=pypi-vulns-blocked\nTry to download a blocked package:\nlibrary(remotes)\noptions(repos = c(CRAN = \"https://pkg.demo.posit.team/cran-vulns-blocked/__linux__/noble/latest\"))\ninstall_version(\"jsonlite\", \"1.8.7\")\nor we can instead curl it to see what happens:\ncurl https://pkg.demo.posit.team/cran-vulns-blocked/latest/src/contrib/Archive/jsonlite/jsonlite_1.8.7.tar.gz\n\n\nrosv\nrosv r package: https://github.com/cran/rosv\nlibrary(rosv)\n\n# Pull the entire set of PyPI vulnerability data\npkg_vul &lt;- osv_query(ecosystem = 'PyPI', all_affected = FALSE)\npypi_vul &lt;- create_osv_list(pkg_vul, as.data.frame = FALSE, NA_value = ' ')\n\n# Pull the entire set of cran vulnerability data\npkg_vul &lt;- osv_query(ecosystem = 'CRAN', all_affected = FALSE)\ncran_vul &lt;- create_osv_list(pkg_vul, as.data.frame = FALSE, NA_value = ' ')\n\n# Scan an R project \ndf &lt;- osv_scan('r_project')\n\n\nr-advisory-database\nr-advisory-database: https://github.com/RConsortium/r-advisory-database\n\n\nTroubleshooting\n\n“unsupported protocol scheme”“”\n\nThis error is due to the missing https:// in front of the URL. Add that and try again.\n\n“Error: unable to get the claims from the token: signature is invalid”\n\nWas the package manager server recently updated? The encryption key changes between versions (is re-generated). It is used for all secret management in PPM, including JWT tokens, encrypted configuration values like database passwords, and credentials for git builders. Logic needs to be added to manage the key across versions if you want things like API keys to persist. Follow the docs here and consider setting the supported env var PACKAGEMANAGER_ENCRYPTION_KEY."
  },
  {
    "objectID": "work/secrets-and-pinning-files.html",
    "href": "work/secrets-and-pinning-files.html",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "",
    "text": "Imagine a world where you can sleep peacefully at night knowing that your secrets (that you need for connecting to data sources) are being handled responsibly and will never leak."
  },
  {
    "objectID": "work/secrets-and-pinning-files.html#built-in-with-your-version-control",
    "href": "work/secrets-and-pinning-files.html#built-in-with-your-version-control",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "Built in with your version control",
    "text": "Built in with your version control\nGithub has secret scanning for enterprise accounts or public repositories: https://docs.github.com/en/code-security/secret-scanning/introduction/about-secret-scanning"
  },
  {
    "objectID": "work/secrets-and-pinning-files.html#environment-variables",
    "href": "work/secrets-and-pinning-files.html#environment-variables",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "Environment variables",
    "text": "Environment variables\nWhen working with pulling data from secure databases or other sources a developer might find themselves in a situation of needing to provide very sensitive information, such as a password or a token, in order to access the data that is needed or to successfully deploy a project. Handling those secrets in way that doesn’t expose them in the code directly is critical and where using environmental variable’s for securing sensitive variables is strongly recommended.\nAdditionally there may be parameters that are often needed that can be accessed as a variable more easily rather than having to type in every time.\nFor both of these cases knowing how environment variables can be leveraged can be very rewarding and it is surprising how little effort it can to take to set up."
  },
  {
    "objectID": "work/secrets-and-pinning-files.html#python-os-package",
    "href": "work/secrets-and-pinning-files.html#python-os-package",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "Python: os package",
    "text": "Python: os package\nimport os\n\n# Setting a new environment variable\nos.environ[\"API_KEY\"] = \"YOUR_API_KEY\"\n\n# Retrieving the environment variable\nvar = os.environ[\"variable_name\"]"
  },
  {
    "objectID": "work/secrets-and-pinning-files.html#r-usethis-package",
    "href": "work/secrets-and-pinning-files.html#r-usethis-package",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "R: usethis package",
    "text": "R: usethis package\nusethis has a function for creating and editing the .Renviron file:\nlibrary(usethis)\n\n# Edit the global .Renviron file\nusethis::edit_r_environ()\n\n# Edit the project specific .Renviron file\nusethis::edit_r_environ(scope = \"project\")\nAdd the variables to that file in the format variable_name = “variable_value” and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE with session -&gt; restart R.\n# Saved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhile it’s recommended to add env vars to the environ file and not use in your code (otherwise it defeats the point), you can set an env var on the fly with:\nSys.setenv(\"name\" = \"value\")"
  },
  {
    "objectID": "work/secrets-and-pinning-files.html#r-working-with-the-.renviron-file",
    "href": "work/secrets-and-pinning-files.html#r-working-with-the-.renviron-file",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "R: Working with the .Renviron file",
    "text": "R: Working with the .Renviron file\nWhen R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files: - User - Project\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\nExample with a user level .Renviron config file\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\nExample with a project level .Renviron config file\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\nGitignore\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\nExample excluding a number of pieces that would be undesirable to check into version control:\n\n\n.gitignore\n\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# Example code in package build process\n*-Ex.R\n\n# Output files from R CMD build\n/*.tar.gz\n\n# Output files from R CMD check\n/*.Rcheck/\n\n# RStudio files\n.Rproj.user/\n\n# produced vignettes\nvignettes/*.html\nvignettes/*.pdf\n\n# OAuth2 token, see https://github.com/hadley/httr/releases/tag/v0.3\n.httr-oauth\n\n# knitr and R markdown default cache directories\n/*_cache/\n/cache/\n\n# Temporary files created by R markdown\n*.utf8.md\n*.knit.md\n\n# Shiny token, see https://shiny.rstudio.com/articles/shinyapps.html\nrsconnect/\n\n# Deployment details from rsconnect-python\nrsconnect-python/\n\n# Temporary files\n.DS_Store\n__pycache__\n.ipynb_checkpoints\n\nrmarkdown-notebook/flights.csv\n\n.venv\nvenv\n.env\n.Rprofile\n\n/.luarc.json\n\n# OS and python artifacts\n.DS_Store\n__pycache__/\n*.py[cod]\n\n# docs artifacts\n/docs/_site\n\n# Package Manager license\n/ppm.lic\n\n\n\nExample with project level github secrets for environment variables\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\nReferences for adding environment variables through the Connect UI\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI."
  },
  {
    "objectID": "work/secrets-and-pinning-files.html#handling-secret-files",
    "href": "work/secrets-and-pinning-files.html#handling-secret-files",
    "title": "Handling secrets and pinning files to Posit Connect",
    "section": "Handling secret files",
    "text": "Handling secret files\nOptions:\n\nbase64 encode the json string, set an environment variable, read the env var, and base64 decode it - If it fits in an env var, you could always cache it from the env var to your temp dir right before you use\nSave it somewhere on Connect, let developers know the path\nUse a supervisor script to save into the sandbox environment\nPossibly save it as a pin?\n\n\nUseful linux commands and generating a dummy key file\n# cd ~/.ssh\nopenssl genrsa -out test_key 4096\nopenssl rsa -in test_key -pubout -out test_key.pub\nopenssl pkcs8 -topk8 -inform pem -in test_key -outform PEM -v2 aes-256-cbc -out test_key.p8\n\nThe second command generate a RDS private key (unencrypted). https://www.openssl.org/docs/man1.1.1/man1/openssl-genrsa.html\nThe third command generate a public key for the private key above.\nThe fourth command generate an encrypted private key using a cipher aes-256-cbc. Please refer the following URL for more about AES 256 CBC.https://datatracker.ietf.org/doc/html/rfc3602\n\n# Set permissions \nchmod -R 400 ~/.ssh/mykey.pem \n\n# Add key to ssh-agent \nssh-agent -s\neval `ssh-agent -s`\nssh-add ~/.ssh/mykey.pem\nIf you already have a ssh key and want to get the key.pub public key info:\nssh-keygen -y -f ~/.ssh/mykey.pem\nssh-keygen -y -f ~/.ssh/mykey.pem &gt; key.pub\n\n\nPin the key to Connect\n\nSetup\nEnv variables that need to be set are:\nCONNECT_SERVER=&lt;server, eg https://colorado.posit.co&gt;\nCONNECT_API_KEY=&lt;API key from Connect server&gt;\n\n\nCode for pinning a file\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\nlibrary(pins)\nlibrary(rsconnect)\nlibrary(usethis)\n\n#Check our environment variables\n# usethis::edit_r_environ()\n#Needs: \n# CONNECT_SERVER=&lt;server&gt;\n# CONNECT_API_KEY=&lt;API key from Connect server&gt;\n\nboard &lt;- board_connect(auth = \"envvar\")\n\nboard %&gt;% pin_upload(paths=\"test_key\", name=\"test_key\")\nboard %&gt;% pin_upload(paths=\"test_key.p8\", name=\"test_key-p8\")\nboard %&gt;% pin_upload(paths=\"test_key.pub\", name=\"test_key-pub\")\n\ncache_path=board %&gt;% pin_download(name=\"test_key\")\nprint(cache_path)\n\n\nPython\nPython pins package has parity: https://rstudio.github.io/pins-python/reference/pin_upload.html#pins.boards.BaseBoard.pin_upload\n\n\nEncode key file as base64 json string\nlibrar(readr)\nlibrary(openssl)\n\n# Locally, encode the key and save as environment variable\nkey = openssl::base64_encode(readr::read_file(file=\"test_key.pub\"))\n\n# Save as env var\nSys.setenv(\"TEST_SSH_KEY\" = \"123\")\ntest_key_set &lt;- Sys.getenv(\"TEST_SSH_KEY\")\n\n# Create a tempfile in the sandboxed current location\ncached_key &lt;- tempfile()\n\n# Cache the SSH file from the environment variable\nreadr::write_file(openssl::base64_decode(Sys.getenv(\"TEST_SSH_KEY\")), file = cached_key)\nit may be useful to nest in an if statement so that this only happens on Connect, but on Workbench it will still use the managed credentials:\n# Use SSH key if on Connect, otherwise use managed credentials\nif (Sys.getenv(\"RSTUDIO_PRODUCT\") == \"CONNECT\"){\n}"
  },
  {
    "objectID": "work/sharepoint-oh-no.html",
    "href": "work/sharepoint-oh-no.html",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": ":warning: This is now outdated. Please refer to this article and the Microsoft365R package documentation developed by Hong Ooi instead for up-to-date information.\n\n\n\nMicrosoft 365 is a subscription extension of the Microsoft Office product line with cloud hosting support. Microsoft 365 uses Azure Active Directory (Azure AD) for user authentication and application access through developed APIs. The Microsoft supported method for interfacing with R developed content is with the Microsoft365R package which was developed by Hong Ooi and has extensive documentation. It supports access to Teams, SharePoint Online, Outlook, and OneDrive.\n\n\n\n\n:warning: Discussion between the developers and the Global Azure Administration team about the content and security requirements within your organization should determine which of the approaches should be supported.\n\nThere are four main authentication approaches supported by Microsoft365R. Note that multiple approaches can be supported at the same time.\n\n\n\n\n\n\n\n\n\nMethod\nauth_type\nPermissions\nCapability\n\n\n\n\nUser sign-in flow: Default\ndefault\nUser\nInteractive only (local IDE and Workbench, interactive Shiny content)\n\n\nUser sign-in flow: Device Code\ndevice_code\nUser\nInteractive only (local IDE and Workbench)\n\n\nService principal / Client secret\nclient_credentials\nApplication\nInteractive and non-interactive (same as above plus scheduled content)\n\n\nEmbedded credentials\nresource_owner\nUser\nInteractive and non-interactive (same as above plus scheduled content)\n\n\n\nAuthentication for Microsoft365R is through Microsoft’s Azure cloud platform through a registered application with appropriate assigned permissions in order to obtain ‘OAuth 2.0’ tokens.\n\n\nDepending on your organization’s security policy some steps may require support from your Azure Global Administrator.\nUser Sign-in Flow: Default\nA custom app can be created or the default app registration “d44a05d5-c6a5-4bbb-82d2-443123722380” that comes with the Microsoft365R package can be used. The user permissions will need to be enabled as detailed in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator. Additionally Redirect URLs will need to be added through Azure under App Registrations -&gt; select your app -&gt; Authentication -&gt; Platform configurations -&gt; Mobile and desktop applications -&gt;Add URI as well as also enabling nativeclient.\nFor adding Redirect URLs, which will give a typical web-app authentication experience for interactive applications:\n\nFor the desktop RStudio IDE the URL is: http://localhost:1410/.\nFor content hosted in shinyapps.io this would be of the form https://youraccount.shinyapps.io/appname (including the port number if specified).\nA SSL certificate will be required for non-local connections. This means that the Connect and Workbench URLs will need to be HTTPS. A wildcard could be used instead of adding the Redirect URL for each piece of content/user where appropriate for server-wide access.\n\nUser Sign-in Flow: Device Code\nIn addition to user level app permissions outlined above the device code workflow option will need to be enabled.\nEnabling the device code workflow is through the App Registration dashboard in Azure -&gt; click on the created app -&gt; Authentication -&gt; Allow public client flows and setting Enable the following mobile and desktop flows to yes. The device code workflow does not need Redirect URLs, instead providing a code and a link for the developer to access in a separate browser window (or even on a separate device) for sign-in.\nService Principal / Client Secret\nA custom app will need to be registered in Azure with Application permissions. The permissions can be based off of the user permissions but can be assigned as needed for the application and to comply with any security restrictions.\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. As a control using environmental variable’s for storing the client secret is recommended. Starting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\nEmbedded Credentials\nA custom app will need to be registered in Azure with User permissions as specified in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator.\nThe credentials being embedded can be a user or a service account, as long as access to the desired content inside Microsoft 365 has been granted. Creating service accounts per content is recommended to enable faster troubleshooting and easier collaboration. As a control the Username / Password should never be exposed directly in the code, instead using Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\n\n\n\n\n\n\nThe user sign-in flow option provides the typical web browser authentication experience. A user will need to be available to interact with the authentication pop-up in order to which makes this an option for interactive applications (such as the local RStudio IDE, Workbench, or an interactive Shiny app), but not applicable for scheduled content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app = app)\n\n\n\nIn some interactive cases it may be easier to use the device code flow where the user is prompted with a code and a link which is opened in a separate screen for logging in. For example for using a Workbench instance that was deployed without an SSL certificate. This does require interaction from the user and as such will not be applicable for scheduled content nor hosted content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette, with the Service Principal via using a Client Secret discussed in this section being the Microsoft recommended approach.\n\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. Instead the recommended approach is to store it as an Environment Variable which can be done through the Connect UI.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette. In cases where the additional access that comes with Application level permissions isn’t appropriate for the organization’s security requirements the embedded credentials approach can be used.\n\nThe credentials embedded will need to be granted access to the desired content and can either be a user or a service account. Working with your Azure Global Administrator to create service accounts per content is recommended to enable fast troubleshooting and easier collaboration.\nSensitive variables such username / password should be embedded as Environment Variables so that they aren’t exposed in the code directly.which can be done through the Connect UI. See the example here.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nuser &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_USER\")\npwd &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_PASSWORD\")\n\n# Create auth token cache directory, otherwise it will prompt the user on the console for input\ncreate_AzureR_dir()\n\n# create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, \n                    username = user, \n                    password = pwd,\n                    auth_type=\"resource_owner\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nIn the case of authentication failures clearing cached authentication tokens/files can be done with:\nlibrary(AzureAuth)\nlibrary(AzureGraph)\n\ntenant = MyTenant\n\nAzureAuth::clean_token_directory()\nAzureGraph::delete_graph_login(tenant=\"mytenant\")\n\n\n\n\n\n\nThe authentication method used in this example could be swapped out for any of the examples shown above. The documentation on Microsoft365R contains extensive examples beyond what is included below.\nlibrary(Microsoft365R)\nlibrary(AzureGraph)\nlibrary(AzureAuth)\n\nsite_url = MySharepointSiteURL\ntenant = MyTenant\napp = MyApp\ndrive_name = MyDrive # For example by default this will likely be \"Documents\"\nfile_src = MyFileName.TheExtension\n\n# Add sensitive variables as environment variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory, otherwise it will prompt the the console for input\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a SharePoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n# An example using the SharePoint site to get to a specific drive\ndrv &lt;- site$get_drive(drive_name)\n\n# Download a specific file\ndrv$download_file(src = file_src, dest = \"tmp.csv\", overwrite = TRUE)\n\n# Retrieve lists of the different types of items in our sharepoint site. Documents uploaded under the 'Documents' drive are retrieved with list_files(). \ndrv$list_items()\ndrv$list_files() \ndrv$list_shared_files()\ndrv$list_shared_items()\n\n# Files can also be uploaded back to SharePoint\ndrv$upload_file(src = file_dest, dest = file_dest)\n\n\n\nMicrosoft resources can be used for hosting data in pins format using board_ms365() from pins. The authentication method used in this example could be swapped out for any of the examples shown above.\nlibrary(Microsoft365R)\nlibrary(pins)\n\nsite_url = MySite\napp=MyApp\n\n# Create a Microsoft Graph login\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n# An example getting the default drive \ndoclib &lt;- site$get_drive()\n\n# Connect ms365 as a pinned board. If this folder doesn't already exist it will be created on execution. \nboard &lt;- board_ms365(drive = doclib, \"general/project1/board\")\n\n# Write a dataset as a pin to Sharepoint\nboard %&gt;% pin_write(iris, \"iris\", description = \"This is a test\")\n\n# View the metadata of the pin we just created \nboard %&gt;% pin_meta(\"iris\")\n\n# Read the pin\ntest &lt;- board %&gt;% pin_read(\"iris\")\n\n\n\n\nThere are a few cases not covered in this article where the below resources may be useful:\n\nFor user level authentication into servers refer to the Marketplace offering and the Connect documentation.\nFor Python users the Microsoft REST API is the Microsoft developed method with examples.\nAs a last resort, mapping SharePoint, OneNote, or other systems as a network drive to the hosting server could be considered, using a program such as expandrive.\n\n\n\n\nOn the off chance that anyone makes it to the end this article got a chuckle out of me and may be relatable: https://www.theregister.com/2022/07/15/on_call/"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#introduction",
    "href": "work/sharepoint-oh-no.html#introduction",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": "Microsoft 365 is a subscription extension of the Microsoft Office product line with cloud hosting support. Microsoft 365 uses Azure Active Directory (Azure AD) for user authentication and application access through developed APIs. The Microsoft supported method for interfacing with R developed content is with the Microsoft365R package which was developed by Hong Ooi and has extensive documentation. It supports access to Teams, SharePoint Online, Outlook, and OneDrive."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#summary",
    "href": "work/sharepoint-oh-no.html#summary",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": ":warning: Discussion between the developers and the Global Azure Administration team about the content and security requirements within your organization should determine which of the approaches should be supported.\n\nThere are four main authentication approaches supported by Microsoft365R. Note that multiple approaches can be supported at the same time.\n\n\n\n\n\n\n\n\n\nMethod\nauth_type\nPermissions\nCapability\n\n\n\n\nUser sign-in flow: Default\ndefault\nUser\nInteractive only (local IDE and Workbench, interactive Shiny content)\n\n\nUser sign-in flow: Device Code\ndevice_code\nUser\nInteractive only (local IDE and Workbench)\n\n\nService principal / Client secret\nclient_credentials\nApplication\nInteractive and non-interactive (same as above plus scheduled content)\n\n\nEmbedded credentials\nresource_owner\nUser\nInteractive and non-interactive (same as above plus scheduled content)\n\n\n\nAuthentication for Microsoft365R is through Microsoft’s Azure cloud platform through a registered application with appropriate assigned permissions in order to obtain ‘OAuth 2.0’ tokens.\n\n\nDepending on your organization’s security policy some steps may require support from your Azure Global Administrator.\nUser Sign-in Flow: Default\nA custom app can be created or the default app registration “d44a05d5-c6a5-4bbb-82d2-443123722380” that comes with the Microsoft365R package can be used. The user permissions will need to be enabled as detailed in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator. Additionally Redirect URLs will need to be added through Azure under App Registrations -&gt; select your app -&gt; Authentication -&gt; Platform configurations -&gt; Mobile and desktop applications -&gt;Add URI as well as also enabling nativeclient.\nFor adding Redirect URLs, which will give a typical web-app authentication experience for interactive applications:\n\nFor the desktop RStudio IDE the URL is: http://localhost:1410/.\nFor content hosted in shinyapps.io this would be of the form https://youraccount.shinyapps.io/appname (including the port number if specified).\nA SSL certificate will be required for non-local connections. This means that the Connect and Workbench URLs will need to be HTTPS. A wildcard could be used instead of adding the Redirect URL for each piece of content/user where appropriate for server-wide access.\n\nUser Sign-in Flow: Device Code\nIn addition to user level app permissions outlined above the device code workflow option will need to be enabled.\nEnabling the device code workflow is through the App Registration dashboard in Azure -&gt; click on the created app -&gt; Authentication -&gt; Allow public client flows and setting Enable the following mobile and desktop flows to yes. The device code workflow does not need Redirect URLs, instead providing a code and a link for the developer to access in a separate browser window (or even on a separate device) for sign-in.\nService Principal / Client Secret\nA custom app will need to be registered in Azure with Application permissions. The permissions can be based off of the user permissions but can be assigned as needed for the application and to comply with any security restrictions.\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. As a control using environmental variable’s for storing the client secret is recommended. Starting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\nEmbedded Credentials\nA custom app will need to be registered in Azure with User permissions as specified in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator.\nThe credentials being embedded can be a user or a service account, as long as access to the desired content inside Microsoft 365 has been granted. Creating service accounts per content is recommended to enable faster troubleshooting and easier collaboration. As a control the Username / Password should never be exposed directly in the code, instead using Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#authentication-examples",
    "href": "work/sharepoint-oh-no.html#authentication-examples",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": "The user sign-in flow option provides the typical web browser authentication experience. A user will need to be available to interact with the authentication pop-up in order to which makes this an option for interactive applications (such as the local RStudio IDE, Workbench, or an interactive Shiny app), but not applicable for scheduled content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app = app)\n\n\n\nIn some interactive cases it may be easier to use the device code flow where the user is prompted with a code and a link which is opened in a separate screen for logging in. For example for using a Workbench instance that was deployed without an SSL certificate. This does require interaction from the user and as such will not be applicable for scheduled content nor hosted content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette, with the Service Principal via using a Client Secret discussed in this section being the Microsoft recommended approach.\n\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. Instead the recommended approach is to store it as an Environment Variable which can be done through the Connect UI.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette. In cases where the additional access that comes with Application level permissions isn’t appropriate for the organization’s security requirements the embedded credentials approach can be used.\n\nThe credentials embedded will need to be granted access to the desired content and can either be a user or a service account. Working with your Azure Global Administrator to create service accounts per content is recommended to enable fast troubleshooting and easier collaboration.\nSensitive variables such username / password should be embedded as Environment Variables so that they aren’t exposed in the code directly.which can be done through the Connect UI. See the example here.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nuser &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_USER\")\npwd &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_PASSWORD\")\n\n# Create auth token cache directory, otherwise it will prompt the user on the console for input\ncreate_AzureR_dir()\n\n# create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, \n                    username = user, \n                    password = pwd,\n                    auth_type=\"resource_owner\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nIn the case of authentication failures clearing cached authentication tokens/files can be done with:\nlibrary(AzureAuth)\nlibrary(AzureGraph)\n\ntenant = MyTenant\n\nAzureAuth::clean_token_directory()\nAzureGraph::delete_graph_login(tenant=\"mytenant\")"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#sharepoint-examples",
    "href": "work/sharepoint-oh-no.html#sharepoint-examples",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": "The authentication method used in this example could be swapped out for any of the examples shown above. The documentation on Microsoft365R contains extensive examples beyond what is included below.\nlibrary(Microsoft365R)\nlibrary(AzureGraph)\nlibrary(AzureAuth)\n\nsite_url = MySharepointSiteURL\ntenant = MyTenant\napp = MyApp\ndrive_name = MyDrive # For example by default this will likely be \"Documents\"\nfile_src = MyFileName.TheExtension\n\n# Add sensitive variables as environment variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory, otherwise it will prompt the the console for input\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a SharePoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n# An example using the SharePoint site to get to a specific drive\ndrv &lt;- site$get_drive(drive_name)\n\n# Download a specific file\ndrv$download_file(src = file_src, dest = \"tmp.csv\", overwrite = TRUE)\n\n# Retrieve lists of the different types of items in our sharepoint site. Documents uploaded under the 'Documents' drive are retrieved with list_files(). \ndrv$list_items()\ndrv$list_files() \ndrv$list_shared_files()\ndrv$list_shared_items()\n\n# Files can also be uploaded back to SharePoint\ndrv$upload_file(src = file_dest, dest = file_dest)\n\n\n\nMicrosoft resources can be used for hosting data in pins format using board_ms365() from pins. The authentication method used in this example could be swapped out for any of the examples shown above.\nlibrary(Microsoft365R)\nlibrary(pins)\n\nsite_url = MySite\napp=MyApp\n\n# Create a Microsoft Graph login\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n# An example getting the default drive \ndoclib &lt;- site$get_drive()\n\n# Connect ms365 as a pinned board. If this folder doesn't already exist it will be created on execution. \nboard &lt;- board_ms365(drive = doclib, \"general/project1/board\")\n\n# Write a dataset as a pin to Sharepoint\nboard %&gt;% pin_write(iris, \"iris\", description = \"This is a test\")\n\n# View the metadata of the pin we just created \nboard %&gt;% pin_meta(\"iris\")\n\n# Read the pin\ntest &lt;- board %&gt;% pin_read(\"iris\")"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#other-microsoft-related-resources",
    "href": "work/sharepoint-oh-no.html#other-microsoft-related-resources",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": "There are a few cases not covered in this article where the below resources may be useful:\n\nFor user level authentication into servers refer to the Marketplace offering and the Connect documentation.\nFor Python users the Microsoft REST API is the Microsoft developed method with examples.\nAs a last resort, mapping SharePoint, OneNote, or other systems as a network drive to the hosting server could be considered, using a program such as expandrive."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#end",
    "href": "work/sharepoint-oh-no.html#end",
    "title": "Microsoft 365 / Sharepoint and R",
    "section": "",
    "text": "On the off chance that anyone makes it to the end this article got a chuckle out of me and may be relatable: https://www.theregister.com/2022/07/15/on_call/"
  },
  {
    "objectID": "work/system-dependencies-r.html",
    "href": "work/system-dependencies-r.html",
    "title": "Finding R System Dependencies",
    "section": "",
    "text": "Let’s try to get an environment of packages and understand the system dependencies. This would be useful for fresh installs into a new environment."
  },
  {
    "objectID": "work/system-dependencies-r.html#using-pak-and-renv-to-find-r-system-dependencies",
    "href": "work/system-dependencies-r.html#using-pak-and-renv-to-find-r-system-dependencies",
    "title": "Finding R System Dependencies",
    "section": "",
    "text": "Let’s try to get an environment of packages and understand the system dependencies. This would be useful for fresh installs into a new environment."
  },
  {
    "objectID": "work/system-dependencies-r.html#a-note-on-methodology",
    "href": "work/system-dependencies-r.html#a-note-on-methodology",
    "title": "Finding R System Dependencies",
    "section": "A note on methodology",
    "text": "A note on methodology\nIn an ideal world a user will be able to identify what packages they are using. Do you really need that package from 3 years ago that you installed but ended up not using? Technology doesn’t solve all problems! Sometimes the “soft” method is the best way to solve a problem."
  },
  {
    "objectID": "work/system-dependencies-r.html#steps",
    "href": "work/system-dependencies-r.html#steps",
    "title": "Finding R System Dependencies",
    "section": "Steps",
    "text": "Steps\n\nCreate the current environment as a renv project and snapshot it\nlibrary(pak)\nlibrary(renv)\n\n# Or restore a project with renv::restore() if using a pre-existing projectd\nrenv::init()\nrenv::snapshot()\n\n\nFind what OS we are on\n# R.version # Nope\n# version # Nope\n# .Platform # nope\n# .Platform$OS.type # nope\n# Sys.info() # nope\n# Sys.info()[\"sysname\"] # nope\n# system(\"cat /etc/*release\") # closer\n# system(\"lsb_release -a\") # closer\n# pak::system_r_platform_data()$distribution # this is the one!\n\nif(.Platform$OS.type == \"unix\"){\n  Sys.setenv(\"PKG_SYSREQS_PLATFORM\"=pak::system_r_platform_data()$distribution)\n  print(Sys.getenv(\"PKG_SYSREQS_PLATFORM\"))\n} else { ## windows\n  Sys.setenv(\"PKG_SYSREQS_PLATFORM\"=\"windows\") # supported by pak\n  print(Sys.getenv(\"PKG_SYSREQS_PLATFORM\"))\n  warning(\"Windows is not support by pak\")\n}\n\n\nOptionally, recreate the environment on another server using renv and pak\ncp rserver/renv.lock /new-location \n\ncd /new-location && \\\n    echo -e 'options(renv.config.pak.enabled=TRUE)\\noptions(repos=c(CRAN=\"https://packagemanager.posit.co/cran/__linux__/rhel9/2025-03-10\")) Sys.getenv(\"PKG_SYSREQS_PLATFORM\" &gt; .Rprofile && \\\n    R -q -e 'install.packages(c(\"renv\"))' && \\\n    R -q -e 'renv::activate()' && \\\n    R -q -e 'renv::restore()'\n\n\nFind the installed packages\nlibrary(jsonlite)\nlibrary(tidyverse)\nmy_renvlock &lt;- fromJSON(\"renv.lock\")\n\npkgs_df&lt;- map_dfr(my_renvlock$Packages, ~ enframe(.) |&gt;\n  filter(name %in% c(\"Package\", \"Version\")) |&gt;\n  mutate(value = as.character(value)) |&gt;\n  pivot_wider())\n\n\nFind the system dependencies for those installed packages\ndeps&lt;-pak::pkg_sysreqs(pkgs_df$Package, upgrade = FALSE,sysreqs_platform=Sys.getenv(\"PKG_SYSREQS_PLATFORM\"))\n\n# Get the commands to run to install system deps to support the package environment\nif (!identical(deps$pre_install,character(0))) cat(unique(deps$pre_install),sep=\"\\n\")\nif (!identical(deps$install_scripts,character(0))) cat(deps$install_scripts,sep=\"\\n\")\nif (!identical(deps$post_install,character(0))) cat(unique(deps$post_install),sep=\"\\n\")\n\n\nAlternatively, find the most common package downloaded from package manager and support those\nCredit: https://github.com/rstudio/rstudio-pro/issues/6536#issuecomment-2694317773\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(pak)\n\nbase_url &lt;- \"https://packagemanager.posit.co/__api__/metrics/packages\"\nquery_params &lt;- list(\n  \"_sort\" = \"count\",\n  \"_order\" = \"desc\",\n  \"_days\" = 30,\n  \"_sourceType\" = \"r\",\n  \"_limit\" = 1000\n)\n\n# Make the GET request\nresponse &lt;- GET(url = base_url, query = query_params)\n\nif (status_code(response) == 200) {\n  # Parse the JSON content\n  content &lt;- content(response, \"text\")\n  data &lt;- fromJSON(content)\n  \n  # Set platform name ()\n  Sys.setenv(\"PKG_SYSREQS_PLATFORM\"=\"rockylinux-9\")\n  # extract system reqs using pak\n  deps&lt;-pak::pkg_sysreqs(data$name)\n  \n  # Print commands\n  if (!identical(deps$pre_install,character(0))) cat(unique(deps$pre_install),sep=\"\\n\")\n  if (!identical(deps$install_scripts,character(0))) cat(deps$install_scripts,sep=\"\\n\")\n  if (!identical(deps$post_install,character(0))) cat(unique(deps$post_install),sep=\"\\n\")\n  \n} else {\n  cat(\"Error: Unable to fetch data from P3M. Status code:\", status_code(response), \"\\n\")\n}"
  },
  {
    "objectID": "work/version-control.html",
    "href": "work/version-control.html",
    "title": "Github setup on a new computer",
    "section": "",
    "text": "Resources:"
  },
  {
    "objectID": "work/version-control.html#at-a-glance",
    "href": "work/version-control.html#at-a-glance",
    "title": "Github setup on a new computer",
    "section": "At a glance",
    "text": "At a glance\nCheck for previous credentials and set username if needed:\ngit config --list \ngit config --global user.email \"lisa.anders@posit.co\"\ngit config --global user.name \"leesahanders\"\nSwitch to rstudio terminal, checking for credentials:\ngh::gh_whoami()\nGenerate the PAT:\nusethis::create_github_token()\nCopy the generated PAT to your clipboard. Provide the PAT to this function when asked for it:\ngitcreds::gitcreds_set()\nCheck that it stored with:\ngitcreds_get()\nNow you can clone down a project. For the first time using a PAT it will give you a link to visit for authenticating the first time.\nFor Sagemaker you need to configure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'\n\nSAML SSO error\nYou might then get an error like this:\n\n\n\nSAML SSO error\n\n\nWhich you resolve by opening the URL it puts in the error and clicking through the dialog options."
  },
  {
    "objectID": "work/version-control.html#check-how-a-piece-of-content-was-cloned-from-git",
    "href": "work/version-control.html#check-how-a-piece-of-content-was-cloned-from-git",
    "title": "Github setup on a new computer",
    "section": "Check how a piece of content was cloned from git",
    "text": "Check how a piece of content was cloned from git\ngit remote show origin"
  },
  {
    "objectID": "work/version-control.html#pat",
    "href": "work/version-control.html#pat",
    "title": "Github setup on a new computer",
    "section": "PAT",
    "text": "PAT\nNot the recommended way when on linux - gitcreds doesn’t actually “work” on linux. Tom Mock has chatted w/ Jenny about this a few times, which eventually led to: https://github.com/r-lib/gitcreds/issues/47\nAlternatively, there are some git config options from the terminal. See: https://happygitwithr.com/https-pat.html?q=env#pat-doesnt-persist-on-linux\nusethis::create_github_token()\nI highly recommend selecting “repo”, “user”, and “workflow”.\nCopy the generated PAT to your clipboard. Provide this PAT next time a Git operation asks for your password OR store the PAT explicitly.\ngitcreds::gitcreds_set()\nFix my email:\ngit config --global user.email \"lisa.anders@posit.co\"\nCheck that it stored with:\ngitcreds_get()\nOther useful functions:\nusethis::gh_token_help()\n\nusethis::git_sitrep()\n\ngh::gh_whoami()\nUse the HTTPS address for opening a new project."
  },
  {
    "objectID": "work/version-control.html#ssh-key",
    "href": "work/version-control.html#ssh-key",
    "title": "Github setup on a new computer",
    "section": "SSH key",
    "text": "SSH key\n\nFrom inside RStudio:\nSteps:\n\nGo to Tools -&gt; Global Options -&gt; Git / SVN\nCreate SSH Key\nApprove the key and add a password (if appropriate)\nView Public Key\nCopy that public key that was provided into the SSH and GPG keys section in your git under your profile settings.\n\nYou’ll copy / update code using the SSH method from git.\n\n\nFrom terminal:\nGenerate a new key with: ssh-keygen -t ed25519 -C \"your_email@example.com\"\nAdd your ssh key to the background client: eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_ed25519\nOr find an existing key with: ls -al ~/.ssh\nCopy the key to your clipboard: clip &lt; ~/.ssh/id_ed25519.pub\nclip &lt; ~/.ssh/id_ed25519.pub\nTip: With Windows Subsystem for Linux (WSL), you can use clip.exe. Otherwise if clip isn’t working, you can locate the hidden .ssh folder, open the file in your favorite text editor, and copy it to your clipboard.\nFollow here to add it to your github account.\n\n\nFor various modifications and needing to update passphrases:\nHow to remove / add a password: https://stackoverflow.com/questions/112396/how-do-i-remove-the-passphrase-for-the-ssh-key-without-having-to-create-a-new-ke\nhttps://docs.gitlab.com/ee/gitlab-basics/start-using-git.html\nSet up SSH keys and use with Git. Follow the instructions here.\nYou can check that SSH keys exist and the name with:\nls ~/.ssh\n\nssh-keygen -p\ngit config --global user.name \"leesahanders\"\ngit config --global user.email \"lisa.anders@rstudio.com\"\ngit config --global user.email \"lisa.anders@posit.co\"\ngit config --global user.email \"lisamaeanders@gmail.com\"\ngit config --global --list\neval \"$(ssh-agent -s)\"\nssh-add /root/.ssh/id_ed25519\nGet the key and add to git in UI: clip &lt; /root/.ssh/id_ed25519.pub\nnano /root/.ssh/id_ed25519.pub\nUse the SSH address to clone new projects."
  },
  {
    "objectID": "work/version-control.html#other-tips-and-tricks",
    "href": "work/version-control.html#other-tips-and-tricks",
    "title": "Github setup on a new computer",
    "section": "Other tips and tricks",
    "text": "Other tips and tricks\nIf switching to a project that was shared with you need to run: git config --global --add safe.directory /usr/home/xu.fei/shiny-test1\nAnd then close and reopen the session"
  },
  {
    "objectID": "work/version-control.html#troubleshooting",
    "href": "work/version-control.html#troubleshooting",
    "title": "Github setup on a new computer",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“Unable to Fork”\nThis can happen if you’re using ssh to fetch the remote and you don’t have ssh installed.\nHowever note that support for password authentication was removed on August 13, 2021 from Workbench.\nThis means that one of the methods above needs to be done instead http of using ssh as part of the built in UI for git cloning\n\n\nPersistence in Sagemaker\n“Sessions on SageMaker cannot be suspended and resumed. This is b/c the underlying compute is ephemeral, so any session state is lost.”\nThe behavior that can be expected and is normal is that credentials will work when a session is launched. But the credentials are not being preserved once the session is suspended and you try to resume it.\n\nhttps://usethis.r-lib.org/articles/articles/git-credentials.html\nhttps://blog.djnavarro.net/posts/2021-08-08_git-credential-helpers/\n\n\nChange where the credentials are stored\ngit config --global credential.helper 'store --file ~/.my-credentials'\n\ngit config --global credential.helper 'cache --file ~/.my-credentials --timeout=10000000'\n\n\n\nSAML SSO error"
  },
  {
    "objectID": "work/workbench-custom-login.html",
    "href": "work/workbench-custom-login.html",
    "title": "Creating a custom Workbench login page",
    "section": "",
    "text": "Login can be a challenging process for many users and having the admin provide additional details can be very handy to point folks in the right direction. Also, being able to include your own branding isn’t just a nice jazzy thing but can be a security requirement for some teams to help provide another layer of protection via visual indicators."
  },
  {
    "objectID": "work/workbench-custom-login.html#why-would-someone-want-to-do-this",
    "href": "work/workbench-custom-login.html#why-would-someone-want-to-do-this",
    "title": "Creating a custom Workbench login page",
    "section": "",
    "text": "Login can be a challenging process for many users and having the admin provide additional details can be very handy to point folks in the right direction. Also, being able to include your own branding isn’t just a nice jazzy thing but can be a security requirement for some teams to help provide another layer of protection via visual indicators."
  },
  {
    "objectID": "work/workbench-custom-login.html#background",
    "href": "work/workbench-custom-login.html#background",
    "title": "Creating a custom Workbench login page",
    "section": "Background",
    "text": "Background\nA customer requested that this page be made clearer: https://docs.posit.co/ide/server-pro/admin/authenticating_users/customizing_signin.html\nThey were confused that an example wasn’t built for them at the called out location /opt/config/rstudio-login.html.\nThis support article has some useful examples that are useful: https://support.posit.co/hc/en-us/articles/231327847-Customizing-the-Posit-Workbench-RStudio-Server-Pro-Login-Page\nThere are also some templates in this folder that might be useful to reference: /usr/lib/rstudio-server/www/templates/encrypted-sign-in.html"
  },
  {
    "objectID": "work/workbench-custom-login.html#plain-html",
    "href": "work/workbench-custom-login.html#plain-html",
    "title": "Creating a custom Workbench login page",
    "section": "Plain html",
    "text": "Plain html\nFor this particular customer request I’ve been working on their goal is to modify that actual contents of the login information pane. Here is some (hopefully) useful exploration -\nModifying the config /etc/rstudio/rserver.conf with:\nauth-login-page-html=/etc/rstudio/login.html\nAnd then creating that file /etc/rstudio/login.html with contents like:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Requesting access&lt;/h1&gt;\n    &lt;p&gt;In order to get access to this software a ticket will need to be submitted through IT.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nThis results in a login page with the information appended at the bottom:\n\n\n\nWorkbench login page"
  },
  {
    "objectID": "work/workbench-custom-login.html#with-more-jazz",
    "href": "work/workbench-custom-login.html#with-more-jazz",
    "title": "Creating a custom Workbench login page",
    "section": "With more jazz",
    "text": "With more jazz\nIf we append the instructions on the support article and add a logo to /usr/lib/rstudio-server/www/images then we can jazz things up further.\nNote there are already files/images in /usr/lib/rstudio-server/www/images and in this example instead of using logo.png we made a custom image called frog.png by downloading an image from the internet with: wget http://images.clipartpanda.com/frog-clip-art-frog_green.png -O /usr/lib/rstudio-server/www/images/logo.png\n/etc/rstudio/login.html:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Requesting access&lt;/h1&gt;\n    &lt;p&gt;In order to get access to this software a ticket will need to be submitted through IT.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n&lt;script type=\"text/javascript\"&gt;\nwindow.onload=function(){\n\nvar logo = document.createElement(\"img\");\nlogo.setAttribute(\"src\",\"images/frog.png\");\nlogo.setAttribute(\"height\", \"36px\");\nlogo.setAttribute(\"width\", \"36px\");\nlogo.setAttribute(\"style\", \"float: right;\");\ndocument.getElementById(\"banner\").appendChild(logo);\n\nvar cap = document.getElementById(\"caption_header\");\ncap.innerHTML = \"This is a message added by your admin. Sign into Posit with your assigned credentials credentials\";    \n}\n&lt;/script&gt;\n\n&lt;style&gt;\n#banner {background-color: #4682b4;}\n#caption {border-bottom-color: #4682b4;}\n&lt;/style&gt;\nThis results in a login page with information appended at the bottom as well as custom coloring and a logo and the header for the login blurb changed:\n\n\n\nWorkbench login page"
  },
  {
    "objectID": "work/workbench-custom-login.html#reference",
    "href": "work/workbench-custom-login.html#reference",
    "title": "Creating a custom Workbench login page",
    "section": "Reference",
    "text": "Reference\nGit issue\n\n\n\nWorkbench login page\nWorkbench login page"
  }
]