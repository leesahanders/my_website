[
  {
    "objectID": "site_map.html",
    "href": "site_map.html",
    "title": "Site Map",
    "section": "",
    "text": "Site Map Listing\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nJan 4, 2021\n\n\nBATTLEBOTS 2021\n\n\n9 min\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nResin Dice\n\n\n12 min\n\n\n\n\n\n\n\nJul 28, 2022\n\n\nWhat is Quarto?\n\n\n1 min\n\n\n\n\n\n\n\nAug 2, 2022\n\n\nA little data goes a long way\n\n\n1 min\n\n\n\n\n\n\n\nAug 3, 2022\n\n\nAI generated art\n\n\n2 min\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nPhone games\n\n\n1 min\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nObservable JS for snappy datascience\n\n\n1 min\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nChatbot\n\n\n4 min\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSound\n\n\n4 min\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nFord Maverick Soft Topper installation\n\n\n1 min\n\n\n\n\n\n\n\nAug 6, 2023\n\n\nVortico rockets\n\n\n1 min\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nProblems with persistence when in the cloud\n\n\n3 min\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nFord Maverick Truck Cubby Holder\n\n\n2 min\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\n4 min\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nThe importance of good programming\n\n\n6 min\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nWorkplace articles\n\n\n1 min\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nCoding write-ups and resources\n\n\n1 min\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nHey Google - make me coffee!\n\n\n2 min\n\n\n\n\n\n\n\n \n\n\n \n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nTechnical writeups and musings\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nPresentations\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nLists\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nHome\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nBlog posts and random musings\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nAbout\n\n\n1 min\n\n\n\n\n\nNo matching items\n\n\nRefer to: https://quarto.org/docs/websites/website-listings.html"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Check out some of the presentations I’ve made while working at Posit:\n\n2024\n\n\n\n\n\n&gt;\n\n\nIt’s dangerous to go alone - take this!\n\n\nLisa’s best practices for pain-free data science\n\n\n Check out the slides Check out the repo\n\n\n\n\n\n2023 and 2022\n\n\n\n\n\n&gt;\n\n\nReporting in R with Posit Team\n\n\nUsing the Posit Team set of products to create data science reports.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nImproving app performance\n\n\nAnalyzing your R scripts for performance improvements using profvis, shinyloadtest, and shinytest2.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nReproduceable Workflows\n\n\nCreating reproduceable data science workflows.\n\n\nCheck out the slides"
  },
  {
    "objectID": "posts-no-pictures.html",
    "href": "posts-no-pictures.html",
    "title": "Technical writeups and musings",
    "section": "",
    "text": "Some random technical writeups for things I’ve found useful, all brought to you by Quarto’s blogging capability.\n\n\n\n\n\n\n\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nLisa\n\n\n4 min\n\n\n\n\n\n\n\nProblems with persistence when in the cloud\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nLisa\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts-no-pictures/delta-lake-and-azure.html",
    "href": "posts-no-pictures/delta-lake-and-azure.html",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "",
    "text": "This is some work I did exploring how to access the underlying databricks data storage, without having to go through databricks. Wanted to squirrel this aware so it’s easy to find in the future!"
  },
  {
    "objectID": "posts-no-pictures/delta-lake-and-azure.html#azure-data-lake",
    "href": "posts-no-pictures/delta-lake-and-azure.html#azure-data-lake",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Azure Data Lake",
    "text": "Azure Data Lake\n\nSet up\nLanding page for Azure: &lt;https://portal.azure.com/ &gt;\nFollow this article: &lt;https://learn.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account &gt;\nThe trick: ADL isn’t it’s own separate category, it gets created as part of a storage account.\nSteps:\n\nGo to storage account\nCreate and give it a name\nSelect: LRS\nSwitch to premium: block blobs\nChange to hierarchical blob\nSet tags:\n\n\nrs:owner\nrs:project = soleng\nrs:environment = dev\n\n\nOnce in just need access keys or shared access signature in order to gain access\n\n\n\nAdd data\nYou can add data manually by creating a container and then using the upload icon.\n\n\n\nimage"
  },
  {
    "objectID": "posts-no-pictures/delta-lake-and-azure.html#authentication",
    "href": "posts-no-pictures/delta-lake-and-azure.html#authentication",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Authentication",
    "text": "Authentication\nAccess your authentication details through the Access Keys or Shared Access Signature links on the left. I prefer Access Keys since they are easier to use.\nFor authentication from an R script we’ll be using https://github.com/Azure/AzureStor\nYou’ll need to know:\n\nThe Blob endpoint for your Azure Data Lake storage\nAn Access Key (this can also be done with a Shared Access Signature)\n\nlibrary(AzureStor)\n\nblob_endpoint &lt;- \"https://REDACTED.blob.core.windows.net/\"\n\nbl_endp_key &lt;- storage_endpoint(blob_endpoint, key=\"REDACTED\")\n\n# List containers and files in containers\nlist_storage_containers(bl_endp_key)\ncont1 &lt;- storage_container(bl_endp_key, \"container1\")\nlist_storage_files(cont1)\n\n# Download a file\nstorage_download(cont1, \"/crm_call_center_logs.parquet\")\n\n# Upload a file \nstorage_upload(cont1, \"crm_call_center_logs.parquet\", \"newdir/crm_call_center_logs.parquet\")\nYou can also create and delete containers:\n# Create a container\nnewcont &lt;- create_storage_container(bl_endp_key, \"container3\")\n\n# Create a directory in the container\ncont3 &lt;- storage_container(bl_endp_key, \"container3\")\ncreate_storage_dir(cont3, \"newdir\")\n\n# Delete a container\ndelete_storage_container(newcont)"
  },
  {
    "objectID": "posts-no-pictures/delta-lake-and-azure.html#reading-and-writing-delta-files",
    "href": "posts-no-pictures/delta-lake-and-azure.html#reading-and-writing-delta-files",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Reading and Writing Delta Files",
    "text": "Reading and Writing Delta Files\nDelta files can be read by using the sparklyr package: https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_read_delta.html Thanks for the help with the magic incantation below!\nIn order to do this we will need to manage a Spark cluster. We can run it in local mode so that we aren’t connecting to an external cluster with billing:\nlibrary(sparklyr)\n\n#Install a local version of Spark\nspark_install(version=3.4)\n\n# Set Spark configuration to be able to read delta tables\nconf &lt;- spark_config()\nconf$`spark.sql.extensions` &lt;- \"io.delta.sql.DeltaSparkSessionExtension\"\nconf$`spark.sql.catalog.spark_catalog` &lt;- \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n\n# For spark 3.4 \nconf$sparklyr.defaultPackages &lt;- \"io.delta:delta-core_2.12:2.4.0\"\n\n# Open a connection\nsc &lt;- spark_connect(\"local\", version = 3.4, packages = \"delta\", conf = conf)\n\n# For this example we will use a built-in dataframe to save example data files, including one for delta tables\ntbl_mtcars &lt;- copy_to(sc, mtcars, \"spark_mtcars\")\n\n# Write spark dataframe to disk\nspark_write_csv(tbl_mtcars,  path = \"test_file_csv\", mode = \"overwrite\")\nspark_write_parquet(tbl_mtcars,  path = \"test_file_parquet\", mode = \"overwrite\")\nspark_write_delta(tbl_mtcars,  path = \"test_file_delta\", mode = \"overwrite\")\n\n# Read dataframes into normal memory\nspark_tbl_handle &lt;- spark_read_csv(sc, path = \"test_file_csv\")\nregular_df_csv &lt;- collect(spark_tbl_handle)\nspark_tbl_handle &lt;- spark_read_parquet(sc, path = \"test_file_parquet\")\nregular_df_parquet &lt;- collect(spark_tbl_handle)\nspark_tbl_handle &lt;- spark_read_delta(sc, path = \"test_file_delta\")\nregular_df_delta &lt;- collect(spark_tbl_handle)\n\n# Disconnect\nspark_disconnect(sc)\nYou should now have normal dataframes in your regular R environment that can be used for further analytics:\n\n\n\nimage\n\n\nNote: For Spark 3.5 you might have success with “io.delta:delta-core_2.12:3.0.0”"
  },
  {
    "objectID": "posts-no-pictures/delta-lake-and-azure.html#troubleshooting",
    "href": "posts-no-pictures/delta-lake-and-azure.html#troubleshooting",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFrom R:\n# See spark details (troubleshooting)\nspark_config()\nspark_get_java()\nspark_available_versions()\nspark_installed_versions()\n\n# See session details\nutils::sessionInfo() \nFrom bash:\nnamei -l /usr/lib/spark\nRecommended troubleshooting: https://spark.rstudio.com/guides/troubleshooting.html"
  },
  {
    "objectID": "posts-no-pictures/delta-lake-and-azure.html#about",
    "href": "posts-no-pictures/delta-lake-and-azure.html#about",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "About",
    "text": "About\n\nAzure Data Lake: Azure Data Lake Storage Gen2 Introduction - Azure Storage\n\nAzure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob Storage.\n\n\nData Lake Storage Gen2 converges the capabilities of Azure Data Lake Storage Gen1 with Azure Blob Storage. For example, Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you’ll also get low-cost, tiered storage, with high availability/disaster recovery capabilities.\n\n\nA superset of POSIX permissions: The security model for Data Lake Gen2 supports ACL and POSIX permissions along with some extra granularity specific to Data Lake Storage Gen2. Settings may be configured through Storage Explorer or through frameworks like Hive and Spark.\n\nTLDR: Azure Data Lake is a place where data can be saved (similar to S3 buckets on Amazon).\n\n\nDelta Tables: https://docs.delta.io/latest/delta-intro.html\n\nDelta Lake is an open source project that enables building a Lakehouse architecture on top of data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\n\nYou can check Delta Lake releases here: https://docs.delta.io/latest/releases.html\nTLDR: Delta tables are a data file format, specifically used with Spark clusters (for example Databricks).\n\n\n\nimage\nimage"
  },
  {
    "objectID": "posts/quarto.html",
    "href": "posts/quarto.html",
    "title": "What is Quarto?",
    "section": "",
    "text": "The obligatory starting template with a couple links."
  },
  {
    "objectID": "posts/quarto.html#quarto",
    "href": "posts/quarto.html#quarto",
    "title": "What is Quarto?",
    "section": "Quarto",
    "text": "Quarto\nIt’s a whole new exciting world! And the default project has a lot of exciting files to explore.\nI’m using:\n\nhttps://quarto.org/docs/websites/website-blog.html\nhttps://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html"
  },
  {
    "objectID": "posts/quarto.html#example",
    "href": "posts/quarto.html#example",
    "title": "What is Quarto?",
    "section": "Example",
    "text": "Example\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "posts/quarto.html#running-code",
    "href": "posts/quarto.html#running-code",
    "title": "What is Quarto?",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "posts/ojs_lyme.html",
    "href": "posts/ojs_lyme.html",
    "title": "Observable JS for snappy datascience",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "posts/ojs_lyme.html#observable-js",
    "href": "posts/ojs_lyme.html#observable-js",
    "title": "Observable JS for snappy datascience",
    "section": "",
    "text": "This Quarto document is made interactive using Observable JS. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about OJS interactive documents at https://quarto.org/docs/interactive/ojs/."
  },
  {
    "objectID": "posts/ojs_lyme.html#data-wrangling",
    "href": "posts/ojs_lyme.html#data-wrangling",
    "title": "Observable JS for snappy datascience",
    "section": "Data wrangling",
    "text": "Data wrangling\nLoad the data, using Lyme data from the CDC.\n\n\nShow the code\ndata = FileAttachment(\"data/lyme_data.csv\").csv({ typed: true })\n\n\n\n\n\n\n\nFilter options\n\n\nShow the code\nviewof state = Inputs.select(data.map(d =&gt; d.Stname), {multiple: true, value: \"Alaska\", label: \"Choose a state: \", sort: true, unique: true})\nfiltered = data.filter(function(data) {\n  return state.includes(data.Stname);\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotData\n\n\nStacked bar chart of cases by state by year, remember to select an option above.\n\n\nShow the code\nPlot.plot({\n  y: {\n    grid: true\n  },\n  marks: [\n    Plot.barY(filtered, {x: \"date\", y: \"total_cases\", fill: \"#bab0ab\"}),\n    &lt;!-- plot.text(filtered, {x: \"date\", y: \"total_cases\", text: \"Stname\", textAnchor: \"end\", dx: -6}),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\nAggregated dataset for all states.\n\n\nShow the code\nInputs.table(data, { sort: \"date\", reverse: true })"
  },
  {
    "objectID": "posts/ojs_lyme.html#references",
    "href": "posts/ojs_lyme.html#references",
    "title": "Observable JS for snappy datascience",
    "section": "References",
    "text": "References\n\nhttps://journals.plos.org/plosone/article?id=10.1371/journal.pone.0260122\nhttps://www.hopkinslymetracker.org/\nhttps://quarto.org/docs/interactive/ojs/\nData from: https://www.cdc.gov/lyme/stats/survfaq.html\\\nAdditional data options: https://data.world/datasets/lyme-disease\nSome plots options using observable: https://github.com/observablehq/plot and https://observablehq.com/@observablehq/plot-bar\nSome various tutorials: https://observablehq.com/tutorials"
  },
  {
    "objectID": "posts/maverick cubby holder.html",
    "href": "posts/maverick cubby holder.html",
    "title": "Ford Maverick Truck Cubby Holder",
    "section": "",
    "text": "Assembled cubby holder fits perfectly!"
  },
  {
    "objectID": "posts/maverick cubby holder.html#libre-cad-httpslibrecad.org",
    "href": "posts/maverick cubby holder.html#libre-cad-httpslibrecad.org",
    "title": "Ford Maverick Truck Cubby Holder",
    "section": "Libre CAD https://librecad.org/",
    "text": "Libre CAD https://librecad.org/\nThis is where the bulk of your work will be done. Some of the key functions to be aware of:\n\nLine, 2 points\nMeasure, distance point to point\nModify, Move/Copy, Rotate, Trim 2, Attributes, Explode text into letters, Mirror\n\nTips:\n\nYou’ll need to change the snap settings at the bottom to change what it connects with.\nWhen drawing a line use the @ symbol to signify from the current reference (instead of absolute). EG. @0,10\nYour actual cut layer should be its own layer separate from a designs / motifs layer.\n\nIn order to move your files (.dxf) into your next software (where it will be layed out on a page for the actual laser cutter to interpret), it will need to be saved/exported (as an .svg)."
  },
  {
    "objectID": "posts/maverick cubby holder.html#inkscape-httpsinkscape.org",
    "href": "posts/maverick cubby holder.html#inkscape-httpsinkscape.org",
    "title": "Ford Maverick Truck Cubby Holder",
    "section": "Inkscape https://inkscape.org/",
    "text": "Inkscape https://inkscape.org/\nThis is where your design is laid out for the laser cutter to cut.\n\nObject, Group, Transform\nEdit, Resize Page to Selection"
  },
  {
    "objectID": "posts/maverick cubby holder.html#all-about-tabs",
    "href": "posts/maverick cubby holder.html#all-about-tabs",
    "title": "Ford Maverick Truck Cubby Holder",
    "section": "All about tabs",
    "text": "All about tabs\n3.4mm tab thicknesses work with 1/8 inch wood."
  },
  {
    "objectID": "posts/kettle automation.html",
    "href": "posts/kettle automation.html",
    "title": "Hey Google - make me coffee!",
    "section": "",
    "text": "I bought a smart kettle (goveelife-smart-electric-kettle-lite) with the dream of being able to press a button from bed in order to have hot water available for my coffee first thing in the morning.\nWhile this was great, it relies on bluetooth for the app to work. Normally I keep bluetooth off on my phone (it’s safer that way, plus a longer battery!)\nSo I automated it!\n\nSteps\nThese steps were followed with my Android\nStep 1: Install and configure the Govee app to be connected to your water kettle\nStep 2: Install and configure Google Home and integrate with the Govee App. Assign your kettle to the kitchen (or wherever, you do you boo). Optional: Install and configure Google Assistant, for the voice activation.\nStep 3: Create a Google Home Routine\n\nThis is a personal routine, so we have more options (like being able to adjust phone settings if needed).\nStarters: You can have this trigger for different things, like having it be voice activated when you say “hey google, make coffee!”, or when an alarm is dismissed, or at a specific time, etc.\nActions: Adjust home device, turn on the smart kettle (somehow by activating this way, I didn’t need to turn on bluetooth! Which is great, that’s the whole step I wanted to skip)\n\nStep 4: Test that the routine works\nStep 5: Add the routine as an icon to the phone screen (since voice activation isn’t my jam). Long click on the routine you’ve created and then click on the icon with the arrow over the phone to create a widget and place wherever you want on your phone screen.\n\n\n\nCreating a widget\n\n\nStep 6: Enjoy :-)\n\n\nWhat would I do differently next time?\nThere are some cool button options that look they have integration, or even a vibration sensor:\n\nSmart button\nVibration sensor\n\nHonestly though, this suits my lazy efficient self just fine.\n\n\n\nCreating a widget"
  },
  {
    "objectID": "posts/battlebots 2021.html",
    "href": "posts/battlebots 2021.html",
    "title": "BATTLEBOTS 2021",
    "section": "",
    "text": "BattleBots is a robotic combat TV show. It is an amazing community and clearly a hobby of love by the people that join the show. Robots weigh 250lbs, 500 lbs in special circumstances, and there are strict rules around what is and isn’t allowed for weapons. The show spends time both on the 3 minute match and behind the scenes with interviews with the builders, footage of repairs, and last minute modifications as teams compete for the ultimate trophy.\n\n\n\nOfficial BattleBots page\nReddit Battlebots\nSeason 5 Rumor Mill at Reddit/r/Battlebots\nCombat Robot Resource Guide by Robert Cowan\n\n\n\n\nAll rights are owned by Battlebots on ABC\n\nMinotaur vs. BombshellYeti vs. Lock JawTombstone vs. BroncoHypershock + RakeDeath Roll vs. End GameHUGE vs. BroncoWitch Doctor Vs. actual household items"
  },
  {
    "objectID": "posts/battlebots 2021.html#what-is-battlebots",
    "href": "posts/battlebots 2021.html#what-is-battlebots",
    "title": "BATTLEBOTS 2021",
    "section": "",
    "text": "BattleBots is a robotic combat TV show. It is an amazing community and clearly a hobby of love by the people that join the show. Robots weigh 250lbs, 500 lbs in special circumstances, and there are strict rules around what is and isn’t allowed for weapons. The show spends time both on the 3 minute match and behind the scenes with interviews with the builders, footage of repairs, and last minute modifications as teams compete for the ultimate trophy.\n\n\n\nOfficial BattleBots page\nReddit Battlebots\nSeason 5 Rumor Mill at Reddit/r/Battlebots\nCombat Robot Resource Guide by Robert Cowan\n\n\n\n\nAll rights are owned by Battlebots on ABC\n\nMinotaur vs. BombshellYeti vs. Lock JawTombstone vs. BroncoHypershock + RakeDeath Roll vs. End GameHUGE vs. BroncoWitch Doctor Vs. actual household items"
  },
  {
    "objectID": "posts/battlebots 2021.html#battlebots-analysis",
    "href": "posts/battlebots 2021.html#battlebots-analysis",
    "title": "BATTLEBOTS 2021",
    "section": "Battlebots Analysis",
    "text": "Battlebots Analysis\nBelow is my attempt at diving deep into the world of Battlebots to understand what makes some bots more successful than others. All data is from the Battlebots official website, scraped from the web using the Rvest R package developed by Hadley Wickam.\n\nMessy web scraping to get data\n\n\nShow the code\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(data.table)\nlibrary(stringr)\n#devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\nlibrary(RColorBrewer)\nlibrary(ggmap)\nlibrary(maps)\nlibrary(ggrepel)\nlibrary(sf)\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\nlibrary(htmlwidgets)\nlibrary(webshot)\nlibrary(tidygeocoder)\nlibrary(rgeos)\n\n\n# Set URL list\n#urls = c(\"https://battlebots.com/robot/yeti-2019/\")\n#url &lt;- \"https://battlebots.com/robot/yeti-2019/\"\n\n# Check if data already exists, if not download it\nif(file.exists(\"./files/battlebotsinfo.csv\")){\n  message(\"Loading saved files\")\n  \n  battlebotsinfo = read.csv(\"./files/battlebotsinfo.csv\", stringsAsFactors=FALSE)\n  battlebotsmatch = read.csv(\"./files/battlebotsmatch.csv\", stringsAsFactors=FALSE)\n  battlebotsstat = read.csv(\"./files/battlebotsstat.csv\", stringsAsFactors=FALSE)\n  \n} else {\n  message(\"Generating battlebots data files\")\n  \n  urls = read.csv(\"./files/battlebotsurls.csv\", stringsAsFactors=FALSE)\n  \n  for( i in 1:nrow(urls) ){\n    url = urls[i,1]\n    \n    print(paste0(\"Reading \", i, \" of \", nrow(urls), \": \", url))\n    \n    # Read info and arrange into table \n    # (note that this isn't actually a table in the html source so we had to jump through some hoops to make that happen)\n    info &lt;- read_html(url) %&gt;%\n      html_nodes(\".info-grid\") %&gt;%\n      html_text()\n    \n    info_df_tmp &lt;- map(info, function(x) {\n      tibble(text = unlist(str_split(x, pattern = \"\\\\n\"))) %&gt;%\n        rowid_to_column(var = \"line\")\n    })\n    \n    info_df &lt;- bind_rows(info_df_tmp, .id = \"page\") \n    \n    # Trim off leading whitespace\n    info_trim &lt;- slice(info_df, which.max(text == \"Robot:\") : n()) %&gt;%\n      select(text)\n    \n    # Split into multiple rows , remediate any cases where Values are part of the Record\n    info_split &lt;- info_trim %&gt;%\n      mutate(text = str_trim(text)) %&gt;% \n      mutate(Record = ifelse(grepl(\":\", text, fixed = TRUE), text, NA)) %&gt;%\n      separate(Record, into = c(\"Record\", \"Value\"), sep = \":\", remove = FALSE) %&gt;%\n      fill(Record) %&gt;%\n      mutate(Value = ifelse(is.na(Value), \n               ifelse(grepl(\":\", text, fixed = TRUE), NA, text), Value )) %&gt;%\n      select(Record, Value) %&gt;%\n      filter(!is.na(Value)) %&gt;%\n      filter(!grepl(\"^\\\\s*$\", Value)) %&gt;%\n      unique() %&gt;%\n      group_by(Record) %&gt;%\n      summarize(Value = paste0(Value, collapse = \", \")) %&gt;%\n      ungroup() %&gt;%\n      pivot_wider(names_from = Record, values_from = Value) %&gt;%\n      mutate_all(as.character)\n    \n    # Robot name \n    robot = info_split$Robot\n    \n    # Read both history tables and wrangle into table form\n    history &lt;- read_html(url) %&gt;%\n      html_nodes(\".igsv-table\") \n    \n    if(length(history) &gt; 0){\n    stat_history &lt;- history[1] %&gt;% html_table(header = TRUE, fill = TRUE)\n    stat_history_df &lt;- stat_history[[1]] %&gt;%\n      mutate(Robot = robot) %&gt;%\n      mutate_all(as.character)\n    }\n    \n    if(length(history) &gt; 1){\n      match_history &lt;- history[2] %&gt;% html_table(header = TRUE, fill = TRUE)\n      match_history_df &lt;- match_history[[1]] %&gt;%\n        mutate(Robot = robot) %&gt;%\n        mutate_all(as.character)\n    }\n    \n    # Compile\n    if(i==1){\n      battlebotsinfo = info_split\n      battlebotsmatch = match_history_df\n      battlebotsstat = stat_history_df\n    } else {\n      battlebotsinfo = bind_rows(battlebotsinfo, info_split)\n      \n      if(length(history) &gt; 0){\n        battlebotsstat = bind_rows(battlebotsstat, stat_history_df)\n      }\n      \n      if(length(history) &gt; 1){\n        battlebotsmatch = bind_rows(battlebotsmatch, match_history_df)\n      }\n      \n    }\n    \n  } # End for loop \n  \n  # Save files to make it easier next time\n  write.csv(battlebotsinfo, \"./files/battlebotsinfo.csv\")\n  write.csv(battlebotsmatch, \"./files/battlebotsmatch.csv\")\n  write.csv(battlebotsstat, \"./files/battlebotsstat.csv\")\n\n}\n\n\n# References:\n# https://community.rstudio.com/t/convert-character-string-into-table/9158\n# https://stackoverflow.com/questions/42419765/convert-one-column-into-a-new-column-every-5-rows-a-numeric-interval\n#  - [Scraping in R rvest](https://www.dataquest.io/blog/web-scraping-in-r-rvest/)\n#  - [Tidy Web Scraping in R Tutorial and Resources](https://towardsdatascience.com/tidy-web-scraping-in-r-tutorial-and-resources-ac9f72b4fe47)\n\n\n\n\nShow the code\n### Frequency of Battlebots Weapon Types\n\n# type_frequency &lt;- battlebotsinfo %&gt;%\n#   group_by(Type) %&gt;%\n#   summarize(freq = n()) %&gt;%\n#   rename(\"word\" = \"Type\")\n#   \n# my_wordcloud1 &lt;- wordcloud2(data=type_frequency, size = 0.5, shape = 'pentagon', color='random-dark')\n# \n# my_wordcloud1\n\n#saveWidget(my_wordcloud, \"tmp.html\", selfcontained = F)\n#webshot(\"tmp.html\", \"./images/wc1.png\", delay = 5)\n\n#wordcloud2(demoFreq, color = \"random-light\", backgroundColor = \"grey\")\n\n# https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a\n# https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html#lettercloud-function\n\n\n\n\nFrequency of Builder Day Jobs\n\n\nShow the code\njob_frequency &lt;- battlebotsinfo %&gt;%\n  group_by(Job) %&gt;%\n  summarize(freq = n()) %&gt;%\n  rename(\"word\" = \"Job\")\n\njob_frequency$word &lt;- iconv(job_frequency$word, to = \"UTF-8\")\n\nmy_wordcloud2 &lt;- wordcloud2(data=job_frequency, size = 0.25, color='random-dark')\n\nmy_wordcloud2\n\n\n\n\n\n\n\n\nMap of Builder Locations\n\n\nShow the code\n#Load location data\nif(file.exists(\"./files/battlebotslocation.csv\")){\n  message(\"Loading saved files\")\n  \n  bbmap = read.csv(\"./files/battlebotslocation.csv\", stringsAsFactors=FALSE)\n} else {\n  message(\"Generating battlebots location files\")\n  \n  #Set up the API key for google \n  #This now costs money\n  #Go to: https://cloud.google.com/maps-platform/\n  #register_google(key = \"\", write = TRUE)\n  \n  bbmap &lt;- battlebotsinfo %&gt;%\n    select(Robot, Hometown) %&gt;%\n    mutate(Team = paste(\"Team\", Robot, \", from\", Hometown)) %&gt;%\n    mutate_geocode(Hometown)\n  \n  write.csv(bbmap, \"./files/battlebotslocation.csv\")\n}\n  \nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\np &lt;- ggplot(data = world) + \n  geom_sf(color = \"black\", fill = \"lightgray\") +\n  xlab(\"Longitude\") + ylab(\"Latitude\") +\n  geom_point( data=bbmap, aes(x=lon, y=lat, text=Team), color=\"red\", size=1) +\n  # ggrepel::geom_label_repel(data = bbmap,\n  #            aes(x = lon, y = lat, label = Robot),\n  #            size = 1.5, alpha = 0.8,\n  #            label.r = unit(0.5, \"lines\"), label.size = 0.25,\n  #            segment.color = \"red\", segment.size = 1,\n  #            seed = 1002) +\n  #geom_text_repel(data=bbmap,aes(x = lon, y = lat, label = Robot),box.padding = 0.4,point.padding = 0.25,size=4,show.legend=FALSE)+\n  ggtitle(\"Battlebots Hometowns World map\", subtitle = paste0(\"(\", length(unique(bbmap$Robot)), \" teams)\"))\n\n#pp &lt;- ggplotly(p, tooltip=\"text\")\n\n#assign(\"pp\", plotly::ggplotly(p, tooltip=\"text\"), envir=parent.frame())\n\n#pp\n\np\n\n\n\n\n\n\n\n\n\nShow the code\n#htmlwidgets::saveWidget( plotly::ggplotly(p, tooltip=\"text\"), \"tmp3.html\")\n\n#print( htmltools::tags$iframe(src=\"temp.html\", width=640, height=480) )\n\n# &lt;iframe src=\"temp.html\" width='1000px' height='800px'&gt;&lt;/iframe&gt; \n\n  \n#https://stackoverflow.com/questions/29037851/how-do-i-plot-us-cities-using-ggplot\n#https://stackoverflow.com/questions/22752009/plot-on-ggmap-by-using-city-and-state\n#https://towardsdatascience.com/plotting-spatial-data-in-r-a38a405a07f1\n#https://cran.r-project.org/web/packages/usmap/vignettes/advanced-mapping.html \n#https://www.r-graph-gallery.com/330-bubble-map-with-ggplot2.html\n#https://ggplot2-book.org/maps.html\n#http://yluo86.github.io/rmaps\n#https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html\n#https://www.statsandr.com/blog/world-map-of-visited-countries-in-r/\n#https://cran.r-project.org/web/packages/tidygeocoder/vignettes/tidygeocoder.html\n#https://towardsdatascience.com/reverse-geocoding-in-r-f7fe4b908355\n#https://stackoverflow.com/questions/13905098/how-to-get-the-longitude-and-latitude-coordinates-from-a-city-name-and-country-i\n#https://stackoverflow.com/questions/59144842/why-does-ggplotly-does-not-work-in-rmarkdown-the-same-way-ggplot-does\n\n\n\n\n\nWin Frequency by Robot Type\n\n\nShow the code\n#TODO: Add plot showing frequency of type of robot \n\n# type_frequency &lt;- battlebotsinfo %&gt;%\n#   group_by(Type) %&gt;%\n#   summarize(freq = n()) %&gt;%\n#   rename(\"word\" = \"Type\")\n\nwin_frequency_raw &lt;- merge(battlebotsinfo, battlebotsstat, by=\"Robot\") %&gt;%\n  select(Stats, Career, Robot, Type) %&gt;%\n  filter(Stats %in% c(\"Total matches\", \"Total wins\")) %&gt;%\n  #mutate(Career = as.numeric(Stats)) %&gt;%\n  pivot_wider(names_from = Stats, values_from = Career) %&gt;%\n  unnest(`Total wins`) %&gt;%\n  mutate(`Total wins` = as.numeric(`Total wins`),\n         `Total matches` = as.numeric(`Total matches`))\n\ntype_wins &lt;- win_frequency_raw %&gt;%\n  group_by(Type) %&gt;%\n  summarize(wins = sum(`Total wins`) / sum(`Total matches`))\n\nrobot_wins &lt;- win_frequency_raw %&gt;%\n  group_by(Type) %&gt;%\n  mutate(CountType = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(Robot, Type, CountType) %&gt;%\n  summarize(wins = sum(`Total wins`) / sum(`Total matches`)) %&gt;%\n  filter(CountType &gt; 1) %&gt;%\n  arrange(wins)\n\n\n\np &lt;- ggplot(robot_wins, aes(x=Type, y=wins)) + geom_boxplot() + theme(axis.text.x = element_text(angle = -90, hjust = 1))\n\n\np &lt;- ggplot(robot_wins, aes(x=reorder(Type, -wins, na.rm = TRUE), y=wins)) + geom_boxplot() + theme(axis.text.x = element_text(angle = -90, hjust = 1)) +  labs(y=\"Wins\", x=\"Type\")\np"
  },
  {
    "objectID": "posts/battlebots 2021.html#resources",
    "href": "posts/battlebots 2021.html#resources",
    "title": "BATTLEBOTS 2021",
    "section": "Resources",
    "text": "Resources\nBattlebots:\n\nOfficial BattleBots page\nReddit Battlebots\nSeason 5 Rumor Mill at Reddit/r/Battlebots\nCombat Robot Resource Guide by Robert Cowan\n\nResources for building a similar RMarkdown page:\n\nCreating dynamic tabs in Rmarkdown\nHTML Special Features: KU CRMDA Markdown\nhttps://github.com/rstudio/rmarkdown/issues/1681\nR: 2 column layout in Rmarkdown with tabset\nScraping in R rvest\nTidy Web Scraping in R Tutorial and Resources"
  },
  {
    "objectID": "lists.html",
    "href": "lists.html",
    "title": "Lists",
    "section": "",
    "text": "Have you heard of awesome lists? Probably some of the best compilations out there on there on the internet.\nWith that said here are some random compilations of my own.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nCoding write-ups and resources\n\n\n1 min\n\n\n\n\nWorkplace articles\n\n\n1 min\n\n\n\n\nThe importance of good programming\n\n\n6 min\n\n\n\n\nSound\n\n\n4 min\n\n\n\n\nPhone games\n\n\n1 min\n\n\n\n\nA little data goes a long way\n\n\n1 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lists/sound.html",
    "href": "lists/sound.html",
    "title": "Sound",
    "section": "",
    "text": "Make beautiful music with technology!"
  },
  {
    "objectID": "lists/sound.html#otomata",
    "href": "lists/sound.html#otomata",
    "title": "Sound",
    "section": "Otomata",
    "text": "Otomata\nMake beautiful generative algorithm inspired music using cellular automation.\n\nLink to the app: https://marwahaha.github.io/otomata/\n\nSome historic links:\n\nOriginally at (but now no longer works): https://earslap.com/page/otomata.html\nFrom the reddit post: https://www.reddit.com/r/otomata/comments/lrd7n4/otomata_lives_again_web_version/\nGithub: https://github.com/marwahaha/otomata\n\n\n\n\nreddit post"
  },
  {
    "objectID": "lists/sound.html#sonic-pi",
    "href": "lists/sound.html#sonic-pi",
    "title": "Sound",
    "section": "Sonic Pi",
    "text": "Sonic Pi\nMake your own music using programming with Sonic Pi: https://sonic-pi.net/"
  },
  {
    "objectID": "lists/sound.html#sky-cotl",
    "href": "lists/sound.html#sky-cotl",
    "title": "Sound",
    "section": "Sky COTL",
    "text": "Sky COTL\nSky Children of Light is a phone game that has a built in music player that I really love. Instead of having to turn pages in sheet music it has floating icons that change shape over the notes you need to play, making it really easy to pick up once you get the hang of it.\n\nSky music library: https://sky-music.github.io/\nSky music app library: https://sky-music.herokuapp.com/songLibrary.html\nSky music app: https://sky-music.herokuapp.com/\n\nHere’s an example of one that I created (an attempt to translate “Regret” by Gackt):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView this post on Instagram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA post shared by @lisa.needs.to.make"
  },
  {
    "objectID": "lists/good-programming.html",
    "href": "lists/good-programming.html",
    "title": "The importance of good programming",
    "section": "",
    "text": "A handful of articles that have stood out over the years as interesting case studies into the importance of good programming, and what that actually means."
  },
  {
    "objectID": "lists/good-programming.html#uk-post-office-scandal",
    "href": "lists/good-programming.html#uk-post-office-scandal",
    "title": "The importance of good programming",
    "section": "UK Post Office Scandal",
    "text": "UK Post Office Scandal\n\nDevelopers blamed for the post office scandal?\nVictim testimony\nRemote access and mistakes blamed on post masters, a smoking gun\nExample of the terrible quality of the code - were they paid by line of code submitted? Does this imply a fauly language conversion? Risks of overflow errors?\nOne of the independent investigators from 2012 breaks silence\nEvidence that back in 2012 the indepent investigators were told about remote access, implying pejury in all later cases\nThey Could, And They Did Change Branch Transaction Data\nMiscarriage of justice - the Rose report\nList of current issues / bugs as of 2017? with the software. One can only imagine how much worse it used to be\nProject manager on the original project discusses his impressions - how not to commission a complex project Inquiry Phase 2: Star Witness – Dave gives it both barrels\nKnown errors in the software, perjury, and lack of disclosure\nInterview with IT - throwing coconuts\nInterview with the other IT\nAttempt by the post to recuse the judge - right after a verdict was handed down. Sour grapes, anyone?\nThe cover up\nWhy did the lawyer for the post office act this way?\nThinking of alleging or pleading fraud: better read this first"
  },
  {
    "objectID": "lists/code.html",
    "href": "lists/code.html",
    "title": "Coding write-ups and resources",
    "section": "",
    "text": "Source for good articles\nThe true cost of the cloud\nThe case for professional services\nOpinionated article on Python installations\nThis page addresses poetry and conda\nObject oriented programming is the biggest mistake of all time\nGoogle drive is production\nParadox of choice\nWhy we’re leaving the cloud\nMoving beyond “algorithmic bias is a data problem”\nThat keynote talk “USENIX Security ’18-Q: Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?” : 4:08 is where he talks about ml\nHere’s What Ethical AI Really Means - YouTube philosophy tube\nPodcast on Stackoverflow Architecture\nData Feminism\nInvisible Women\nQuality Jam 2017: Michael Bolton “A Ridiculously Rapid Introduction to Rapid Software Testing”\nHome not so sweet home gist\n\nIncludes links to pages like Everything that uses configuration files should report where they’re located\n\nHelp linux ate my RAM\nFree images for including in presentations\nDatadog sales people are annoying\nCounting explosions at Unity, a data analysts perspective\nDatabase rebuild incident\nHow do lava lamps help with Internet encryption?\nFree email for testing things: mailtrap\nIf you haven’t done a Pandas data analysis project in awhile, it’s probably not a bad idea to watch this guy’s vids\nPassword purgatory\nEverything I learned about concurrency and reliability I learned at the Waffle House\nPositive affirmations for site reliability engineers\nvaporwave RStudio theme"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome!\nI’m a former product engineer with experience in biotech, semiconductor, and MEMS/BioMEMS manufacturing that stumbled into computational shenanigans in my journey of trying to make the most out of the data available to my team. I was fortunate enough to set up a RStudio / Posit Connect server at a previous company and from then on was hooked on helping folks set up infrastructure for doing good data science.\nIn my spare time I enjoy playing board games at the local game shop, building things, hot chocolate, and talking about anything SciFi.\nResume"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello there! You must be pretty awesome if you are checking out my humble website. Welcome!"
  },
  {
    "objectID": "lists/data.html",
    "href": "lists/data.html",
    "title": "A little data goes a long way",
    "section": "",
    "text": "A little data goes a long way… here are some examples that I think illustrte this in a particularly fascinating way (at least to me)."
  },
  {
    "objectID": "lists/data.html#fighting-the-gender-pay-gap-one-twitter-bot-at-a-time",
    "href": "lists/data.html#fighting-the-gender-pay-gap-one-twitter-bot-at-a-time",
    "title": "A little data goes a long way",
    "section": "Fighting the gender pay gap one twitter bot at a time",
    "text": "Fighting the gender pay gap one twitter bot at a time\nCompanies tweeted for International Women’s Day. Then this account called out their pay gaps.\nCheck out the bot for yourself."
  },
  {
    "objectID": "lists/data.html#for-better-science",
    "href": "lists/data.html#for-better-science",
    "title": "A little data goes a long way",
    "section": "For Better Science",
    "text": "For Better Science"
  },
  {
    "objectID": "lists/data.html#for-better-science-1",
    "href": "lists/data.html#for-better-science-1",
    "title": "A little data goes a long way",
    "section": "",
    "text": "The rabbit hole of learning about how bad science is uncovered and recalled started with:\n\nSylvain Lesné is a failed scientist\nCassava fraud and Alzheimer’s capitalism"
  },
  {
    "objectID": "lists/data.html#researchers-uncover-the-use-of-coal-ash-for-playground-materials-in-small-town",
    "href": "lists/data.html#researchers-uncover-the-use-of-coal-ash-for-playground-materials-in-small-town",
    "title": "A little data goes a long way",
    "section": "Researchers uncover the use of coal ash for playground materials in small town",
    "text": "Researchers uncover the use of coal ash for playground materials in small town\nA recent story about coal ash that was used in a playground and the work the local newspaper and the University (Duke) is doing to figure out the path forward:\n\nhttps://www.knoxnews.com/story/news/local/tennessee/tvacoalash/2020/05/17/duke-testing-shows-kingston-coal-ash-uranium-triple-report-levels/5035210002/\nhttps://www.knoxnews.com/story/news/crime/2021/08/09/claxton-playground-contaminated-radioactive-dust-still-open/5470284001/\nhttps://www.knoxnews.com/story/news/2017/07/21/kingston-coal-ash-spill-workers-treated-expendables-lawsuit-sick-and-dying-contends/451537001/"
  },
  {
    "objectID": "lists/data.html#unprecedented-access-to-food-safety-and-consumer-recalls",
    "href": "lists/data.html#unprecedented-access-to-food-safety-and-consumer-recalls",
    "title": "A little data goes a long way",
    "section": "Unprecedented access to food safety and consumer recalls",
    "text": "Unprecedented access to food safety and consumer recalls\n\nFood safety\nConsumer products"
  },
  {
    "objectID": "lists/phone_games.html",
    "href": "lists/phone_games.html",
    "title": "Phone games",
    "section": "",
    "text": "Seedship\n\n\nA fun (and free game) where you are an AI exploring space trying to find the new home for the 1000 remaining survivors of the human race stored onboard.\nLink: https://philome.la/johnayliff/seedship/play/index.html\n\n\n\nhttps://garticphone.com/\n\n\n\nhttps://www.urbandead.com/\n\n\n\nhttps://robotodyssey.online/\n\n\n\nhttps://skribbl.io/\n\n\n\nhttps://www.trickstercards.com/home/euchre/\n\n\n\nhttps://codenames.game/\n\n\n\nTrack your coverage gaps with: https://www.mkttoolbox.com/login/?ret_url=%2F\n\n\n\nhttps://semantle.com/\n\n\n\nhttps://knucklebones.io/en"
  },
  {
    "objectID": "lists/phone_games.html#random-assortment-of-games-that-i-particularly-enjoy-and-would-endorse",
    "href": "lists/phone_games.html#random-assortment-of-games-that-i-particularly-enjoy-and-would-endorse",
    "title": "Phone games",
    "section": "",
    "text": "Seedship\n\n\nA fun (and free game) where you are an AI exploring space trying to find the new home for the 1000 remaining survivors of the human race stored onboard.\nLink: https://philome.la/johnayliff/seedship/play/index.html\n\n\n\nhttps://garticphone.com/\n\n\n\nhttps://www.urbandead.com/\n\n\n\nhttps://robotodyssey.online/\n\n\n\nhttps://skribbl.io/\n\n\n\nhttps://www.trickstercards.com/home/euchre/\n\n\n\nhttps://codenames.game/\n\n\n\nTrack your coverage gaps with: https://www.mkttoolbox.com/login/?ret_url=%2F\n\n\n\nhttps://semantle.com/\n\n\n\nhttps://knucklebones.io/en"
  },
  {
    "objectID": "lists/workplace.html",
    "href": "lists/workplace.html",
    "title": "Workplace articles",
    "section": "",
    "text": "Yolo manifesto\nComfy software\nOpen source as the “new normal”\nOvercoming matrix madness\nMatrix organizations\nOvercoming matrix madness\nCall center burnout\nProduct led vs sales led organizations\nLAYER\nBeing glue\nDiscussion of cognitive load and how big of a barrier it can be\nCognitive load\nHow organizations are like slime molds\nForbes - let the engineers lead (boeing)\nFailure to Learn: the BP refinery disaster"
  },
  {
    "objectID": "posts/ai_art.html",
    "href": "posts/ai_art.html",
    "title": "AI generated art",
    "section": "",
    "text": "AI is controversial, and I’d recommend watching these two videos to understand some technical points on why that is:\n\nKeynote talk “USENIX Security ’18-Q: Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?”\nHere’s What Ethical AI Really Means - YouTube philosophy tube"
  },
  {
    "objectID": "posts/ai_art.html#but-first-a-disclaimer",
    "href": "posts/ai_art.html#but-first-a-disclaimer",
    "title": "AI generated art",
    "section": "",
    "text": "AI is controversial, and I’d recommend watching these two videos to understand some technical points on why that is:\n\nKeynote talk “USENIX Security ’18-Q: Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?”\nHere’s What Ethical AI Really Means - YouTube philosophy tube"
  },
  {
    "objectID": "posts/ai_art.html#ai-generated-art",
    "href": "posts/ai_art.html#ai-generated-art",
    "title": "AI generated art",
    "section": "AI Generated Art",
    "text": "AI Generated Art\nAI isn’t a new concept. Using AI’s to generate art isn’t a new concept. Having that available as app that actually creates some pretty fun results is pretty new.\nAs someone whose artistic ambitions always are greater than my actual ability, this has been a surprisingly fun and rewarding technology to play around with.\nThe three that I recommend are:\n\nMy first AI art generating app (5 free pieces of art per day): https://www.starryai.com/\nDoesn’t require log in (5 free pieces of art per day, however using incognito mode or edge counts as a separate user for 15 pieces of art per day): https://creator.nightcafe.studio/\nNo limit to number of pieces, landscapes only: https://www.wombo.art/\n\nThere are a bunch more I’m looking forward to trying: &lt;https://www.unite.ai/10-best-ai-art-generators/#:~:text=Another one of the best,trained with millions of images&gt;"
  },
  {
    "objectID": "posts/ai_art.html#highlights",
    "href": "posts/ai_art.html#highlights",
    "title": "AI generated art",
    "section": "Highlights",
    "text": "Highlights\nImages in : img/starryai/\n\n\n\n\n\n\n\n\n\nsolarpunk\n\n\n\n\n\n\n\nsolar panel satellite\n\n\n\n\n\n\n\npluto\n\n\n\n\n\n\n\n\n\nspaceship flying down the sidewalk\n\n\n\n\n\n\n\nHydraulic dam fairytale\n\n\n\n\n\n\n\nthe storm rolled in\n\n\n\n\n\n\n\n\n\nI believe in a thing called love\n\n\n\n\n\n\n\nPlaceholder\n\n\n\n\n\n\n\nSpacecat\n\n\n\n\n\n\n\n\n\nparty around a bonfire\n\n\n\n\n\n\n\nfireflies\n\n\n\n\n\n\n\nthunderstorm over galveston\n\n\n\n\n\n\n\n\n\nwoodland creatures having a tea party\n\n\n\n\n\n\n\nPacific Rim\n\n\n\n\n\n\n\nMind the Gap\n\n\n\n\n\nImages in : img/nightcafe\n\n\n\n\n\n\n\n\n\nRobotic owl in space\n\n\n\n\n\n\n\nThe monster from alien on a date\n\n\n\n\n\n\n\nselfie\n\n\n\n\n\n\n\n\n\nbrain on fire\n\n\n\n\n\n\n\ncyborg bartender\n\n\n\n\n\n\n\nammonite seashell on the beach being worshipped by monsters\n\n\n\n\n\nTips on dealing with displaying images in a rendered quarto document: https://quarto.org/docs/authoring/figures.html#figure-panels\n\n\n\nsolarpunk\nsolar panel satellite\npluto\nspaceship flying down the sidewalk\nHydraulic dam fairytale\nthe storm rolled in\nI believe in a thing called love\nPlaceholder\nSpacecat\nparty around a bonfire\nfireflies\nthunderstorm over galveston\nwoodland creatures having a tea party\nPacific Rim\nMind the Gap\nRobotic owl in space\nThe monster from alien on a date\nselfie\nbrain on fire\ncyborg bartender\nammonite seashell on the beach being worshipped by monsters"
  },
  {
    "objectID": "posts/chatbot.html",
    "href": "posts/chatbot.html",
    "title": "Chatbot",
    "section": "",
    "text": "Check out the code: https://github.com/leesahanders/Chatbot\nCheck out the hosted chatbot: https://leesahanders.shinyapps.io/Chatbot/\nThe goal of this project was to build a basic chatbot and explore a couple ways of hosting it. You can check it out in action in a Shiny app hosted at leesahanders.shinyapps.io/Chatbot/"
  },
  {
    "objectID": "posts/chatbot.html#meet-the-bot",
    "href": "posts/chatbot.html#meet-the-bot",
    "title": "Chatbot",
    "section": "Meet the bot",
    "text": "Meet the bot\n\n\n\n\n\n\n\n\nChatbot\nName\nDescription\n\n\n\n\n\nLeafey\nLeafey is here to provide some plant therapy. Leafey is very simple with just key phrase look ups based on user inputs.\n\n\n\n\nIt’s just the one bot for now, but check back later and there might be more added."
  },
  {
    "objectID": "posts/chatbot.html#chat-model",
    "href": "posts/chatbot.html#chat-model",
    "title": "Chatbot",
    "section": "Chat model",
    "text": "Chat model\nShout out to the incredible resource at https://www.r-bloggers.com/2021/01/eliza-chatbot-in-r-build-yourself-a-shrink/ where the core code parts for the chatbot are from.\nAt it’s most basic the important elements are: 1. List of answers based on key phrases 2. List of default answers if no key phrases are found 3. Pattern matching function\nEssentially it works by using an input from the user and searching for key phrases and based on what it finds for a match returning the best matched answer. For more details check out the writeout the folks at r-bloggers did (linked above).\nMore chatbots can be created by copying the chatbot_leafey file and changing the name and contents to reflect the personality of the new one."
  },
  {
    "objectID": "posts/chatbot.html#hosting",
    "href": "posts/chatbot.html#hosting",
    "title": "Chatbot",
    "section": "Hosting",
    "text": "Hosting\nThe chat bot can now be kicked off inside whichever system wanted - whether that is in a shiny app, discord integration, or just in console using a while loop.\n\nConsole\nShout out to the incredible resource at https://www.r-bloggers.com/2021/01/eliza-chatbot-in-r-build-yourself-a-shrink/ where this code is from for kicking off your chatbot to interact with in the console until you hit exit:\ncat(\"Leafey: Hello, I am Leafey!\\n\")\nwhile (TRUE) {\n  input &lt;- readline(\"You: \")\n  if (input == \"quit\") break\n  cat(\"Leafey:\", Leafey(input))\n}\n\n\nShiny\nThe Shiny app is essentially just a wrapper for the chatbot functions. The trick was setting it up so that the user could select a chatbot and it would load the rest (so it would be reasonably scaleable), and capturing the chat with a log so that the user can see the full history of the conversation.\nIn order to use this feel free to clone the project and update the various sections as needed. Pay close attention to the section happening after the submit / chatbot selection action button is triggered. This is where the loading in of the appropriate chatbot is happening and various parameters are being set/reset.\n\nGithub link\nThe fancy ribbon is courtesy the gitlink.\nribbon_css(\"https://github.com/leesahanders/Chatbot\", text = \"Code on Github\", fade = FALSE),\nA less fancy way to include a link would be:\n      # Adding div tag to the sidebar with git link           \n      tags$div(class=\"header\", checked=NA,\n               #tags$p(\"Raw code located on Git\"),\n               tags$a(href=\"https://github.com/leesahanders/Chatbot\", \"Raw code located on Git, check it out by clicking here\")\n      ),\n\n\nUsing the enter key for triggering the action button\nHuge thanks to Rahul Mishra for finding an easy to use solution.\nIn the initialization of the app include the javascript function:\njscode &lt;- '\n$(function() {\n  var $els = $(\"[data-proxy-click]\");\n  $.each(\n    $els,\n    function(idx, el) {\n      var $el = $(el);\n      var $proxy = $(\"#\" + $el.data(\"proxyClick\"));\n      $el.keydown(function (e) {\n        if (e.keyCode == 13) {\n          $proxy.click();\n        }\n      });\n    }\n  );\n});\n'\nThen inside the UI include the call to the javascript where “Send” is changed to the label for your actionButton:\n  tags$head(tags$script(HTML(jscode))),\n  `data-proxy-click` = \"Send\","
  },
  {
    "objectID": "posts/chatbot.html#references",
    "href": "posts/chatbot.html#references",
    "title": "Chatbot",
    "section": "References",
    "text": "References\nCredit where credit is due - jokes are from: - https://www.rd.com/article/plant-puns/\nFor the Shiny app development I used a bunch of resources for getting the details:\n\nhttps://stackoverflow.com/questions/65365805/how-to-align-button-next-to-text-input\nhttps://stackoverflow.com/questions/56608214/how-can-i-keep-input-track-log-in-shiny-then-print-it-and-save-it\nhttps://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html (probably my favorite page on the internet!)\nhttps://clarewest.github.io/blog/post/making-tables-shiny/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/kable.html\nhttps://stackoverflow.com/questions/62139431/how-can-i-make-the-first-col-aligned-to-left-and-the-rest-aligned-to-center-with\n\nAnd some resources that are more aspirational, for future plans and features: - https://shiny.rstudio.com/articles/notifications.html - https://community.rstudio.com/t/shiny-contest-submission-table-editor-shiny-app/23600 - https://stackoverflow.com/questions/32335951/using-enter-key-with-action-button-in-r-shiny\nSomeday it would be cool to explore integrating a chatbot with discord, these resources look like they’d be useful: - https://github.com/jljsio/discordr - wrapper for Python package https://realpython.com/how-to-make-a-discord-bot-python/ - https://www.reddit.com/r/rprogramming/comments/epqfnl/making_a_discord_bot_in_r/"
  },
  {
    "objectID": "posts/maverick soft topper.html",
    "href": "posts/maverick soft topper.html",
    "title": "Ford Maverick Soft Topper installation",
    "section": "",
    "text": "Soft topper is a fabric rear cover for truck beds.\nFinally taking the time to actually install it on my Ford Maverick. Went in like a breeze, though there is some loss in visibility once it is on. Here are some of the measurements.\n\n\n\nCarefully placing the clamps\n\n\n\n\n\nThe aluminum frame assembled and pushed back\n\n\n\n\n\nWith the fabric cover snapped in place the rear storage is safe from the elements. Next up to work on the camper modifications!"
  },
  {
    "objectID": "posts/resin.html",
    "href": "posts/resin.html",
    "title": "Resin Dice",
    "section": "",
    "text": "Flourescent dice set (all glitter dice)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView this post on Instagram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA post shared by @lisa.needs.to.make\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoyal purple dice set with beautiful crystals due to cold temperatures\n\n\n\n\n\n\n\n\nFire red dice set, with some glitter dice!"
  },
  {
    "objectID": "posts/resin.html#recent-dice-highlights",
    "href": "posts/resin.html#recent-dice-highlights",
    "title": "Resin Dice",
    "section": "",
    "text": "Flourescent dice set (all glitter dice)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView this post on Instagram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA post shared by @lisa.needs.to.make\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoyal purple dice set with beautiful crystals due to cold temperatures\n\n\n\n\n\n\n\n\nFire red dice set, with some glitter dice!"
  },
  {
    "objectID": "posts/resin.html#experiments",
    "href": "posts/resin.html#experiments",
    "title": "Resin Dice",
    "section": "Experiments",
    "text": "Experiments\n\nSilicone Mold\n\n\n\n\n\n\n\n\n\n\nThe basics I’ve gathered for learning how to make molds are:\n\nSmooth-On OOMOO\nNitrile Gloves\nSmall hand torch (originally meant for creme brulee)\nA Safety Pin\nPlasticine Clay\nVarious small disposable cups, disposable plastic fork, measuring cup\nHandful of my least favorite dice\n\n\n\n\n\n\n\n\nGoing to the crafts store the whole time I was thinking “this is going to go GREAT. How hard could it be? The videos are so calming, surely the whole process is straight up zen.”\nWRONG.\nAfter learning how difficult it is to attach a pin into a D6 (I would describe it more as teetering perilously rather than actually mounted) we got to move onto lessons into practically what pot time is. While on thebox it may say pot time of 30 minutes in practice I had less than 20 minutes to get the silicone into the mold before it was hardening to the point of not flowing smoothly.\n\n\n\n\n\n\n\nAlso had complete failure of the attempt at a vacuum chamber using my hand vaccuum, a small bell jar, and the valve on one of those clothes bags that let you vacuum the air out for better storage. No bubbles were successfully pulled out using this method, whether due to lack of a seal between the bell jar and the valve or if just wasn’t a strong enough vacuum.\n\n\n\n\n\n\n\nThe three mold attempts are sitting out to cure overnight before I’ll begin the process of removing the die and trying out adding resin. I’m doubtful any of these turned out but what a wonderful way to spend a couple hours!\n\n\n\n\n\n\n\n10/4/2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry 2 of creating the silicone mold. I was able to use the lessons from last time and create a mold with the sprue large enough to get a good amount in there while still having it be small enough that hopefully polishing won’t be horrible.\n\n\n\n\n\n\n\nInteresting enough I think that the mixture is very sensitive to the quantities of each part. While I was very careful to match the volumes using my trusty 1/2 cup measuring spoon we can notice a distinct change in color. Despite this change in color it did successfully set and it doesn’t feel different and will do nicely as a mold.\n\n\n\n\n\n\n\nI’m a little concerned with my splits in the mold for removing the pieces. Hopefully it will seal up well enough that we won’t have the resin leaking out or really large obvious lines.\n\n\n\n\n\n\n\n10/5-6/2020\n\n\n\n\n\n\n\nPouring the Resin\n\n\n\n\n\n\n\n\n\n\nThe ingredients for pouring the resin:\n\nDried flowers\nFood coloring\nMica powders\nEasyCast resin\nDixie cups, pipettes, nitrile glove, facemask (not pictured)\n\n\n\n\n\n\n\n\nThis batch I decided to focus on blues.\nRecipe:\n\n2 drops blue food coloring\nFor 2 of the dice added the green mica powder\n\nSince I layered it in with pure blue on bottom and the blue + mica powder on top it really developed a beautiful layering effect.\n\n\n\n\n\n\n\n10/6/2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecipe:\n\n1 drop ‘kiwi’ food coloring\nHalf of a handle full of green metallic mica powder\nBlue flower\n\nThoroughly mix equal parts of resin for 2 minutes, transfer to second container and add coloring. Mix for an additional minute and use pipettes (two pipette method where one can fill while you use the other one) to fill each mold. Gently insert a flower once 3/4 full. Swish the containers by rotating to encourage air bubbles to exit from difficult places in the mold.\n\n\n\n\n\n\n\nWow this just turned out BEAUTIFUL.\n\n\n\n\n\n\n\n10/7/2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSame recipe as before, still looks beautiful and we now have a matching set of 6!\n\n\n\n\n\n\n\n10/8/2020\n\n\n\n\n\n\n\nPolishing\n\n\n\n\n\n\n\n\n\n\nI’ve gathered my polishing components: namely various grits of sandpaper and some car automotive buff.\n\n\n\n\n\n\n\nFirst step is trimming off the sprue. This was done using a pair of shears (I used wirecutters). The ends come off pretty explosively (think nail trimming but times a million) so definitely be careful which direction you are pointing the die and the sprue.\nAll three have some kind of bubble where resin wasn’t able to really fill the mold. Maybe doing a “swish” after filling will help next time for preventing this?\n\n\n\n\n\n\n\nHere we have the before polishing with all the imperfections and remainder of the sprue.\n\n\n\n\n\n\n\nAnd here is our after with all the edges buffed out. Looking better, right? I think we can do better though because I did notice that the sides that came out looking good and shiny after being buffedstarted looking cloudy.\nIdeas for next time are to spend more time polishing on the finest grit. Maybe that will help get it super shiny? And spend longer polishing. It’s about time for a netflix marathon anyway.\n\n\n\n\n\n\n\n10/8/2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVery happy to report success with the “swish”. Only 1 out of the three dice has a bubble in the corner disrupting the pattern (and this one is the hardest mold, with the highest point being higher than where the sprue intersects).\n\n\n\n\n\n\n\n10/9/2020\n\n\n\n\n\n\n\nGlitter Dice\n\n\n\n\n\n\n\n\n\n\n\nI’ve been playing with making glitterdice lately. Glitterdice are where there is a sphere of liquid and glitter with a cap (UV curable resin) in the center of the dice (think snow globe). It’s hard to capture on camera but creates some really striking and fun to roll dice.\n\n\n\n\n\n\n\n4/3/2021\n\n\n\n\n\n\n\nRolling the Dice\nNow that our unique dice have been cured, painted, and polished it was time to put them to the test. Sure they look pretty (minus the occasional bubble and polish mark) but can they ROLL?\nHere, I am proud to present to you, are our candidates for the roll off!\n\nEach die was rolled 50 times to give some statistical significance to the results. Ideally we would have a hundred rolls or more per die (1,000 would be a really great number) but I decided to balance statistics with the chances of developing arthritis.\nIn addition to rolling each of our manufactured dice 50 times a professional metal die was rolled 50 times to give a reference for our measurements.\nThe conclusion:\nWell. Looks like my metal dice is weighted (both literally but also rolls 6 much more often). The resin dice performed better than I expected. I was expected to see the dice with remnants of a bubble or bumps to really have a very significant impact, but honestly they look comparable to the metal die.\nOur results:\n\n\nShow the code\nlibrary(\"readxl\")\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)\n#library(rjags)\n\nrolls = read_excel(\"./img/resin/Dice_rolls.xlsx\")\n\nrolls_long &lt;- rolls %&gt;%\n pivot_longer(cols = 1:10, names_to = \"Dice\", values_to = \"RollValue\")\n\nrolls_summary &lt;- rolls_long %&gt;%\n  group_by(Dice, RollValue) %&gt;%\n  summarize(Count = n())\n\np &lt;- ggplot(rolls_summary, aes(x=RollValue, y=Count, fill = Dice )) + \n  geom_bar(stat = \"identity\", position = position_stack(reverse = TRUE)) +\n  facet_wrap(~ Dice) +\n theme(legend.position = \"top\")\n\np\n\n\n\n\n\n\n\n\n\nShow the code\np &lt;- ggplot(rolls_summary) + \n  geom_bar(aes(x=Dice, y=Count, fill = RollValue ), stat = \"identity\", position = position_stack(reverse = TRUE)) +\n  coord_flip() + \n  theme(legend.position = \"top\")\n\np\n\n\n\n\n\n\n\n\n\n\n10/10/2020"
  },
  {
    "objectID": "posts/resin.html#resources",
    "href": "posts/resin.html#resources",
    "title": "Resin Dice",
    "section": "Resources",
    "text": "Resources\nHere are some of the resources that helped me on my journey down this exciting path (in no particular order):\n\nhttps://www.evewynn.com/moldmaking\nhttp://www.hanleybrady.com/making-dice-part-1-molds/\nhttps://www.instructables.com/Custom-Dice/\nhttps://www.thingiverse.com/thing:3795542\nhttps://www.youtube.com/watch?v=iRDte2j54F0\nhttps://www.reddit.com/r/dice/comments/8zemqn/looking_to_resin_cast_my_own_dice/\nhttps://www.youtube.com/watch?v=FQ1A7ZjTsx8\nhttps://rpubs.com/benjamin_chittick/59278\nhttps://rstudio-education.github.io/hopr/project-1-weighted-dice.html\nhttp://ditraglia.com/Econ103Public/Rtutorials/Rtutorial4.html\nhttps://dk81.github.io/dkmathstats_site/rvisual-bargraphs.html\nhttps://cran.r-project.org/web/packages/tidydice/vignettes/tidydice.html\nhttps://quarto.org/docs/authoring/figures.html#figure-panels"
  },
  {
    "objectID": "posts/vortico rockets.html",
    "href": "posts/vortico rockets.html",
    "title": "Vortico rockets",
    "section": "",
    "text": "Vorticos are a spinning rocket that helicopter down to a safe landing. By painting different patterns on the wings you can get some interesting optical effects!\nVery fun project spread out over a couple days to assemble, paint, and then finally shoot!\n\n\n\nPainted and assembled vortico rockets\n\n\n\n\n\nLaunching the vorticos is best with a smaller rod to prevent wobble on takeoff\n\n\n\n\n\nUp it goes!\n\n\nGet one for yourself here: https://www.rocketarium.com/Rockets/Vortico"
  },
  {
    "objectID": "posts-no-pictures/git-and-sagemaker.html",
    "href": "posts-no-pictures/git-and-sagemaker.html",
    "title": "Problems with persistence when in the cloud",
    "section": "",
    "text": "This is a random trick that took me longer than I care to admit to figure out - and wanted to squirrel it away so it’s easy to find in the future!"
  },
  {
    "objectID": "posts-no-pictures/git-and-sagemaker.html#problem-when-on-linux",
    "href": "posts-no-pictures/git-and-sagemaker.html#problem-when-on-linux",
    "title": "Problems with persistence when in the cloud",
    "section": "Problem when on Linux:",
    "text": "Problem when on Linux:\n\nIn general gitcreds doesn’t work well on linux (which has led to this git issue (Ship our own credential helper on Linux · Issue #47 · r-lib/gitcreds ). There is an excellent blog post that is very useful that goes deeper into what is going on: Notes from a data witch - Managing GitHub credentials from R, difficulty level linux"
  },
  {
    "objectID": "posts-no-pictures/git-and-sagemaker.html#problem-when-on-sagemaker",
    "href": "posts-no-pictures/git-and-sagemaker.html#problem-when-on-sagemaker",
    "title": "Problems with persistence when in the cloud",
    "section": "Problem when on Sagemaker:",
    "text": "Problem when on Sagemaker:\n\nAdditionally, on Sagemaker things like credentials will be by default stored to the ephemeral EC2 instance and lost when the session is closed. A different method needs to be pursued in order for the token to persist."
  },
  {
    "objectID": "posts-no-pictures/git-and-sagemaker.html#tldr-solution",
    "href": "posts-no-pictures/git-and-sagemaker.html#tldr-solution",
    "title": "Problems with persistence when in the cloud",
    "section": "TLDR Solution:",
    "text": "TLDR Solution:\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'"
  },
  {
    "objectID": "posts-no-pictures/git-and-sagemaker.html#testing",
    "href": "posts-no-pictures/git-and-sagemaker.html#testing",
    "title": "Problems with persistence when in the cloud",
    "section": "Testing",
    "text": "Testing\nI’ll add a disclaimer that it is similar to the .Renviron approach where the credentials would be stored as plain text, however to a location chosen by the user.\nLoad libraries:\nlibrary(usethis) \nlibrary(gitcreds) \nlibrary(gh) \nlibrary(credentials)\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'\nFrom the documentation:\n\nThe “store” mode saves the credentials to a plain-text file on disk, and they never expire. This means that until you change your password for the Git host, you won’t ever have to type in your credentials again. The downside of this approach is that your passwords are stored in cleartext in a plain file in your home directory. The other options involve needing to change the root container to include alternative git credential helpers (libsecret or gcm core) which as far as I can tell are not currently available and would be something I’d recommend reaching out to Amazon about as they control that image.\n\nGenerate the PAT:\nusethis::create_github_token()\nCopy the generated PAT to your clipboard. Provide the PAT to this function when asked for it:\ngitcreds::gitcreds_set()\nCheck that it stored with:\ngitcreds_get()"
  },
  {
    "objectID": "posts-no-pictures/git-and-sagemaker.html#alternatives",
    "href": "posts-no-pictures/git-and-sagemaker.html#alternatives",
    "title": "Problems with persistence when in the cloud",
    "section": "Alternatives",
    "text": "Alternatives\nThe old way “store a PAT as the GITHUB_PAT environment variable in .Renviron.” is typically what is recommended as being more compatible with linux if you are able to switch back to it, but it can present a security issue. We’ve also commonly seen folks using the gh package for generating PATs like in Managing Personal Access Tokens\nAlternatively, there are some git config options from the terminal. See: Chapter 9 Personal access token for HTTPS | Happy Git and GitHub for the useR"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog posts and random musings",
    "section": "",
    "text": "Here’s some random thoughts, all brought to you by Quarto’s blogging capability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHey Google - make me coffee!\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nLisa\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nFord Maverick Truck Cubby Holder\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nLisa\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nVortico rockets\n\n\n\n\n\n\n\n\nAug 6, 2023\n\n\nLisa\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFord Maverick Soft Topper installation\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\nLisa\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nChatbot\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nLisa\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nObservable JS for snappy datascience\n\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nLisa\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAI generated art\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\nLisa\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Quarto?\n\n\n\n\n\n\n\n\nJul 28, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nResin Dice\n\n\n\n\n\n\n\n\nApr 4, 2021\n\n\nLisa\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nBATTLEBOTS 2021\n\n\n\n\n\n\n\n\nJan 4, 2021\n\n\nLisa\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations_posit.html",
    "href": "presentations_posit.html",
    "title": "Lisa Anders - questionable.quarto",
    "section": "",
    "text": "Check out some of the presentations I’ve made while working at Posit:\n\n2024\n\n\n\n\n\n&gt;\n\n\nIt’s dangerous to go alone - take this!\n\n\nLisa’s best practices for pain-free data science\n\n\n Check out the slides Check out the repo\n\n\n\n\n\n2023 and 2022\n\n\n\n\n\n&gt;\n\n\nReporting in R with Posit Team\n\n\nUsing the Posit Team set of products to create data science reports.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nImproving app performance\n\n\nAnalyzing your R scripts for performance improvements using profvis, shinyloadtest, and shinytest2.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nReproduceable Workflows\n\n\nCreating reproduceable data science workflows.\n\n\nCheck out the slides"
  },
  {
    "objectID": "work/google-resources.html",
    "href": "work/google-resources.html",
    "title": "Access to resources in Google, an exploration",
    "section": "",
    "text": "Access to resources in google (bigquery, drive, etc) will depend on where the user is connecting from:\n\nLocal desktop: any method is fine\nWorkbench / server web app based: “OOB” workflows or non-interactive\nConnect / server web app non-interactive: Non-interactive only"
  },
  {
    "objectID": "work/google-resources.html#service-account-token",
    "href": "work/google-resources.html#service-account-token",
    "title": "Access to resources in Google, an exploration",
    "section": "Service account token",
    "text": "Service account token\nFollow these steps:\n\nCreate a service account\nFrom the GCP Console, in the target GCP Project, go to IAM & Admin &gt; Service accounts\nAssign permissions: googledrive docs does not have any explicit roles, service account used to test bigrquery has roles BigQuery Admin and Storage Admin\nAfter creating the service account, click on the three dots and “actions”, manage keys, add key as json, download credentials as json file\nThis key is a secret! Consider how it should be protected\nProvide path of json file to authentication\nGrant the service account permissions to resources as needed (it doesn’t inherit permissions) (For example, I had to visit https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project=redacted to enable access for google drive and gogle sheets, which it gave me the link to in an error message for my specific project)\n\nReference: https://gargle.r-lib.org/articles/non-interactive-auth.html#provide-a-service-account-token-directly and https://gargle.r-lib.org/articles/get-api-credentials.html#service-account-token\n# use a service account token, like drive_auth(path = \"/path/to/your/service-account-token.json\")\n# drive_auth(path = Sys.getenv(\"google_drive_service_account\"), scopes = \"drive.readonly\")\n# drive_auth(path = Sys.getenv(\"google_drive_service_account\"), scopes = \"drive.readonly\")\ncredentials_service_account(\n  #scopes = \"https://www.googleapis.com/auth/userinfo.email\",\n  path = Sys.getenv(\"google_drive_service_account\")\n)\n# now use googledrive\ngoogledrive::drive_find(n_max = 5)"
  },
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Technical writeups and musings",
    "section": "",
    "text": "Some random technical writeups for things I’ve found useful, all brought to you by Quarto’s blogging capability.\n\n\n\n\n\n\n\n\n\nAccess to resources in Google, an exploration\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nLisa\n\n\n6 min\n\n\n\n\n\n\n\nDebugging R Package Environments (renv): A long winded writeup\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nLisa\n\n\n48 min\n\n\n\n\n\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nLisa\n\n\n4 min\n\n\n\n\n\n\n\nProblems with persistence when in the cloud\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nLisa\n\n\n3 min\n\n\n\n\n\n\n\nSecuring credentials\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nLisa\n\n\n5 min\n\n\n\n\n\n\n\nConnecting to resources in Microsoft 365 / Sharepoint\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nLisa\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "work/sharepoint-oh-no.html",
    "href": "work/sharepoint-oh-no.html",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": ":warning: This is now outdated. Please refer to this article and the Microsoft365R package documentation developed by Hong Ooi instead for up-to-date information.\n\n\n\nMicrosoft 365 is a subscription extension of the Microsoft Office product line with cloud hosting support. Microsoft 365 uses Azure Active Directory (Azure AD) for user authentication and application access through developed APIs. The Microsoft supported method for interfacing with R developed content is with the Microsoft365R package which was developed by Hong Ooi and has extensive documentation. It supports access to Teams, SharePoint Online, Outlook, and OneDrive.\n\n\n\n\n:warning: Discussion between the developers and the Global Azure Administration team about the content and security requirements within your organization should determine which of the approaches should be supported.\n\nThere are four main authentication approaches supported by Microsoft365R. Note that multiple approaches can be supported at the same time.\n\n\n\n\n\n\n\n\n\nMethod\nauth_type\nPermissions\nCapability\n\n\n\n\nUser sign-in flow: Default\ndefault\nUser\nInteractive only (local IDE and Workbench, interactive Shiny content)\n\n\nUser sign-in flow: Device Code\ndevice_code\nUser\nInteractive only (local IDE and Workbench)\n\n\nService principal / Client secret\nclient_credentials\nApplication\nInteractive and non-interactive (same as above plus scheduled content)\n\n\nEmbedded credentials\nresource_owner\nUser\nInteractive and non-interactive (same as above plus scheduled content)\n\n\n\nAuthentication for Microsoft365R is through Microsoft’s Azure cloud platform through a registered application with appropriate assigned permissions in order to obtain ‘OAuth 2.0’ tokens.\n\n\nDepending on your organization’s security policy some steps may require support from your Azure Global Administrator.\nUser Sign-in Flow: Default\nA custom app can be created or the default app registration “d44a05d5-c6a5-4bbb-82d2-443123722380” that comes with the Microsoft365R package can be used. The user permissions will need to be enabled as detailed in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator. Additionally Redirect URLs will need to be added through Azure under App Registrations -&gt; select your app -&gt; Authentication -&gt; Platform configurations -&gt; Mobile and desktop applications -&gt;Add URI as well as also enabling nativeclient.\nFor adding Redirect URLs, which will give a typical web-app authentication experience for interactive applications:\n\nFor the desktop RStudio IDE the URL is: http://localhost:1410/.\nFor content hosted in shinyapps.io this would be of the form https://youraccount.shinyapps.io/appname (including the port number if specified).\nA SSL certificate will be required for non-local connections. This means that the Connect and Workbench URLs will need to be HTTPS. A wildcard could be used instead of adding the Redirect URL for each piece of content/user where appropriate for server-wide access.\n\nUser Sign-in Flow: Device Code\nIn addition to user level app permissions outlined above the device code workflow option will need to be enabled.\nEnabling the device code workflow is through the App Registration dashboard in Azure -&gt; click on the created app -&gt; Authentication -&gt; Allow public client flows and setting Enable the following mobile and desktop flows to yes. The device code workflow does not need Redirect URLs, instead providing a code and a link for the developer to access in a separate browser window (or even on a separate device) for sign-in.\nService Principal / Client Secret\nA custom app will need to be registered in Azure with Application permissions. The permissions can be based off of the user permissions but can be assigned as needed for the application and to comply with any security restrictions.\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. As a control using environmental variable’s for storing the client secret is recommended. Starting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\nEmbedded Credentials\nA custom app will need to be registered in Azure with User permissions as specified in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator.\nThe credentials being embedded can be a user or a service account, as long as access to the desired content inside Microsoft 365 has been granted. Creating service accounts per content is recommended to enable faster troubleshooting and easier collaboration. As a control the Username / Password should never be exposed directly in the code, instead using Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\n\n\n\n\n\n\nThe user sign-in flow option provides the typical web browser authentication experience. A user will need to be available to interact with the authentication pop-up in order to which makes this an option for interactive applications (such as the local RStudio IDE, Workbench, or an interactive Shiny app), but not applicable for scheduled content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app = app)\n\n\n\nIn some interactive cases it may be easier to use the device code flow where the user is prompted with a code and a link which is opened in a separate screen for logging in. For example for using a Workbench instance that was deployed without an SSL certificate. This does require interaction from the user and as such will not be applicable for scheduled content nor hosted content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette, with the Service Principal via using a Client Secret discussed in this section being the Microsoft recommended approach.\n\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. Instead the recommended approach is to store it as an Environment Variable which can be done through the Connect UI.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette. In cases where the additional access that comes with Application level permissions isn’t appropriate for the organization’s security requirements the embedded credentials approach can be used.\n\nThe credentials embedded will need to be granted access to the desired content and can either be a user or a service account. Working with your Azure Global Administrator to create service accounts per content is recommended to enable fast troubleshooting and easier collaboration.\nSensitive variables such username / password should be embedded as Environment Variables so that they aren’t exposed in the code directly.which can be done through the Connect UI. See the example here.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nuser &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_USER\")\npwd &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_PASSWORD\")\n\n# Create auth token cache directory, otherwise it will prompt the user on the console for input\ncreate_AzureR_dir()\n\n# create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, \n                    username = user, \n                    password = pwd,\n                    auth_type=\"resource_owner\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nIn the case of authentication failures clearing cached authentication tokens/files can be done with:\nlibrary(AzureAuth)\nlibrary(AzureGraph)\n\ntenant = MyTenant\n\nAzureAuth::clean_token_directory()\nAzureGraph::delete_graph_login(tenant=\"mytenant\")\n\n\n\n\n\n\nThe authentication method used in this example could be swapped out for any of the examples shown above. The documentation on Microsoft365R contains extensive examples beyond what is included below.\nlibrary(Microsoft365R)\nlibrary(AzureGraph)\nlibrary(AzureAuth)\n\nsite_url = MySharepointSiteURL\ntenant = MyTenant\napp = MyApp\ndrive_name = MyDrive # For example by default this will likely be \"Documents\"\nfile_src = MyFileName.TheExtension\n\n# Add sensitive variables as environment variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory, otherwise it will prompt the the console for input\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a SharePoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n# An example using the SharePoint site to get to a specific drive\ndrv &lt;- site$get_drive(drive_name)\n\n# Download a specific file\ndrv$download_file(src = file_src, dest = \"tmp.csv\", overwrite = TRUE)\n\n# Retrieve lists of the different types of items in our sharepoint site. Documents uploaded under the 'Documents' drive are retrieved with list_files(). \ndrv$list_items()\ndrv$list_files() \ndrv$list_shared_files()\ndrv$list_shared_items()\n\n# Files can also be uploaded back to SharePoint\ndrv$upload_file(src = file_dest, dest = file_dest)\n\n\n\nMicrosoft resources can be used for hosting data in pins format using board_ms365() from pins. The authentication method used in this example could be swapped out for any of the examples shown above.\nlibrary(Microsoft365R)\nlibrary(pins)\n\nsite_url = MySite\napp=MyApp\n\n# Create a Microsoft Graph login\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n# An example getting the default drive \ndoclib &lt;- site$get_drive()\n\n# Connect ms365 as a pinned board. If this folder doesn't already exist it will be created on execution. \nboard &lt;- board_ms365(drive = doclib, \"general/project1/board\")\n\n# Write a dataset as a pin to Sharepoint\nboard %&gt;% pin_write(iris, \"iris\", description = \"This is a test\")\n\n# View the metadata of the pin we just created \nboard %&gt;% pin_meta(\"iris\")\n\n# Read the pin\ntest &lt;- board %&gt;% pin_read(\"iris\")\n\n\n\n\nThere are a few cases not covered in this article where the below resources may be useful:\n\nFor user level authentication into servers refer to the Marketplace offering and the Connect documentation.\nFor Python users the Microsoft REST API is the Microsoft developed method with examples.\nAs a last resort, mapping SharePoint, OneNote, or other systems as a network drive to the hosting server could be considered, using a program such as expandrive.\n\n\n\n\nOn the off chance that anyone makes it to the end this article got a chuckle out of me and may be relatable: https://www.theregister.com/2022/07/15/on_call/"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#introduction",
    "href": "work/sharepoint-oh-no.html#introduction",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "Microsoft 365 is a subscription extension of the Microsoft Office product line with cloud hosting support. Microsoft 365 uses Azure Active Directory (Azure AD) for user authentication and application access through developed APIs. The Microsoft supported method for interfacing with R developed content is with the Microsoft365R package which was developed by Hong Ooi and has extensive documentation. It supports access to Teams, SharePoint Online, Outlook, and OneDrive."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#summary",
    "href": "work/sharepoint-oh-no.html#summary",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": ":warning: Discussion between the developers and the Global Azure Administration team about the content and security requirements within your organization should determine which of the approaches should be supported.\n\nThere are four main authentication approaches supported by Microsoft365R. Note that multiple approaches can be supported at the same time.\n\n\n\n\n\n\n\n\n\nMethod\nauth_type\nPermissions\nCapability\n\n\n\n\nUser sign-in flow: Default\ndefault\nUser\nInteractive only (local IDE and Workbench, interactive Shiny content)\n\n\nUser sign-in flow: Device Code\ndevice_code\nUser\nInteractive only (local IDE and Workbench)\n\n\nService principal / Client secret\nclient_credentials\nApplication\nInteractive and non-interactive (same as above plus scheduled content)\n\n\nEmbedded credentials\nresource_owner\nUser\nInteractive and non-interactive (same as above plus scheduled content)\n\n\n\nAuthentication for Microsoft365R is through Microsoft’s Azure cloud platform through a registered application with appropriate assigned permissions in order to obtain ‘OAuth 2.0’ tokens.\n\n\nDepending on your organization’s security policy some steps may require support from your Azure Global Administrator.\nUser Sign-in Flow: Default\nA custom app can be created or the default app registration “d44a05d5-c6a5-4bbb-82d2-443123722380” that comes with the Microsoft365R package can be used. The user permissions will need to be enabled as detailed in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator. Additionally Redirect URLs will need to be added through Azure under App Registrations -&gt; select your app -&gt; Authentication -&gt; Platform configurations -&gt; Mobile and desktop applications -&gt;Add URI as well as also enabling nativeclient.\nFor adding Redirect URLs, which will give a typical web-app authentication experience for interactive applications:\n\nFor the desktop RStudio IDE the URL is: http://localhost:1410/.\nFor content hosted in shinyapps.io this would be of the form https://youraccount.shinyapps.io/appname (including the port number if specified).\nA SSL certificate will be required for non-local connections. This means that the Connect and Workbench URLs will need to be HTTPS. A wildcard could be used instead of adding the Redirect URL for each piece of content/user where appropriate for server-wide access.\n\nUser Sign-in Flow: Device Code\nIn addition to user level app permissions outlined above the device code workflow option will need to be enabled.\nEnabling the device code workflow is through the App Registration dashboard in Azure -&gt; click on the created app -&gt; Authentication -&gt; Allow public client flows and setting Enable the following mobile and desktop flows to yes. The device code workflow does not need Redirect URLs, instead providing a code and a link for the developer to access in a separate browser window (or even on a separate device) for sign-in.\nService Principal / Client Secret\nA custom app will need to be registered in Azure with Application permissions. The permissions can be based off of the user permissions but can be assigned as needed for the application and to comply with any security restrictions.\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. As a control using environmental variable’s for storing the client secret is recommended. Starting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\nEmbedded Credentials\nA custom app will need to be registered in Azure with User permissions as specified in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator.\nThe credentials being embedded can be a user or a service account, as long as access to the desired content inside Microsoft 365 has been granted. Creating service accounts per content is recommended to enable faster troubleshooting and easier collaboration. As a control the Username / Password should never be exposed directly in the code, instead using Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#authentication-examples",
    "href": "work/sharepoint-oh-no.html#authentication-examples",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "The user sign-in flow option provides the typical web browser authentication experience. A user will need to be available to interact with the authentication pop-up in order to which makes this an option for interactive applications (such as the local RStudio IDE, Workbench, or an interactive Shiny app), but not applicable for scheduled content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app = app)\n\n\n\nIn some interactive cases it may be easier to use the device code flow where the user is prompted with a code and a link which is opened in a separate screen for logging in. For example for using a Workbench instance that was deployed without an SSL certificate. This does require interaction from the user and as such will not be applicable for scheduled content nor hosted content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette, with the Service Principal via using a Client Secret discussed in this section being the Microsoft recommended approach.\n\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. Instead the recommended approach is to store it as an Environment Variable which can be done through the Connect UI.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette. In cases where the additional access that comes with Application level permissions isn’t appropriate for the organization’s security requirements the embedded credentials approach can be used.\n\nThe credentials embedded will need to be granted access to the desired content and can either be a user or a service account. Working with your Azure Global Administrator to create service accounts per content is recommended to enable fast troubleshooting and easier collaboration.\nSensitive variables such username / password should be embedded as Environment Variables so that they aren’t exposed in the code directly.which can be done through the Connect UI. See the example here.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nuser &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_USER\")\npwd &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_PASSWORD\")\n\n# Create auth token cache directory, otherwise it will prompt the user on the console for input\ncreate_AzureR_dir()\n\n# create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, \n                    username = user, \n                    password = pwd,\n                    auth_type=\"resource_owner\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nIn the case of authentication failures clearing cached authentication tokens/files can be done with:\nlibrary(AzureAuth)\nlibrary(AzureGraph)\n\ntenant = MyTenant\n\nAzureAuth::clean_token_directory()\nAzureGraph::delete_graph_login(tenant=\"mytenant\")"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#sharepoint-examples",
    "href": "work/sharepoint-oh-no.html#sharepoint-examples",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "The authentication method used in this example could be swapped out for any of the examples shown above. The documentation on Microsoft365R contains extensive examples beyond what is included below.\nlibrary(Microsoft365R)\nlibrary(AzureGraph)\nlibrary(AzureAuth)\n\nsite_url = MySharepointSiteURL\ntenant = MyTenant\napp = MyApp\ndrive_name = MyDrive # For example by default this will likely be \"Documents\"\nfile_src = MyFileName.TheExtension\n\n# Add sensitive variables as environment variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory, otherwise it will prompt the the console for input\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a SharePoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n# An example using the SharePoint site to get to a specific drive\ndrv &lt;- site$get_drive(drive_name)\n\n# Download a specific file\ndrv$download_file(src = file_src, dest = \"tmp.csv\", overwrite = TRUE)\n\n# Retrieve lists of the different types of items in our sharepoint site. Documents uploaded under the 'Documents' drive are retrieved with list_files(). \ndrv$list_items()\ndrv$list_files() \ndrv$list_shared_files()\ndrv$list_shared_items()\n\n# Files can also be uploaded back to SharePoint\ndrv$upload_file(src = file_dest, dest = file_dest)\n\n\n\nMicrosoft resources can be used for hosting data in pins format using board_ms365() from pins. The authentication method used in this example could be swapped out for any of the examples shown above.\nlibrary(Microsoft365R)\nlibrary(pins)\n\nsite_url = MySite\napp=MyApp\n\n# Create a Microsoft Graph login\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n# An example getting the default drive \ndoclib &lt;- site$get_drive()\n\n# Connect ms365 as a pinned board. If this folder doesn't already exist it will be created on execution. \nboard &lt;- board_ms365(drive = doclib, \"general/project1/board\")\n\n# Write a dataset as a pin to Sharepoint\nboard %&gt;% pin_write(iris, \"iris\", description = \"This is a test\")\n\n# View the metadata of the pin we just created \nboard %&gt;% pin_meta(\"iris\")\n\n# Read the pin\ntest &lt;- board %&gt;% pin_read(\"iris\")"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#other-microsoft-related-resources",
    "href": "work/sharepoint-oh-no.html#other-microsoft-related-resources",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "There are a few cases not covered in this article where the below resources may be useful:\n\nFor user level authentication into servers refer to the Marketplace offering and the Connect documentation.\nFor Python users the Microsoft REST API is the Microsoft developed method with examples.\nAs a last resort, mapping SharePoint, OneNote, or other systems as a network drive to the hosting server could be considered, using a program such as expandrive."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#end",
    "href": "work/sharepoint-oh-no.html#end",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "On the off chance that anyone makes it to the end this article got a chuckle out of me and may be relatable: https://www.theregister.com/2022/07/15/on_call/"
  },
  {
    "objectID": "work/renv-environments.html",
    "href": "work/renv-environments.html",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "",
    "text": "#| echo: false\n#| include: false\n\nlibrary(renv)\nThis vignette is an overview of environment management in R and a comprehensive summary of the different options that can be configured to support different workflows. Environment management in R is intentionally complex, so figuring out where to even start when debugging can be a challenge. This vignette also goes into specific scenarios that might come up with environment management and recommendations."
  },
  {
    "objectID": "work/renv-environments.html#at-a-glance",
    "href": "work/renv-environments.html#at-a-glance",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "At a glance",
    "text": "At a glance\nOverview of the R environment:\n\n\n\n\n\ngraph LR\n    \n    subgraph ENV[Working R Environment]\n    \n    subgraph CONFIG[Config]\n    \n      subgraph LOCAL[Local R Config]\n      RENVIRON[.Renviron]\n      RPROFILE[.Rprofile]\n      end\n    \n      subgraph SERVER[Server R Config]\n      SRENVIRON[Renviron.site&lt;br/&gt;etc/R.home/Renviron.site]\n      SRRPROFILE[Rprofile.site&lt;/br&gt;etc/Rprofile.site]\n      \n        subgraph W[Posit Workbench]\n        REPOS[\"repos.conf\"]\n        RSESSION[\"rsession.conf\"] \n        end\n    \n      end\n      \n      LOCAL-- User settings &lt;br/&gt;override&lt;br/&gt;global settings --&gt; SERVER\n      \n      subgraph RENVCONFIG[Renv Config]\n      RENVPROJECT[Project Settings&lt;br/&gt;renv/settings.json]\n      \n        subgraph RENVUSER[Config: User Level Settings]\n        RENVUR[\"User Renviron&lt;br/&gt;~/.Renviron\"]\n        RENVRI[\"R installation&lt;br/&gt;etc/Rprofile.site\"]\n        RENVP[\"Project&lt;br/&gt;.Rprofile\"]\n        end\n      end      \n      \n    end\n    \n    subgraph LIBRARY[Package Library Path]\n\n      USERLIBRARY[\"User&lt;br/&gt;R_HOME/library&lt;br/&gt;~/R\"]\n\n      SITELIBRARY[Site&lt;br/&gt;R_HOME/site-library]\n      \n      subgraph RENV[Renv]\n      direction TB\n      CACHE[\"Cache&lt;br/&gt;~/.cache/R/renv/\"]\n      PROJECTCACHE[\"Project Cache&lt;br/&gt;~/renv/library/\"]\n      CACHE-- Unless isolated, symlink --&gt; PROJECTCACHE; \n      SHAREDCACHE[Cross-User Shared Cache]\n      end\n\n    end  \n    \n    LIBRARY --&gt; CONFIG\n    CONFIG --&gt; LIBRARY\n    \n    end\n    \n    subgraph REPOSITORY[Package Repository Source]\n      direction TB\n    \n      subgraph PPM[Posit Package Manager]\n      RE[Package Binaries]\n      RP[Package Sources]\n      end\n    \n      CRAN[CRAN/Pypi/BioConductor/etc]\n    \n      CRAN -- Posit sync service --&gt; PPM;\n\n    end\n    \n    UA[User-Agent request header]-- Binary requested&lt;br/&gt;Details: OS, R version --&gt;PPM\n    \n    UA --&gt; ENV"
  },
  {
    "objectID": "work/renv-environments.html#environment-management-strategies",
    "href": "work/renv-environments.html#environment-management-strategies",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Environment Management strategies",
    "text": "Environment Management strategies\nThere are severeal common environment management strategies. Some strategies can be more prone to pain and challenges later than others. Thinking about the appropriate strategy for your organization in advance can save you from a lot of hurt later.\n\n\n\nalt text\n\n\nImage: https://solutions.posit.co/envs-pkgs/environments/reproduce/reproducibility-strategies-and-danger-zones.png\n\n\n\nSnapshot and Restore\nShared Baseline\nValidated\n\n\n\n\nAll developers are responsible for their own environment management, and enabled for making their enviornments reproduceable through the use of renv’s snapshot() capability. Users can freely access and install packages while following a package-centric workflow. Users are responsible for recording their dependencies for their projects.\nAll developers in the organization are pointed to a snapshot of available packages frozen to a particular date when the managing team had intentionally tested and made them available. On some cadence, let’s say quarterly, the managing team goes through, performs testing again, and provides a new updated snapshot that is available for developers to switch to. There are a lot of advantages in switching with new features, resolved bugs, etc.\nSimilar to the shared baseline stratgey the difference is that changes to the package environment go through an approval and auditing process, and access to packages is strictly enforced."
  },
  {
    "objectID": "work/renv-environments.html#understanding-rs-startup-behavior",
    "href": "work/renv-environments.html#understanding-rs-startup-behavior",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Understanding R’s startup behavior",
    "text": "Understanding R’s startup behavior\nR has a lot of flexibility for different workflows, which is a great thing. However, it also means that the answer to trying to change specific pieces of that customized behavior can have complex answers that depend on example what has been implemented in your environment.\nThis diagram posted by Thomas Lin Pedersen on X showing the R startup flowchart went viral, and for good reason:\n\n\n\nR Startup diagram by Thomas Lin Pedersen on X\n\n\nPosit provides precompiled R binaries for anyone to use, free of charge. The public respository can be visited to understand how they are compiled."
  },
  {
    "objectID": "work/renv-environments.html#where-packages-come-from",
    "href": "work/renv-environments.html#where-packages-come-from",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Where packages come from",
    "text": "Where packages come from\nPackages can come from a couple places, a tarball, version control location, but most commonly is the URL of the repository that the package will be installed from. The package source can be set by assigning an environment variable with the desired location. More than one repository can be specified, for example with:\nrepos &lt;- c(CRAN = \"https://cloud.r-project.org\", WORK = \"https://work.example.org\")\noptions(repos = repos)\nSetting it this way would be a “one off” that would change the “package repository” for the current session. In order to persist the change of repository location, and other settings, various configurations can be applied.\nTypically “package repository”, among developers, is used to refer to R and Python package repositories (not to be confused with linux package repositories, etc). Most R and Python package managers serve only R and Python packages, and don’t handle additional management of system dependencies or packages, which would be risky in a shared server system where conflicts could come up.\nThe most famous R and Python package repositories are:\n\nCRAN - hosting public packages, checking, distributing, and archiving R packages for various platforms\nBioConductor - hosting public packages, checking, distributing, and archiving R packages for various platforms\nPyPi - hosting public packages, checking, distributing, and archiving Python packages for various platforms\n\nPosit Package Manager can be deployed within your organization, completely air-gapped, or with a sync service to Posit, to receive package sources and binaries.\n\nPosit Package Manager - hosting public packages, hosting internal packages, checking, distributing, blocking vulnerabilities, and archiving R and Python packages for various platforms"
  },
  {
    "objectID": "work/renv-environments.html#server-vs-individual-environments",
    "href": "work/renv-environments.html#server-vs-individual-environments",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Server vs individual environments",
    "text": "Server vs individual environments\nDevelopers can work locally on their local machines, in a cloud environment, or using a shared server environment (for example, by using Posit Workbench).\nHaving multiple developers working on a centralized server using Posit Workbench has a couple primary advantages:\n\nBetter IT oversight and security with encrypted traffic and restricted IP addresses\nAdditional configuration options and settings\nAuditing and logging\nLess time spent on software installation and management\nAccess to larger compute resources\nOptions for standardizing settings across all users\n\nWhen sharing a server environment users will sign in separately and work will live in separate user home directories. Workbench can act as an auth client to different data sources. However, the shared system dependencies will need to be carefully managed to support the different workflows that the users are doing."
  },
  {
    "objectID": "work/renv-environments.html#the-renv-package",
    "href": "work/renv-environments.html#the-renv-package",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "The renv package",
    "text": "The renv package\nRenv is an open source R package that allows users to better manage their package environments.\nEver had your code mysteriously stop working or start producing different results after upgrading packages, and had to spend hours debugging to find which package was the culprit? Ever tried to collaborate on code just to get stuck on trying to decipher various package dependencies?\nrenv helps you track and control package changes - making it easy to revert back if you need to. It works with your current methods of installing packages (install.packages()). It comes with a great degree of flexibility and supports a wide range of user workflows.\nRenv assumes:\n\nUsers are familiar with a version control system, like git\nUsers are following a project-centric methodology where the goal is to simultaneously work on different projects with different package environment needs\n\n\n\n\nThe Renv workflow\n\n\nThere is an excellent video by David Aja discussing why he started using renv at the 2022 RStudio Conference here: https://www.rstudio.com/conference/2022/talks/you-should-use-renv/\nUsefully, renv doesn’t have system requirements.\n\nThe lock file\nThe renv lock file is what is generated that allows the environment to be recreated on another system. It might look something like this:\n\n\nClick here to expand an example renv lock file\n\n{\n  \"R\": {\n    \"Version\": \"4.3.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://p3m.dev/cran/latest\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"MASS\": {\n      \"Package\": \"MASS\",\n      \"Version\": \"7.3-60\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"grDevices\",\n        \"graphics\",\n        \"methods\",\n        \"stats\",\n        \"utils\"\n      ],\n      \"Hash\": \"a56a6365b3fa73293ea8d084be0d9bb0\"\n    },\n    \"Matrix\": {\n      \"Package\": \"Matrix\",\n      \"Version\": \"1.6-4\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Requirements\": [\n        \"R\",\n        \"grDevices\",\n        \"graphics\",\n        \"grid\",\n        \"lattice\",\n        \"methods\",\n        \"stats\",\n        \"utils\"\n      ],\n      \"Hash\": \"d9c655b30a2edc6bb2244c1d1e8d549d\"\n    },\n    \"yaml\": {\n      \"Package\": \"yaml\",\n      \"Version\": \"2.3.7\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"0d0056cc5383fbc240ccd0cb584bf436\"\n    }\n  }\n}\n\nIt’s in a json format. There are two main sections:\n\nHeader : This is where the R version is declared as well as package sources (if declared)\nPackages : This is where the specific package versions are specified, as well as various metadata\n\nFor an overview on package sources, see the Package Sources vignette.\nThe package source can be set for three different scenarios:\n\nRemoteType - packages installed by devtools, remotes, and pak\nRepository - packages installed from a package repository; CRAN, Posit Package Manager, etc\nbiocViews - packages installed from BioConductor repositories\n\nLet’s understand how the Repository is set. Notice how under each package the repository is declared like this:\nRepository: &lt;a name&gt;,\nThe Repository: &lt;a name&gt; field is used to denote the repository that the package was originally installed from. Most commonly it might like look:\n\nRepository: CRAN - This indicates that the package was installed from a repository call CRAN, likely a CRAN mirror\nRepository: RSPM - This indicates that the package was installed from Posit Package Manager, regardless of whether it was a binary or source package\n\nThere is a fail over order for determining the correct URL:\n\n\n\n\n\ngraph TD;\n    A(Assign repository URL) --&gt;lock; \n    \n    subgraph lock[renv.lock file]\n    B[Repository name in package definition]\n    c[Repository URL in header]\n    end\n    \n    lock -- Repository name in header --&gt;D;\n    D[Select matching URL] --&gt;END;\n    lock -- Repository name not in header --&gt;E;\n    \n    E{Check env for first repository listed &lt;br&gt; for required package version} -- package exists --&gt;F;\n    F[Select first repository URL] --&gt;END; \n    E -- package does not exist --&gt;G;\n\n    G{Check env for .. repository listed &lt;br&gt; for required package version} -- package exists --&gt;H;\n    H[Select .. repository URL] --&gt;END; \n    G -- package does not exist --&gt;I;\n    \n    I{Check env for last repository listed &lt;br&gt; for required package version} -- package exists --&gt;J;\n    J[Select last repository URL] --&gt;END; \n    I -- package does not exist --&gt;K;\n    \n    K{Check the cellar} -- package exists --&gt;L;\n    L[Select cellar] --&gt;END; \n    K -- package does not exist --&gt;M;    \n    \n    M[Package does not exist, unable to restore]\n    \n    END(End)\n\n\n\n\n\n\nIn words, for a package repository declaration of Repository: RSPM, if there happens to be a repository called RSPM in the repository list, then that repository will be preferred when restoring the package; otherwise, renv will check each repository from first to last for the required version of each package. The renv package cellar is meant to help with packages that aren’t available or accessible for installation. The cellar can be set to point at tarball locations for these tricky packages as an ultimate fail safe."
  },
  {
    "objectID": "work/renv-environments.html#the-pak-package",
    "href": "work/renv-environments.html#the-pak-package",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "The pak package",
    "text": "The pak package\nPak is a useful R package that can help with package installation and dependency look up.\nIf an error is encountered, we may need to enable the package pak to work with renv (or be patient and wait a couple minutes after installing pak). There is a useful git issue discussing this here.\nRenv can be told to use pak for package installation with: RENV_CONFIG_PAK_ENABLED = TRUE\nFor example temporarily with: Sys.setenv(\"RENV_CONFIG_PAK_ENABLED\" = TRUE))\nCheck that it set with: Sys.getenv('RENV_CONFIG_PAK_ENABLED')"
  },
  {
    "objectID": "work/renv-environments.html#package-installation",
    "href": "work/renv-environments.html#package-installation",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Package installation",
    "text": "Package installation\nPackages are installed into a package library, a directory that exists somewhere on disk.\nPackages are associated with that the OS, the particular version of R being used, and if using renv, with that particular project directory. The current library path(s) can be found with: .libPaths(). When packages are installed they will install to a sub folder that is specific to the combination of both of those.\n\nThe default library location\nThe default R installation will install packages into the users home directory, by default located at R_HOME/library. For example, on Windows:\n\\-- C:/Users/LisaAnders/AppData/Local/R\n    \\-- win-library\n        \\-- 4.3\n            \\-- ..packages\n\\-- C:/Program Files/R\n    \\-- R-4.3.1\n        \\-- library\n            \\-- ..packages\nLearn more about managing libraries in base R.\n\n\nShared site library location\nA shared site library can be set it up that will make packages from a global directory available to all users on the system, without the need for them to go through the installation steps. Through configuring Workbench, default repository locations can be set, an alternative directory can be set for use for package installation instead of user home directories, and user package installations can be disabled.\nA default site library can be used, at R_HOME/site-library (in this case /opt/R/3.4.4/lib/R/library), or a site library can be set up by setting .Library.site in R_HOME/etc/Rprofile.site / {$R_HOME}/etc/Rprofile.site. Multiple library locations can be set up to be used.\nWhen using a shared library, user options to change repository settings and package installation can be disabled if desired (typically as part of a validated environment management workflow). In this case, all users are accessing packages from that global site library and packages are added / updated by going through an approvals process with an admin ultimately running the commands that make the change.\nA site library can also be set up that allows users to access both the globally installed packages as well as install packages into the user directory. This is often “the best of both worlds”. New users are able to hit the ground running quickly, and advanced users have control over packages and package versions for their projects.\n\n\nRenv library location\nPackages installed with renv, depending on some configuration options, will use two locations:\n\nUser’s cache - ~/.cache/R/renv/\nProject cache - ~/renv/library/\n\nBy default, the project cache will symlink to the users cache in order to preserve space. Projects can be isolated in order to have the packages copied into the project library so that the project is completely independent of the broader renv cache.\nThe folder structure (note that it is specific to the possible OS’s, and the possible R versions and this is just an example) is:\n~/.cache/R/renv/\n+-- projects \n+-- index\n\\-- binary\n    \\-- linux-centos-7\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n        \\-- R-4.4\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n    \\-- linux-rocky-8.9\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n\\-- source\n    \\-- repository\n        \\-- ..packages\n~/renv/\n+-- activate.R\n+-- settings.json\n+-- staging\n\\-- library\n    \\-- linux-centos-7\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n        \\-- R-4.4\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n    \\-- linux-rocky-8.9\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n\\-- source\n    \\-- repository\n        \\-- ..packages"
  },
  {
    "objectID": "work/renv-environments.html#local-r-config-files",
    "href": "work/renv-environments.html#local-r-config-files",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Local R config files",
    "text": "Local R config files\nThese two configuration files, that may or may not be set, are the moste common for changing the behavior as relates to setting the repository for package installations:\n\n.Renviron : The user R environ file contains all environment variables, often including renv settings, etc (typically located at ~/.Renviron)\n.Rprofile : The user R profile file contains various settings and configuration properties (typically located at ~/.Rprofile)\n\nThe easiest way to access either of this files is with the usethis package.\nlibrary(usethis)\nusethis::edit_r_environ() \nusethis::edit_r_profile()\nThese startup files can be disabled."
  },
  {
    "objectID": "work/renv-environments.html#shared-server-r-config-files",
    "href": "work/renv-environments.html#shared-server-r-config-files",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Shared server R config files",
    "text": "Shared server R config files\nInstead of setting individually with .Renviron and .Rprofile, the same parameters can be set at the server and R installation level. When set, any configuration will be active for any R sessions launched on that server.\n\nRprofile.site : The RProfile.site file is typically located at etc/Rprofile.site\nRenviron.site : The Renviron.site file is specific to the R installation, typically located at file.path(R.home(\"etc\"), \"Renviron.site\").\n\nFor example, this code can be used to maintain the repository configuration across R sessions by adding to the individual users .Rprofile file. It can be maintained across all users on the server by adding to the Rprofile.site file.\nlocal({\n  repos &lt;- c(PackageManager = \"https://packagemanager.posit.co/cran/__linux__/centos7/latest\")\n  repos[\"LocalPackages\"] &lt;- \"https://packagemanager.posit.co/local/__linux__/centos7/latest\"\n  # add the new repositories first, but keep the existing ones\n  options(repos = c(repos, getOption(\"repos\")))\n})\ngetOption(\"repos\")\nUsers can override the global settings in these files Rprofile.site and Renviron.site with their individual .Rprofile files."
  },
  {
    "objectID": "work/renv-environments.html#workbench-files-for-rstudio-pro-sessions",
    "href": "work/renv-environments.html#workbench-files-for-rstudio-pro-sessions",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Workbench files for RStudio Pro sessions",
    "text": "Workbench files for RStudio Pro sessions\nSimilarly, there are configuration files used in Workbench that can set repository preference for package installations:\n\n/etc/rstudio/repos.conf\n/etc/rstudio/rsession.conf\n\nWhen using a shared library, user options to change repository settings and package installation can be disabled if desired:\n# /etc/rstudio/rsession.conf\nallow-r-cran-repos-edit=0\nallow-package-installation=0"
  },
  {
    "objectID": "work/renv-environments.html#configuration-of-renv",
    "href": "work/renv-environments.html#configuration-of-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration of renv",
    "text": "Configuration of renv\nFor most users, renv’s default behavior is powerful and doesn’t need modification.\nHowever, the behavior can also be manually set / modified. Generally speaking though, relying on the defaults is the recommended happy path as renv is designed to just magically work. This does mean that troubleshooting when things go wrong can be tricky, see the troubleshooting section below for some tips on what to look out for.\nThere are also a number of environment variables that can be set that will also similarly effect the behavior as relates to setting the repositories being used as the source for package installation.\nCommonly, these settings are set in the .Renviron file to be set across all sessions for that user, or in the R installation’s Renviron.site file so it is active for all users on that server.\nSettings:\n\nRENV_PATHS_PREFIX : Used for sharing state across operating systems\nRENV_PATHS_CELLAR : Path to tarballs, used as a last ditch effort for installing tricky packages\nRENV_PATHS_CACHE : Path location for a cache shared across multiple users\nRENV_CACHE_USER : When using a shared cache, renv can re-assign ownershp of the cache’d package to a separate user account\nrenv.download.trace : Run options(renv.download.trace = TRUE) to temporarily have more verbose logging\n\nConfig settings:\n\nrenv.config.repos.override : Enforce the use of some repositories over what is defined in the renv.lock file\nrenv.config.ppm.enabled : Attempt to transform the repository URL in order to receive binaries on your behalf (defaults to TRUE)\nrenv.config.ppm.default : If repos have not already been set (for example, from the startup .Rprofile) then projects using renv will use the Posit Public Package Manager instance by default\nrenv.config.ppm.url : The URL for Posit Package Manager to be used for new renv projects\nrenv.config.user.environ : Load the users R environ file, usually encouraged (defaults to true)\nrenv.config.user.profile : Load the users R profile file, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.user.library : option to include the system library on the library paths for projects, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.external.libraries : Similar to renv.config.user.library, external libraries can be included with the project, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.cache.enabled : Enable the global renv package cache, so that packages are installed into the global cache and then linked or copied into the users R library in order to save space (defaults to true)\nrenv.config.cache.symlinks : Use symlinks to reference packages installed into the global renv package cache (if set to FALSE packages are copied from the cache into your project library) (enabled by default, defaults to NULL)\nrenv.config.pak.enabled : Use pak with renv to install packages\n\nSince the configuration settings can be set in multiple places, the priority is given according to:\n\n\n\n\n\ngraph TD;\n    A(Renv configuration selection) --&gt;B;\n    B{R option &lt;br/&gt; renv.config.&lt;name&gt;} -- Not set --&gt;C;\n    B{R option &lt;br/&gt; renv.config.&lt;name&gt;} -- Set --&gt;F;\n    C{Environment variable &lt;br/&gt; RENV_CONFIG_&lt;NAME&gt;} -- Not set --&gt;D;\n    C{Environment variable &lt;br/&gt; RENV_CONFIG_&lt;NAME&gt;} -- Set --&gt;F;\n    D{Default} --&gt;F;\n    F(End)\n\n\n\n\n\n\nIf both the R option and the environment variable option are defined, the R option is preferred.\nWe can check the value of any of these parameters a couple ways:\n# Checking the renv options by reading environment variables and renv config properties\nrenv::paths$library()\nSys.getenv('RENV_PATHS_CACHE')\nSys.getenv('RENV_CACHE_USER')\nrenv::paths$cache()\n\n# Check the r_environ and r_profile contents using the usethis package\nlibrary(usethis)\nusethis::edit_r_environ() \nusethis::edit_r_profile()\n\nRenv and binary package OS and R version detection\nBy default, renv used with Package Manager will dynamically set the URL of your repository to pull package binaries for your respective system.\n\nStarting with R 4.4.0, renv automatically uses a platform prefix for library paths on linux (the equivalent to setting RENV_PATHS_PREFIX_AUTO = TRUE). This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required.\n\n\nSharing state across operating systems\nAs of renv 0.13.0, sharing state across operating systems is now possible. By default, it will construct a prefix based on fields within the system’s /etc/os-release file.\nalso possible to explicitly set with the RENV_PATHS_PREFIX environment variable. For example, it could be set like RENV_PATHS_PREFIX = \"ubuntu-bionic\" in order to programmatically generate a cache path like /mnt/shared/renv/cache/v2/ubuntu-bionic/R-3.5/x86_64-pc-linux-gnu. Alternatively the auto feature can be enabled with RENV_PATHS_PREFIX_AUTO = TRUE to automatically detect the environment and set the path.\nCommonly, this would be set in the .Renviron file to be set across all sessions for that user, or in the R installation’s Renviron.site file so it is active for all users on that server.\n\n\n\nRenv and binary package OS and R version detection\nRenv’s default behavior is powerful when using it with Posit Package Manager. It will automatically try to detect the details about your underlying system and set the corrrect URL path so that the appropriate binaries are downloading. If it is unable to find a binary, then it will fail over to the source URL."
  },
  {
    "objectID": "work/renv-environments.html#configuration-of-posit-package-manager",
    "href": "work/renv-environments.html#configuration-of-posit-package-manager",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration of Posit Package Manager",
    "text": "Configuration of Posit Package Manager\nPosit Package Manager is a hosting repository that can be deployed inside a companies network. It is often used in conjunction with vulnerability detection and package blocking for security. It is also useful for hosting internally developed packages that are meant to stay confidential and only used within that particular enterprise organization.\n\nConfiguring R Environments\nConfiguring Python Environments\n\nFor Workbench the URL for Package Manager is commonly configured so that it is at least used as the default repository for both R and Python packages from within the customers enterprise network.\nOptionally, the Posit Package Manager url can be configured to be specific to:\n\nSnapshot dates\nParticular curated repository/repositories\nParticular OS (in order to install binaries)\n\n\nPackage Manager and binary package OS and R version detection\nBinary packages are incredibly useful, enabling faster downloads by skipping the compilation step. When a binary package is requested (by using the __linux__ URL), Package Manager will make a best effort to serve the requested binary package. If that package is unavailable or unsupported on the user’s binary distribution Package Manager will fall back to serving the packages source version.\nPosit Package Manager has the option for the R user agent header can be configured. The user’s User-Agent request header indicates to Package manager which appropriate binary package to server, based on the R version and the OS. A diagnostic script is provided for generating a diagnostic to make sure this is set correctly. The diagnostic will fail to indicate that the OS and R version in the User-Agent request header needs to be updated.\n\n\nClick here to expand for the diagnostic script\n\n# User agent diagnostic script for Posit Package Manager binary packages\n\nlocal({\n  if (.Platform$OS.type != \"unix\" || Sys.info()[\"sysname\"] == \"Darwin\") {\n    message(\"Success! Posit Package Manager does not require additional configuration to install binary packages on macOS or Windows.\")\n    return(invisible())\n  }\n\n  dl_method &lt;- getOption(\"download.file.method\", \"\")\n  dl_extra_args &lt;- getOption(\"download.file.extra\", \"\")\n  user_agent &lt;- getOption(\"HTTPUserAgent\", \"\")\n\n  if (dl_method == \"\") {\n    dl_method &lt;- if (isTRUE(capabilities(\"libcurl\"))) \"libcurl\" else \"internal\"\n  }\n\n  default_ua &lt;- sprintf(\"R (%s)\", paste(getRversion(), R.version$platform, R.version$arch, R.version$os))\n\n  instruction_template &lt;- 'You must configure your HTTP user agent in R to install binary packages.\n\nIn your site-wide startup file (Rprofile.site) or user startup file (.Rprofile), add:\n\n# Set default user agent\n%s\n\n\nThen restart your R session and run this diagnostic script again.\n'\n\n  message(c(\n    sprintf(\"R installation path: %s\\n\", R.home()),\n    sprintf(\"R version: %s\\n\", R.version.string),\n    sprintf(\"OS version: %s\\n\", utils::sessionInfo()$running),\n    sprintf(\"HTTPUserAgent: %s\\n\", user_agent),\n    sprintf(\"Download method: %s\\n\", dl_method),\n    sprintf(\"Download extra args: %s\\n\", dl_extra_args),\n    \"\\n----------------------------\\n\"\n  ))\n\n  if (dl_method == \"libcurl\") {\n    if (!grepl(default_ua, user_agent, fixed = TRUE) ||\n        (getRversion() &gt;= \"3.6.0\" && substr(user_agent, 1, 3) == \"R (\")) {\n      config &lt;- 'options(HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])))'\n      message(sprintf(instruction_template, config))\n      return(invisible())\n    }\n  } else if (dl_method %in% c(\"curl\", \"wget\")) {\n    if (!grepl(sprintf(\"--header \\\"User-Agent: %s\\\"\", default_ua), dl_extra_args, fixed = TRUE)) {\n      ua_arg &lt;- \"sprintf(\\\"--header \\\\\\\"User-Agent: R (%s)\\\\\\\"\\\", paste(getRversion(), R.version[\\\"platform\\\"], R.version[\\\"arch\\\"], R.version[\\\"os\\\"]))\"\n      if (dl_extra_args == \"\") {\n        config &lt;- sprintf(\"options(download.file.extra = %s)\", ua_arg)\n      } else {\n        config &lt;- sprintf(\"options(download.file.extra = paste(%s, %s))\", shQuote(dl_extra_args), ua_arg)\n      }\n      message(sprintf(instruction_template, config))\n      return(invisible())\n    }\n  }\n\n  message(\"Success! Your user agent is correctly configured.\")\n})"
  },
  {
    "objectID": "work/renv-environments.html#configuration-on-workbench-for-r-repository-using-run.r-programmatically-setting-the-repository-location",
    "href": "work/renv-environments.html#configuration-on-workbench-for-r-repository-using-run.r-programmatically-setting-the-repository-location",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration on Workbench for R repository using run.R / Programmatically setting the repository location",
    "text": "Configuration on Workbench for R repository using run.R / Programmatically setting the repository location\nInstead of the above, a run.R file can be used to programmatically set the repository and library location for users. This is commonly used in validated workflows, where the additional oversight is critical.\nExample created by Michael here."
  },
  {
    "objectID": "work/renv-environments.html#scenario-1-setting-up-a-shared-site-library-on-workbench",
    "href": "work/renv-environments.html#scenario-1-setting-up-a-shared-site-library-on-workbench",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 1: Setting up a shared site library on Workbench",
    "text": "Scenario 1: Setting up a shared site library on Workbench\nThe shared site library is specific to an installed version of R. For example for R version 4.3.2 installed to: /opt/R/4.3.2/lib/R/library:\n\nEdit the Rprofile.site file to set the repository URL\n\n# /opt/R/4.3.2/etc/Rprofile.site\nlocal({\n  options(repos = c(CRAN = \"https://r-pkgs.example.com/cran/128\"))\n})\n\n(optional) The default site library can be used, at R_HOME/site-library (in this case /opt/R/3.4.4/lib/R/library), or a site library can be set up by setting .Library.site in R_HOME/etc/Rprofile.site. Multiple library locations can be set up to be used.\nRun R as the root/admin account and install all desired packages\n\n# Multiple packages can be installed at the same time like this: \nexport R_VERSION=4.3.2\n\n/opt/R/${R_VERSION}/bin/R\n\nsudo /opt/R/${R_VERSION}/bin/Rscript -e 'install.packages(c(\"haven\",\"forcats\",\"readr\",\"lubridate\",\"shiny\", \"DBI\", \"odbc\", \"rvest\", \"plotly\",\"rmarkdown\", \"rsconnect\",\"pins\",\"png\",\"tidyverse\", \"Rcpp\"), repos = \"http://cran.us.r-project.org\")'\n\nq()\n\nUsers access packages on the system (without needing to install)\n\nWhen using a shared library, the ability for users to change repository settings and package installation can be disabled:\n# /etc/rstudio/rsession.conf\nallow-r-cran-repos-edit=0\nallow-package-installation=0"
  },
  {
    "objectID": "work/renv-environments.html#scenario-2-setting-up-a-project-to-use-renv",
    "href": "work/renv-environments.html#scenario-2-setting-up-a-project-to-use-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 2: Setting up a project to use renv",
    "text": "Scenario 2: Setting up a project to use renv\n# install renv\ninstall.package(\"renv\") \nlibrary(renv)\n\n# activate the project as an renv project\nrenv::activate()\n\n# generate the renv.lock file \nrenv::snapshot()\n\n# check the status of renv \nrenv::status()\n\n# On a separate system the snapshot can be used to install the specific packages and versions \nrenv::restore() \n\n# Restore a project with an explicit repository URL, note that this does not update the renv.lock file, it will need to be manually edited\nrenv::restore(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\"), rebuild=TRUE)\n\n# Add additional logging\noptions(renv.download.trace = TRUE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-3-determining-the-root-package-that-is-causing-a-failing-dependency",
    "href": "work/renv-environments.html#scenario-3-determining-the-root-package-that-is-causing-a-failing-dependency",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 3: Determining the root package that is causing a failing dependency",
    "text": "Scenario 3: Determining the root package that is causing a failing dependency\nFor example, error message:\n\n2024/05/17 9:24:10 AM: Error in dyn.load(file, DLLpath = DLLpath, …) : 2024/05/17 9:24:10 AM: unable to load shared object ‘/opt/rstudio-connect/mnt/app/packrat/lib/x86_64-pc-linux-gnu/4.3.2/magick/libs/magick.so’: 2024/05/17 9:24:10 AM: libMagick++-6.Q16.so.8: cannot open shared object file: No such file or directory 2024/05/17 9:24:10 AM: Calls: loadNamespace -&gt; library.dynam -&gt; dyn.load\n\nWe can look through our project repository and see that the magick package isn’t directly being called. So the question is, which package is calling it as dependency?\nThe easiest way to look up the dependency is to open the renv.lock file and find which package has it listed as a dependency.\nSome other tricks that might be useful are:\n\nWe can use renv to look at top level dependencies: renv::dependencies()\nWe can use base R to look up package dependencies: tools::package_dependencies(\"leaflet\", recursive = TRUE)[[1]]\nRenv can be told to use pak for package installation with: RENV_CONFIG_PAK_ENABLED = TRUE\nCheck that it set with: Sys.getenv('renv.config.pak.enabled')\nWe can use pak to look up all package dependencies in a tree format: pak::pkg_deps_tree(\"tibble\")\nWe can also get more details about the packages with: pak::pak_sitrep()\nIf an error is encountered, we may need to enable the package pak to work with renv (or be patient and wait a couple minutes after installing pak). There is a useful git issue discussing this here.\n\nWe can then clean up the project and remove packages that are installed, but no longer referenced in the project source, with renv::clean() and save that to the renv lock file with renv::snapshot(). Don’t forget to update your manifest.json file if this is a project being published to Connect with rsconnect::writeManifest()."
  },
  {
    "objectID": "work/renv-environments.html#scenario-4-upgrading-a-project-using-renv-from-r-4.1-to-r-4.4",
    "href": "work/renv-environments.html#scenario-4-upgrading-a-project-using-renv-from-r-4.1-to-r-4.4",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 4: Upgrading a project using renv from R 4.1 to R 4.4",
    "text": "Scenario 4: Upgrading a project using renv from R 4.1 to R 4.4\n\nWhy is this relevant? R CVE detection, vulnerability removed with R 4.4\n\nWhat is recommended: For each project, individually capture the requirements with renv. Change the R version and use the renv.lock file to install the captured requirements for the new R version. Perform tests, updating code and package versions as needed.\nWhat is not recommended: An in-place upgrading. Meaning, we do not recommend removing existing R versions and forcing all projects to use R 4.4. It is likely that code will break and will need developer work to make compatible with the new R version."
  },
  {
    "objectID": "work/renv-environments.html#scenario-5-os-migration-for-individual-r-projects-using-renv",
    "href": "work/renv-environments.html#scenario-5-os-migration-for-individual-r-projects-using-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 5: OS migration for individual R projects using renv",
    "text": "Scenario 5: OS migration for individual R projects using renv\nRefer to here\nAll packages will need to be rebuilt.\nThese two locations in particular, the user home directories and global R or Python directories, will likely need to be flushed and rebuilt:\n\n~/R\n~/.local/lib/python3.*\n\nReference this script from David which programmatically reinstalls all packages installed into user home directories, or the global R or Python directories.\nRebuild renv:\n# Delete existing libraries\nunlink(\"renv/library\", recursive=TRUE)\n\n# Restart R session\n.rs.restartR()\n\n# Change anything that is needed, repository URL, etc\n\n# Re-install libraries\nrenv::restore(rebuild = TRUE)\nRebuild venv:\n# Activate existing venv\nsource .venv/bin/activate\n\n# Capture all installed packages\npython -m pip freeze &gt; requirements-freeze.txt\n\n# Deactivate and delete\ndeactivate\nrm -rf .venv/\n\n# Change anything that is needed, repository URL, etc\n\n# Create a new virtual environment\npython -m venv .venv\nsource .venv/bin/activate \npython -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements-freeze.txt\nFor Connect, the content runtimes will need to be cleared and rebuilt. This can be done pre-emptively.\nDelete:\n# Enumerate the caches known to your server.\nrsconnect system caches list \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key\n\n# Validate cache targeted for deletion.\nrsconnect system caches delete \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --language Python \\\n    --version 3.9.5 \\\n    --dry-run\n\n# Delete one cache.\nrsconnect system caches delete \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --language Python \\\n    --version 3.9.5\nRebuild:\n# Enumerate every \"published\" content item and save its GUID.\nrsconnect content search \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --published | jq '.[].guid' &gt; guids.txt\n\n# Queue each GUID for build.\nxargs printf -- '-g %s\\n' &lt; guids.txt | xargs rsconnect content build add \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key\n\n# Build each queued content item.\nrsconnect content build run \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key"
  },
  {
    "objectID": "work/renv-environments.html#scenario-6-changing-the-project-repository-url",
    "href": "work/renv-environments.html#scenario-6-changing-the-project-repository-url",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 6: Changing the project repository URL",
    "text": "Scenario 6: Changing the project repository URL\nOften the package repository is set to a specific source URL. This can be due to it being within your network, or so that you are getting binaries for a specific OS version, etc.\nUsing the RENV_CONFIG_REPOS_OVERRIDE setting:\noptions('repos')\n\n# Set the override as a one off \nSys.setenv(\"RENV_CONFIG_REPOS_OVERRIDE\" = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \n\n# Check that it set \nSys.getenv(\"RENV_CONFIG_REPOS_OVERRIDE\")\n\n# Turn on debug logging so we can see more information about where packages are coming from and verify it's using the correct URL\noptions(renv.download.trace = TRUE)\n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE) \n\n#Override only applies during restore, and won't update the renv.lock file, so either manually update the renv.lock file with the appropriate URLor using renv::snapshot(repos = \"\")\nUsing the repos setting during rebuild:\n# Rebuild \nrenv::restore(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\"), rebuild=TRUE)\n\n# Snapshot s the URL change is reflected\nrenv::snapshot(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \nChanging it directly in the renv.lock file:\noptions('repos')\n\n# Either manually update the renv.lock file with the appropriate URL or using\nrenv::snapshot(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-7-recovering-an-old-project-that-didnt-have-an-renv-and-isnt-working-with-latest-r-package-versions",
    "href": "work/renv-environments.html#scenario-7-recovering-an-old-project-that-didnt-have-an-renv-and-isnt-working-with-latest-r-package-versions",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 7: Recovering an old project that didn’t have an renv and isn’t working with latest R, package versions",
    "text": "Scenario 7: Recovering an old project that didn’t have an renv and isn’t working with latest R, package versions\nUse the snapshot date option with package manager to “guess” when the environment would have been built with renv so that package versions can be individually tweaked until the project works. Use the renv::revert feature with version control to update the packages with the ability to downgrade as needed."
  },
  {
    "objectID": "work/renv-environments.html#scenario-8-going-between-os-on-the-same-workbench-system-using-slurm-singularity-with-a-renv-project",
    "href": "work/renv-environments.html#scenario-8-going-between-os-on-the-same-workbench-system-using-slurm-singularity-with-a-renv-project",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 8: Going between OS on the same Workbench system using slurm / singularity with a renv project",
    "text": "Scenario 8: Going between OS on the same Workbench system using slurm / singularity with a renv project\nWith the interaction between renv and package manager, as well as the additions with recognition from renv when the OS and R version has changed, things should just work magically as long as the project is configured to use these pieces:\n\nrenv\npackage manager (binaries enabled)\n\nOn a system that has been configured to use slurm with singularity images (that are different OS’s) we can run these lines to get a feel for what is going on:\n# Turn on debug logging so we can see more information about where packages are coming from and verify it's using the correct URL\noptions(renv.download.trace = TRUE)\n\n# Check the default repository URL\noptions('repos')\n\n# Check the OS version\nsystem(\"cat /etc/os-release\")\n\n# Check the details of our singularity environment\nsystem(\"env | grep SINGULARITY\")\n\n# Check that auto-path prefix re-writing is set\nSys.getenv(\"RENV_PATHS_PREFIX_AUTO\")\n\n# We can attempt to set the URL to a specific binary, when we snapshot it will update the lock file to have the generic URL\nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/centos8/latest\")) \n\n# We can attempt to set the URL to a specific binary, when we snapshot it will update the lock file to have the generic URL\nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\")) \n\n# Update the renv to use a source URL as RSPM \nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/latest\")) \n\n# We can also manually set the repo outside of renv this way, for example to successfully download renv\noptions(repos=c(CRAN=\"https://cran.r-project.org\"))\n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE) \nInside the renv lock file we might see a couple different things:\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/__linux__/centos8/latest\"\n      },\nThis will cause problems and will tell renv to install the wrong version of packages for the wrong OS.\nIf we try to snapshot a binary repository URL with renv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\")) then we will see the renv.lock will be updated to:\n    \"Repositories\": [\n      {\n        \"Name\": \"RSPM\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\nThis correction from the binary URL to the base URL will happen regardless of whether the OS matches the one we are using or not.\nWhen we install a package we will see that it is downloading the binary. This is the magic of RENV_PATHS_PREFIX_AUTO! This happens regardless of whether our package source is CRAN or RSPM.\nWe can test what the outputs are for each scenario:\n\nBefore a project has been initialized\nOnce a project has been initialized, with renv\nClosing the project and re-opening it with a different image (different OS) and restoring packages (‘renv::restore(rebuild=TRUE)’)\n\nThe auto-path prefix re-writing is really powerful. This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required. The caveats to know are:\n\nStarting with 4.4, renv automatically uses a platform prefix for library paths on linux.\nR versions below this may need to have the paths prefix set (for example for just the session with Sys.setenv(\"RENV_PATHS_PREFIX_AUTO\" = TRUE), though most likely this should be set at the user or global level).\n\nWe can set auto-path prefix re-writing at the user level by adding RENV_PATHS_PREFIX_AUTO = TRUE into the user r environ file:\nlibrary(usethis)\nusethis::edit_r_environ()"
  },
  {
    "objectID": "work/renv-environments.html#package-installation-errors-on-workbench",
    "href": "work/renv-environments.html#package-installation-errors-on-workbench",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Package installation errors on Workbench",
    "text": "Package installation errors on Workbench\nHere’s an example error message that occurred during package installation inside Workbench (install.packages(askpass)):\n\n* installing binary package ‘askpass’ … cp: cannot open ‘./libs/askpass.so’ for reading: Operation not permitted /usr/bin/gtar: You may not specify more than one ‘-Acdtrux’, ‘–delete’ or ‘–test-label’ option Try ‘/usr/bin/gtar –help’ or ‘/usr/bin/gtar –usage’ for more information. /usr/bin/gtar: This does not look like a tar archive /usr/bin/gtar: Exiting with failure status due to previous errors\n\nA good first trouble shooting step is to SSH on the server and open an R session as root and attempt to install the same package. This helps to rule out where the issue is coming from, the global R configuration, the server, or a specific user issue or something with the Workbench configuration. Create a R session after SSH-ing into the server with /opt/R/${R_VERSION}/bin/R\n\nWhere to start\nGet the system information: Sys.info()\nGet session details: sessionInfo()\n\n\nProblems with pak\nGet details about pak (if used): pak::pak_sitrep()\nCheck if renv has been configured to use pak: Sys.getenv('renv.config.pak.enabled')\n\n\nProblems with renv : where to start\nCan they provide a renv diagnostic? It is generated by running this: renv::diagnostics().\n\n\nProblems with renv : cache location\nCheck the location of the renv cache:\nrenv::paths$library()\nSys.getenv('RENV_PATHS_CACHE')\noptions('renv.config.external.libraries')\noptions('renv.download.trace')\nrenv::paths$cache()\nSys.getenv('RENV_PATHS_PREFIX_AUTO')\nMake sure that it is located to a writeable location (if it is a mount, see the note about file mounts below, this could be a source of issues):\nsystem('namei -l /rsspdata/common/renv_cache/renv/v5/R-3.6/x86_64-pc-linux-gnu')\nCheck that the renv cache location matches the library locations: .libPaths()\nBy default packages are installed into the global cache at ~/.cache/R/renv/ and symlinked from the users cache within the project at ~/renv/library/.\nAre they using a shared renv cache, or an external library,\nDo they know if they’ve implemented settings in either of these, and could they share the contents?\n\nRprofile.site : The RProfile.site file is typically located at etc/Rprofile.site\nRenviron.site : The Renviron.site file is specific to the R installation (in this case I’m interested in if it exists for R 4.3 and R 3.6), typically located at file.path(R.home(\"etc\"), \"Renviron.site\").\nCheck if an external library is referenced in the environment: options('renv.config.external.libraries')\n\nIs the goal to use a shared renv cache location? There are a couple caveats with shared cache’s that can make them tricky. (1) cache permissions can be set with ACL’s, needing admin oversight to make sure are set correctly, (2) packages in the cache are owned by the requesting user, unless the RENV_CACHE_USER option is set. When set, renv will attempt to run chown -R &lt;package&gt; &lt;user&gt; to update cache ownership after the package has been copied into the cache.\nIf the desired behavior is to have a shared renv cache then these two settings will likely need to be added to the project .Renviron, user .Renviron, or site Renviron.site file:\n\nRENV_PATHS_CACHE : Path location for a cache shared across multiple users\nRENV_CACHE_USER : When using a shared cache, renv can re-assign ownership of the cache’d package to a separate user account\n\nI’d be curious, if it’s possible for them, to see if they are able to use R 4.4, or to set that parameter RENV_PATHS_PREFIX_AUTO to true (for example for just the session with Sys.setenv(\"RENV_PATHS_PREFIX_AUTO\" = TRUE)) using their current version of R, and repeat the steps of installing a package:\n\nStarting with R 4.4.0, renv automatically uses a platform prefix for library paths on linux (the equivalent to setting RENV_PATHS_PREFIX_AUTO = TRUE). This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required.\n\nOf course, they could also try this for installing the package, bypassing the cache, and see if it works (but I’m worried that there is a ghost setting somewhere that needs to be removed so that issues don’t keep popping up):\n# install a package, bypassing the cache\nrenv::install(\"&lt;package&gt;\", rebuild = TRUE)\n\n# restore packages from the lockfile, bypassing the cache\nrenv::restore(rebuild = TRUE)\n\n\nProblems with renv : other\nCheck:\n\nAre you running the latest renv? If not, upgrade\nAdd additional logging: options(renv.download.trace = TRUE)\nTake a diagnostic: renv::diagnostics()\n\nIf you are having particular issue with a package and it keeps being pulled in from the cache then doing a complete purge and reinstall can be useful:\nrenv::purge(\"stringr\")\nrenv::purge(\"stringi\")\ninstall.packages(\"stringr\")\nrenv::purge removes packages completely from the package cache (which may be shared across projects) rather than just removing the package from the project which is what renv::remove does. This can be useful if a package which had previously been installed in the cache has become corrupted or unusable, and needs to be re-installed.\nFollow these steps to “flush” and rebuild the renv environment, without losing the important parts of your renv.lock that are defining the R version and package versions:\nrenv::snapshot()\n# Make the appropriate changes (for example, changing OS) \n# Update the renv.lock file manually to reflect any needed changes (for example, changing the repository URL) \nrenv::deactivate()\nrenv::activate()\nrenv::restore(rebuild=TRUE) \nCheck that the packages either installed into the global cache at ~/.cache/R/renv/ or the users cache within the project at ~/renv/library/. The folder structure will give some clues for whether source, binaries were installed, and which OS and R version they were installed for if specified.\n\n\nProblems with packages not persisting\nIs this on a cloud vendor? IE sagemaker, google workstations, azureml? Check that the package repository location is being saved to the mounted drive. If it is saved to the general OS that is ephemeral it will be lost when the session is spun down. This also applies for things like git credentials.\n\n\nIncorrect / corrupted R installation\nCheck for an incorrect R installation for the OS, or a R installation that has gotten corrupted. An easy way to test this is to install a new R version, making sure to closely follow the instructions as well as verifying the OS version.\n\n\nIncorrect package repository source URL for the particular system OS\nWhen R installs a binary package, it doesn’t actually check if the package can be loaded after installation, which is different from source packages. So it is unfortunately possible to install a binary package only to find out later that it can’t actually be loaded.\nCheck the URL that the user is installing from: options('repos')\nTemporarily point the repository to global CRAN and check if the packages will successfully install. For example by running this: options(repos=c(CRAN=\"https://cran.r-project.org\")) and then installing any package with install.packages(\"ggplot2\")\nCheck in /etc/rstudio/rsession.conf if there is anything that would set the library location, for example r-libs-user=~/R/library.\nIt may also be useful to verify both the OS you are currently useing as well as checking that the repository you are pointing towards is using the correct OS if it is pulling in the binaries.\nFor debian/ubuntu distributions:\nlsb_release -a\nFor other distributions (more broadly cross-linux compatible command):\ncat /etc/os-release\n\n\nUsers lacking read/write permissions to their home directory\nCheck the home directory permissions on /home/username/. For example with namei -l /home/username/.\nIf useful, could try recursively chown-ing the directory with the user experiencing the issue and chmod 750 to make sure there is access.\nThis can commonly happen after a migration from one server to another, if the correct permissions weren’t correctly carried over. This is why we commonly recommend using rsync with the -a flag for transfer any files / directories. This syncs directories recursively and preserve symbolic links, groups, ownership, and permissions. Additionally, rsync needs to be used in root mode in order to completely move the various software and home directory components as it includes files with restrictive read and write permissions.\nFor example, the permissions should look something like: -rwx-r--r--\n\n\nUsers lacking permissions to ./libs\nCheck the permissions on ./libs/. For example with namei -l ./libs and ls -la ./libs\n\n\nIncorrect PAM configuration for users\nCheck the output of sudo getent passwd username\nFrom a workbench session the output of the environment, Sys.getenv() and compare between a Workbench session and logged into a R session as root on the server (after SSH-ing in)\nFrom an SSH session as root check the outputs of the user verification commands: sudo /usr/lib/rstudio-server/bin/pamtester --verbose &lt;session-profile&gt; &lt;user&gt; authenticate acct_mgmt setcred open_session\nFor example this command will likely look like: sudo /usr/lib/rstudio-server/bin/pamtester --verbose rstudio-session username authenticate acct_mgmt setcred open_session\nCheck for any umask or mask lines used during user provisioning, in the /etc/sssd/sssd.conf file\n\n\nServer hardening\nAnother thing to check is whether SELinux is enabled on the system. Check the mode with getenforce\nThis can result in user specific errors, in that case compare the SELinux context for a user that has successfully package installations to the one that is having errors.\nOften the following command will work to fix SELinux context issues: restorecon -Rv /home/users/username\nGreat article from our support team discussing how to use selinux\nDisable SELINUX (RHEL only): setenforce 0 && sudo sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config\nCheck for FIPS being enabled: fips-mode-setup --check\nThis article from redhat on FIPS mode is also very useful.\n\n\nMounted share drive\nCheck if /home on the server, or is it a network mount (NFS or CIFS). In NFS, for example, there can be the use of access control lists which can impact permissions. Similarly, when working in a system that has a mounted share drive then would want to check that libraries are being written to that share so you get persistence. Typically this means writing to inside the home directory. Check mounted drives with: df -h\nCheck /etc/fstab to see if the home directories are mounted with noexec\nFor example, this shows that the home directories were mounted with noexec: /dev/mapper/rhel-home  /home  xfs    defaults,noexec,nosuid,nodev   0 0\nThis resulted in this error message:\nlibrary(stringi)Error: package or namespace load failed for 'stringi' in dyn.load(file, DLLpath = DLLpath, ...):\nunable to load shared object '/home/c_jjones/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so':\n  /home/c_jjones/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so: failed to map segment from shared object\n\n\nAzure cloud images\nThe default Azure RHEL images are unfortunately constricted in their ability to do some things.\n\n\nSlurm\nThe Slurm service account should have full privileges to the Slurm environment (like killing jobs).\nIn regards to not being able to run the diagnostics command, could you please provide the following:\n\nEnable debug logging by setting enable-debug-logging=1 in /etc/rstudio/launcher.slurm.conf\nTrigger the issue you are experiencing after restarting the launcher.\nResulting logs will be in: - /var/lib/rstudio-launcher/Slurm/rstudio-slurm-launcher.log\nThe Slurm version, which can be found by running sinfo –version\nThe installation location of Slurm on the host\nYour /etc/slurm.conf (or equivalent) configuration file\nThe output of running sinfo as the Slurm service user configured in /etc/rstudio/launcher.slurm.conf\nRun test job with srun date\nReplace  with a valid username of a user that is set up to run Posit - Workbench in your installation, in the commands below:\nsudo rstudio-server stop\nsudo rstudio-server verify-installation –verify-user=\nsudo rstudio-server start\nThe output of running sudo rstudio-launcher status"
  },
  {
    "objectID": "work/securing-credentials.html",
    "href": "work/securing-credentials.html",
    "title": "Securing credentials",
    "section": "",
    "text": "When working with pulling data from secure databases or other sources a developer might find themselves in a situation of needing to provide very sensitive information, such as a password or a token, in order to access the data that is needed or to successfully deploy a project. Handling those secrets in way that doesn’t expose them in the code directly is critical and where using environmental variable’s for securing sensitive variables is strongly recommended.\nAdditionally there may be parameters that are often needed that can be accessed as a variable more easily rather than having to type in every time.\nFor both of these cases knowing how environment variables can be leveraged can be very rewarding and it is surprising how little effort it can to take to set up.\n\n\nWhen R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files:\n\nUser\nProject\n\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\n\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\n\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\n\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\n\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\n\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\n\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI.\nUse Connect to manage environment variables through the UI: https://docs.posit.co/connect/user/content-settings/index.html#content-vars\nHave your admin use a supervisor script to add Environment Variables automatically: https://docs.posit.co/connect/admin/process-management/index.html#example-environment-variables\nSecuring credentials solutions article: https://solutions.posit.co/connections/db/best-practices/managing-credentials/"
  },
  {
    "objectID": "work/securing-credentials.html#working-with-the-.renviron-file",
    "href": "work/securing-credentials.html#working-with-the-.renviron-file",
    "title": "Securing credentials",
    "section": "",
    "text": "When R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files:\n\nUser\nProject\n\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\n\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\n\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\n\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\n\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\n\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\n\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI.\nUse Connect to manage environment variables through the UI: https://docs.posit.co/connect/user/content-settings/index.html#content-vars\nHave your admin use a supervisor script to add Environment Variables automatically: https://docs.posit.co/connect/admin/process-management/index.html#example-environment-variables\nSecuring credentials solutions article: https://solutions.posit.co/connections/db/best-practices/managing-credentials/"
  },
  {
    "objectID": "work/git-and-sagemaker.html",
    "href": "work/git-and-sagemaker.html",
    "title": "Problems with persistence when in the cloud",
    "section": "",
    "text": "This is a random trick that took me longer than I care to admit to figure out - and wanted to squirrel it away so it’s easy to find in the future!"
  },
  {
    "objectID": "work/git-and-sagemaker.html#problem-when-on-linux",
    "href": "work/git-and-sagemaker.html#problem-when-on-linux",
    "title": "Problems with persistence when in the cloud",
    "section": "Problem when on Linux:",
    "text": "Problem when on Linux:\n\nIn general gitcreds doesn’t work well on linux (which has led to this git issue (Ship our own credential helper on Linux · Issue #47 · r-lib/gitcreds ). There is an excellent blog post that is very useful that goes deeper into what is going on: Notes from a data witch - Managing GitHub credentials from R, difficulty level linux"
  },
  {
    "objectID": "work/git-and-sagemaker.html#problem-when-on-sagemaker",
    "href": "work/git-and-sagemaker.html#problem-when-on-sagemaker",
    "title": "Problems with persistence when in the cloud",
    "section": "Problem when on Sagemaker:",
    "text": "Problem when on Sagemaker:\n\nAdditionally, on Sagemaker things like credentials will be by default stored to the ephemeral EC2 instance and lost when the session is closed. A different method needs to be pursued in order for the token to persist."
  },
  {
    "objectID": "work/git-and-sagemaker.html#tldr-solution",
    "href": "work/git-and-sagemaker.html#tldr-solution",
    "title": "Problems with persistence when in the cloud",
    "section": "TLDR Solution:",
    "text": "TLDR Solution:\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'"
  },
  {
    "objectID": "work/git-and-sagemaker.html#testing",
    "href": "work/git-and-sagemaker.html#testing",
    "title": "Problems with persistence when in the cloud",
    "section": "Testing",
    "text": "Testing\nI’ll add a disclaimer that it is similar to the .Renviron approach where the credentials would be stored as plain text, however to a location chosen by the user.\nLoad libraries:\nlibrary(usethis) \nlibrary(gitcreds) \nlibrary(gh) \nlibrary(credentials)\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'\nFrom the documentation:\n\nThe “store” mode saves the credentials to a plain-text file on disk, and they never expire. This means that until you change your password for the Git host, you won’t ever have to type in your credentials again. The downside of this approach is that your passwords are stored in cleartext in a plain file in your home directory. The other options involve needing to change the root container to include alternative git credential helpers (libsecret or gcm core) which as far as I can tell are not currently available and would be something I’d recommend reaching out to Amazon about as they control that image.\n\nGenerate the PAT:\nusethis::create_github_token()\nCopy the generated PAT to your clipboard. Provide the PAT to this function when asked for it:\ngitcreds::gitcreds_set()\nCheck that it stored with:\ngitcreds_get()"
  },
  {
    "objectID": "work/git-and-sagemaker.html#alternatives",
    "href": "work/git-and-sagemaker.html#alternatives",
    "title": "Problems with persistence when in the cloud",
    "section": "Alternatives",
    "text": "Alternatives\nThe old way “store a PAT as the GITHUB_PAT environment variable in .Renviron.” is typically what is recommended as being more compatible with linux if you are able to switch back to it, but it can present a security issue. We’ve also commonly seen folks using the gh package for generating PATs like in Managing Personal Access Tokens\nAlternatively, there are some git config options from the terminal. See: Chapter 9 Personal access token for HTTPS | Happy Git and GitHub for the useR"
  }
]