[
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Technical writeups and musings",
    "section": "",
    "text": "Some random technical writeups for things I’ve found useful, all brought to you by Quarto’s blogging capability.\n\n\n\n\n\n\n\n\n\nPython projects set up and maintenance\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nLisa\n\n\n2 min\n\n\n\n\n\n\n\nR projects set up and maintenance\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nLisa\n\n\n6 min\n\n\n\n\n\n\n\nJob templating in Kubernetes\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\nLisa\n\n\n3 min\n\n\n\n\n\n\n\nAccess to resources in Google, an exploration\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nLisa\n\n\n6 min\n\n\n\n\n\n\n\nDebugging R Package Environments (renv): A long winded writeup\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nLisa\n\n\n48 min\n\n\n\n\n\n\n\nShiny apps and Analytics\n\n\n\n\n\n\n\n\nApr 29, 2024\n\n\nLisa\n\n\n1 min\n\n\n\n\n\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nLisa\n\n\n4 min\n\n\n\n\n\n\n\nParallelization in R\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nLisa\n\n\n7 min\n\n\n\n\n\n\n\nProblems with git credential persistence when in the cloud\n\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nLisa\n\n\n3 min\n\n\n\n\n\n\n\nSecuring credentials\n\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nLisa\n\n\n5 min\n\n\n\n\n\n\n\nConnecting to resources in Microsoft 365 / Sharepoint\n\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nLisa\n\n\n10 min\n\n\n\n\n\n\n\nPublish and version your models with Vetiver\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nLisa\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "work/sharepoint-oh-no.html",
    "href": "work/sharepoint-oh-no.html",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": ":warning: This is now outdated. Please refer to this article and the Microsoft365R package documentation developed by Hong Ooi instead for up-to-date information.\n\n\n\nMicrosoft 365 is a subscription extension of the Microsoft Office product line with cloud hosting support. Microsoft 365 uses Azure Active Directory (Azure AD) for user authentication and application access through developed APIs. The Microsoft supported method for interfacing with R developed content is with the Microsoft365R package which was developed by Hong Ooi and has extensive documentation. It supports access to Teams, SharePoint Online, Outlook, and OneDrive.\n\n\n\n\n:warning: Discussion between the developers and the Global Azure Administration team about the content and security requirements within your organization should determine which of the approaches should be supported.\n\nThere are four main authentication approaches supported by Microsoft365R. Note that multiple approaches can be supported at the same time.\n\n\n\n\n\n\n\n\n\nMethod\nauth_type\nPermissions\nCapability\n\n\n\n\nUser sign-in flow: Default\ndefault\nUser\nInteractive only (local IDE and Workbench, interactive Shiny content)\n\n\nUser sign-in flow: Device Code\ndevice_code\nUser\nInteractive only (local IDE and Workbench)\n\n\nService principal / Client secret\nclient_credentials\nApplication\nInteractive and non-interactive (same as above plus scheduled content)\n\n\nEmbedded credentials\nresource_owner\nUser\nInteractive and non-interactive (same as above plus scheduled content)\n\n\n\nAuthentication for Microsoft365R is through Microsoft’s Azure cloud platform through a registered application with appropriate assigned permissions in order to obtain ‘OAuth 2.0’ tokens.\n\n\nDepending on your organization’s security policy some steps may require support from your Azure Global Administrator.\nUser Sign-in Flow: Default\nA custom app can be created or the default app registration “d44a05d5-c6a5-4bbb-82d2-443123722380” that comes with the Microsoft365R package can be used. The user permissions will need to be enabled as detailed in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator. Additionally Redirect URLs will need to be added through Azure under App Registrations -&gt; select your app -&gt; Authentication -&gt; Platform configurations -&gt; Mobile and desktop applications -&gt;Add URI as well as also enabling nativeclient.\nFor adding Redirect URLs, which will give a typical web-app authentication experience for interactive applications:\n\nFor the desktop RStudio IDE the URL is: http://localhost:1410/.\nFor content hosted in shinyapps.io this would be of the form https://youraccount.shinyapps.io/appname (including the port number if specified).\nA SSL certificate will be required for non-local connections. This means that the Connect and Workbench URLs will need to be HTTPS. A wildcard could be used instead of adding the Redirect URL for each piece of content/user where appropriate for server-wide access.\n\nUser Sign-in Flow: Device Code\nIn addition to user level app permissions outlined above the device code workflow option will need to be enabled.\nEnabling the device code workflow is through the App Registration dashboard in Azure -&gt; click on the created app -&gt; Authentication -&gt; Allow public client flows and setting Enable the following mobile and desktop flows to yes. The device code workflow does not need Redirect URLs, instead providing a code and a link for the developer to access in a separate browser window (or even on a separate device) for sign-in.\nService Principal / Client Secret\nA custom app will need to be registered in Azure with Application permissions. The permissions can be based off of the user permissions but can be assigned as needed for the application and to comply with any security restrictions.\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. As a control using environmental variable’s for storing the client secret is recommended. Starting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\nEmbedded Credentials\nA custom app will need to be registered in Azure with User permissions as specified in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator.\nThe credentials being embedded can be a user or a service account, as long as access to the desired content inside Microsoft 365 has been granted. Creating service accounts per content is recommended to enable faster troubleshooting and easier collaboration. As a control the Username / Password should never be exposed directly in the code, instead using Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\n\n\n\n\n\n\nThe user sign-in flow option provides the typical web browser authentication experience. A user will need to be available to interact with the authentication pop-up in order to which makes this an option for interactive applications (such as the local RStudio IDE, Workbench, or an interactive Shiny app), but not applicable for scheduled content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app = app)\n\n\n\nIn some interactive cases it may be easier to use the device code flow where the user is prompted with a code and a link which is opened in a separate screen for logging in. For example for using a Workbench instance that was deployed without an SSL certificate. This does require interaction from the user and as such will not be applicable for scheduled content nor hosted content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette, with the Service Principal via using a Client Secret discussed in this section being the Microsoft recommended approach.\n\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. Instead the recommended approach is to store it as an Environment Variable which can be done through the Connect UI.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette. In cases where the additional access that comes with Application level permissions isn’t appropriate for the organization’s security requirements the embedded credentials approach can be used.\n\nThe credentials embedded will need to be granted access to the desired content and can either be a user or a service account. Working with your Azure Global Administrator to create service accounts per content is recommended to enable fast troubleshooting and easier collaboration.\nSensitive variables such username / password should be embedded as Environment Variables so that they aren’t exposed in the code directly.which can be done through the Connect UI. See the example here.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nuser &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_USER\")\npwd &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_PASSWORD\")\n\n# Create auth token cache directory, otherwise it will prompt the user on the console for input\ncreate_AzureR_dir()\n\n# create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, \n                    username = user, \n                    password = pwd,\n                    auth_type=\"resource_owner\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nIn the case of authentication failures clearing cached authentication tokens/files can be done with:\nlibrary(AzureAuth)\nlibrary(AzureGraph)\n\ntenant = MyTenant\n\nAzureAuth::clean_token_directory()\nAzureGraph::delete_graph_login(tenant=\"mytenant\")\n\n\n\n\n\n\nThe authentication method used in this example could be swapped out for any of the examples shown above. The documentation on Microsoft365R contains extensive examples beyond what is included below.\nlibrary(Microsoft365R)\nlibrary(AzureGraph)\nlibrary(AzureAuth)\n\nsite_url = MySharepointSiteURL\ntenant = MyTenant\napp = MyApp\ndrive_name = MyDrive # For example by default this will likely be \"Documents\"\nfile_src = MyFileName.TheExtension\n\n# Add sensitive variables as environment variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory, otherwise it will prompt the the console for input\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a SharePoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n# An example using the SharePoint site to get to a specific drive\ndrv &lt;- site$get_drive(drive_name)\n\n# Download a specific file\ndrv$download_file(src = file_src, dest = \"tmp.csv\", overwrite = TRUE)\n\n# Retrieve lists of the different types of items in our sharepoint site. Documents uploaded under the 'Documents' drive are retrieved with list_files(). \ndrv$list_items()\ndrv$list_files() \ndrv$list_shared_files()\ndrv$list_shared_items()\n\n# Files can also be uploaded back to SharePoint\ndrv$upload_file(src = file_dest, dest = file_dest)\n\n\n\nMicrosoft resources can be used for hosting data in pins format using board_ms365() from pins. The authentication method used in this example could be swapped out for any of the examples shown above.\nlibrary(Microsoft365R)\nlibrary(pins)\n\nsite_url = MySite\napp=MyApp\n\n# Create a Microsoft Graph login\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n# An example getting the default drive \ndoclib &lt;- site$get_drive()\n\n# Connect ms365 as a pinned board. If this folder doesn't already exist it will be created on execution. \nboard &lt;- board_ms365(drive = doclib, \"general/project1/board\")\n\n# Write a dataset as a pin to Sharepoint\nboard %&gt;% pin_write(iris, \"iris\", description = \"This is a test\")\n\n# View the metadata of the pin we just created \nboard %&gt;% pin_meta(\"iris\")\n\n# Read the pin\ntest &lt;- board %&gt;% pin_read(\"iris\")\n\n\n\n\nThere are a few cases not covered in this article where the below resources may be useful:\n\nFor user level authentication into servers refer to the Marketplace offering and the Connect documentation.\nFor Python users the Microsoft REST API is the Microsoft developed method with examples.\nAs a last resort, mapping SharePoint, OneNote, or other systems as a network drive to the hosting server could be considered, using a program such as expandrive.\n\n\n\n\nOn the off chance that anyone makes it to the end this article got a chuckle out of me and may be relatable: https://www.theregister.com/2022/07/15/on_call/"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#introduction",
    "href": "work/sharepoint-oh-no.html#introduction",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "Microsoft 365 is a subscription extension of the Microsoft Office product line with cloud hosting support. Microsoft 365 uses Azure Active Directory (Azure AD) for user authentication and application access through developed APIs. The Microsoft supported method for interfacing with R developed content is with the Microsoft365R package which was developed by Hong Ooi and has extensive documentation. It supports access to Teams, SharePoint Online, Outlook, and OneDrive."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#summary",
    "href": "work/sharepoint-oh-no.html#summary",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": ":warning: Discussion between the developers and the Global Azure Administration team about the content and security requirements within your organization should determine which of the approaches should be supported.\n\nThere are four main authentication approaches supported by Microsoft365R. Note that multiple approaches can be supported at the same time.\n\n\n\n\n\n\n\n\n\nMethod\nauth_type\nPermissions\nCapability\n\n\n\n\nUser sign-in flow: Default\ndefault\nUser\nInteractive only (local IDE and Workbench, interactive Shiny content)\n\n\nUser sign-in flow: Device Code\ndevice_code\nUser\nInteractive only (local IDE and Workbench)\n\n\nService principal / Client secret\nclient_credentials\nApplication\nInteractive and non-interactive (same as above plus scheduled content)\n\n\nEmbedded credentials\nresource_owner\nUser\nInteractive and non-interactive (same as above plus scheduled content)\n\n\n\nAuthentication for Microsoft365R is through Microsoft’s Azure cloud platform through a registered application with appropriate assigned permissions in order to obtain ‘OAuth 2.0’ tokens.\n\n\nDepending on your organization’s security policy some steps may require support from your Azure Global Administrator.\nUser Sign-in Flow: Default\nA custom app can be created or the default app registration “d44a05d5-c6a5-4bbb-82d2-443123722380” that comes with the Microsoft365R package can be used. The user permissions will need to be enabled as detailed in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator. Additionally Redirect URLs will need to be added through Azure under App Registrations -&gt; select your app -&gt; Authentication -&gt; Platform configurations -&gt; Mobile and desktop applications -&gt;Add URI as well as also enabling nativeclient.\nFor adding Redirect URLs, which will give a typical web-app authentication experience for interactive applications:\n\nFor the desktop RStudio IDE the URL is: http://localhost:1410/.\nFor content hosted in shinyapps.io this would be of the form https://youraccount.shinyapps.io/appname (including the port number if specified).\nA SSL certificate will be required for non-local connections. This means that the Connect and Workbench URLs will need to be HTTPS. A wildcard could be used instead of adding the Redirect URL for each piece of content/user where appropriate for server-wide access.\n\nUser Sign-in Flow: Device Code\nIn addition to user level app permissions outlined above the device code workflow option will need to be enabled.\nEnabling the device code workflow is through the App Registration dashboard in Azure -&gt; click on the created app -&gt; Authentication -&gt; Allow public client flows and setting Enable the following mobile and desktop flows to yes. The device code workflow does not need Redirect URLs, instead providing a code and a link for the developer to access in a separate browser window (or even on a separate device) for sign-in.\nService Principal / Client Secret\nA custom app will need to be registered in Azure with Application permissions. The permissions can be based off of the user permissions but can be assigned as needed for the application and to comply with any security restrictions.\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. As a control using environmental variable’s for storing the client secret is recommended. Starting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI.\n\nEmbedded Credentials\nA custom app will need to be registered in Azure with User permissions as specified in the app registrations page. Depending on your organization’s security policy, access to your tenant may need to be granted by an Azure Global Administrator.\nThe credentials being embedded can be a user or a service account, as long as access to the desired content inside Microsoft 365 has been granted. Creating service accounts per content is recommended to enable faster troubleshooting and easier collaboration. As a control the Username / Password should never be exposed directly in the code, instead using Environment Variables. The variables are encrypted on-disk, and in-memory.\n\nThis can be done at the project level with securing deployment through the Connect UI."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#authentication-examples",
    "href": "work/sharepoint-oh-no.html#authentication-examples",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "The user sign-in flow option provides the typical web browser authentication experience. A user will need to be available to interact with the authentication pop-up in order to which makes this an option for interactive applications (such as the local RStudio IDE, Workbench, or an interactive Shiny app), but not applicable for scheduled content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app = app)\n\n\n\nIn some interactive cases it may be easier to use the device code flow where the user is prompted with a code and a link which is opened in a separate screen for logging in. For example for using a Workbench instance that was deployed without an SSL certificate. This does require interaction from the user and as such will not be applicable for scheduled content nor hosted content. The details are discussed in the auth vignette.\nlibrary(Microsoft365R)\n\nsite_url = MySharepointSiteURL\napp = MyApp\n\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette, with the Service Principal via using a Client Secret discussed in this section being the Microsoft recommended approach.\n\nApplication permissions are more powerful than user permissions so it is important to emphasize that exposing the client secret directly should be avoided. Instead the recommended approach is to store it as an Environment Variable which can be done through the Connect UI.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nContent in a non-interactive context (such as scheduled reports) won’t have a user account available for interactive authentication. There are several approaches outlined in the vignette. In cases where the additional access that comes with Application level permissions isn’t appropriate for the organization’s security requirements the embedded credentials approach can be used.\n\nThe credentials embedded will need to be granted access to the desired content and can either be a user or a service account. Working with your Azure Global Administrator to create service accounts per content is recommended to enable fast troubleshooting and easier collaboration.\nSensitive variables such username / password should be embedded as Environment Variables so that they aren’t exposed in the code directly.which can be done through the Connect UI. See the example here.\nUse of the Microsoft developed package AzureAuth may be needed for fully removing console prompt elements so a script can be run in a non-interactive context, for example by explicitly defining the token directory with AzureAuth::create_AzureR_dir().\n\nlibrary(AzureAuth)\nlibrary(AzureGraph)\nlibrary(Microsoft365R)\n\ntenant = MyTenant\nsite_url = MySharepointSiteURL\napp = MyApp\n\n# Add sensitive variables as environmental variables so they aren't exposed\nuser &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_USER\")\npwd &lt;- Sys.getenv(\"EXAMPLE_MS365R_SERVICE_PASSWORD\")\n\n# Create auth token cache directory, otherwise it will prompt the user on the console for input\ncreate_AzureR_dir()\n\n# create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, \n                    username = user, \n                    password = pwd,\n                    auth_type=\"resource_owner\")\n\n# An example of using the Graph login to connect to a Sharepoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n\n\nIn the case of authentication failures clearing cached authentication tokens/files can be done with:\nlibrary(AzureAuth)\nlibrary(AzureGraph)\n\ntenant = MyTenant\n\nAzureAuth::clean_token_directory()\nAzureGraph::delete_graph_login(tenant=\"mytenant\")"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#sharepoint-examples",
    "href": "work/sharepoint-oh-no.html#sharepoint-examples",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "The authentication method used in this example could be swapped out for any of the examples shown above. The documentation on Microsoft365R contains extensive examples beyond what is included below.\nlibrary(Microsoft365R)\nlibrary(AzureGraph)\nlibrary(AzureAuth)\n\nsite_url = MySharepointSiteURL\ntenant = MyTenant\napp = MyApp\ndrive_name = MyDrive # For example by default this will likely be \"Documents\"\nfile_src = MyFileName.TheExtension\n\n# Add sensitive variables as environment variables so they aren't exposed\nclient_secret &lt;- Sys.getenv(\"EXAMPLE_SHINY_CLIENT_SECRET\")\n\n# Create auth token cache directory, otherwise it will prompt the the console for input\ncreate_AzureR_dir()\n\n# Create a Microsoft Graph login\ngr &lt;- create_graph_login(tenant, app, password=client_secret, auth_type=\"client_credentials\")\n\n# An example of using the Graph login to connect to a SharePoint site\nsite &lt;- gr$get_sharepoint_site(site_url)\n\n# An example using the SharePoint site to get to a specific drive\ndrv &lt;- site$get_drive(drive_name)\n\n# Download a specific file\ndrv$download_file(src = file_src, dest = \"tmp.csv\", overwrite = TRUE)\n\n# Retrieve lists of the different types of items in our sharepoint site. Documents uploaded under the 'Documents' drive are retrieved with list_files(). \ndrv$list_items()\ndrv$list_files() \ndrv$list_shared_files()\ndrv$list_shared_items()\n\n# Files can also be uploaded back to SharePoint\ndrv$upload_file(src = file_dest, dest = file_dest)\n\n\n\nMicrosoft resources can be used for hosting data in pins format using board_ms365() from pins. The authentication method used in this example could be swapped out for any of the examples shown above.\nlibrary(Microsoft365R)\nlibrary(pins)\n\nsite_url = MySite\napp=MyApp\n\n# Create a Microsoft Graph login\nsite &lt;- get_sharepoint_site(site_url = site_url, app=app, auth_type=\"device_code\")\n\n# An example getting the default drive \ndoclib &lt;- site$get_drive()\n\n# Connect ms365 as a pinned board. If this folder doesn't already exist it will be created on execution. \nboard &lt;- board_ms365(drive = doclib, \"general/project1/board\")\n\n# Write a dataset as a pin to Sharepoint\nboard %&gt;% pin_write(iris, \"iris\", description = \"This is a test\")\n\n# View the metadata of the pin we just created \nboard %&gt;% pin_meta(\"iris\")\n\n# Read the pin\ntest &lt;- board %&gt;% pin_read(\"iris\")"
  },
  {
    "objectID": "work/sharepoint-oh-no.html#other-microsoft-related-resources",
    "href": "work/sharepoint-oh-no.html#other-microsoft-related-resources",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "There are a few cases not covered in this article where the below resources may be useful:\n\nFor user level authentication into servers refer to the Marketplace offering and the Connect documentation.\nFor Python users the Microsoft REST API is the Microsoft developed method with examples.\nAs a last resort, mapping SharePoint, OneNote, or other systems as a network drive to the hosting server could be considered, using a program such as expandrive."
  },
  {
    "objectID": "work/sharepoint-oh-no.html#end",
    "href": "work/sharepoint-oh-no.html#end",
    "title": "Connecting to resources in Microsoft 365 / Sharepoint",
    "section": "",
    "text": "On the off chance that anyone makes it to the end this article got a chuckle out of me and may be relatable: https://www.theregister.com/2022/07/15/on_call/"
  },
  {
    "objectID": "work/renv-environments.html",
    "href": "work/renv-environments.html",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "",
    "text": "#| echo: false\n#| include: false\n\nlibrary(renv)\nThis vignette is an overview of environment management in R and a comprehensive summary of the different options that can be configured to support different workflows. Environment management in R is intentionally complex, so figuring out where to even start when debugging can be a challenge. This vignette also goes into specific scenarios that might come up with environment management and recommendations."
  },
  {
    "objectID": "work/renv-environments.html#at-a-glance",
    "href": "work/renv-environments.html#at-a-glance",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "At a glance",
    "text": "At a glance\nOverview of the R environment:\n\n\n\n\n\ngraph LR\n    \n    subgraph ENV[Working R Environment]\n    \n    subgraph CONFIG[Config]\n    \n      subgraph LOCAL[Local R Config]\n      RENVIRON[.Renviron]\n      RPROFILE[.Rprofile]\n      end\n    \n      subgraph SERVER[Server R Config]\n      SRENVIRON[Renviron.site&lt;br/&gt;etc/R.home/Renviron.site]\n      SRRPROFILE[Rprofile.site&lt;/br&gt;etc/Rprofile.site]\n      \n        subgraph W[Posit Workbench]\n        REPOS[\"repos.conf\"]\n        RSESSION[\"rsession.conf\"] \n        end\n    \n      end\n      \n      LOCAL-- User settings &lt;br/&gt;override&lt;br/&gt;global settings --&gt; SERVER\n      \n      subgraph RENVCONFIG[Renv Config]\n      RENVPROJECT[Project Settings&lt;br/&gt;renv/settings.json]\n      \n        subgraph RENVUSER[Config: User Level Settings]\n        RENVUR[\"User Renviron&lt;br/&gt;~/.Renviron\"]\n        RENVRI[\"R installation&lt;br/&gt;etc/Rprofile.site\"]\n        RENVP[\"Project&lt;br/&gt;.Rprofile\"]\n        end\n      end      \n      \n    end\n    \n    subgraph LIBRARY[Package Library Path]\n\n      USERLIBRARY[\"User&lt;br/&gt;R_HOME/library&lt;br/&gt;~/R\"]\n\n      SITELIBRARY[Site&lt;br/&gt;R_HOME/site-library]\n      \n      subgraph RENV[Renv]\n      direction TB\n      CACHE[\"Cache&lt;br/&gt;~/.cache/R/renv/\"]\n      PROJECTCACHE[\"Project Cache&lt;br/&gt;~/renv/library/\"]\n      CACHE-- Unless isolated, symlink --&gt; PROJECTCACHE; \n      SHAREDCACHE[Cross-User Shared Cache]\n      end\n\n    end  \n    \n    LIBRARY --&gt; CONFIG\n    CONFIG --&gt; LIBRARY\n    \n    end\n    \n    subgraph REPOSITORY[Package Repository Source]\n      direction TB\n    \n      subgraph PPM[Posit Package Manager]\n      RE[Package Binaries]\n      RP[Package Sources]\n      end\n    \n      CRAN[CRAN/Pypi/BioConductor/etc]\n    \n      CRAN -- Posit sync service --&gt; PPM;\n\n    end\n    \n    UA[User-Agent request header]-- Binary requested&lt;br/&gt;Details: OS, R version --&gt;PPM\n    \n    UA --&gt; ENV"
  },
  {
    "objectID": "work/renv-environments.html#environment-management-strategies",
    "href": "work/renv-environments.html#environment-management-strategies",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Environment Management strategies",
    "text": "Environment Management strategies\nThere are severeal common environment management strategies. Some strategies can be more prone to pain and challenges later than others. Thinking about the appropriate strategy for your organization in advance can save you from a lot of hurt later.\n\n\n\nalt text\n\n\nImage: https://solutions.posit.co/envs-pkgs/environments/reproduce/reproducibility-strategies-and-danger-zones.png\n\n\n\nSnapshot and Restore\nShared Baseline\nValidated\n\n\n\n\nAll developers are responsible for their own environment management, and enabled for making their enviornments reproduceable through the use of renv’s snapshot() capability. Users can freely access and install packages while following a package-centric workflow. Users are responsible for recording their dependencies for their projects.\nAll developers in the organization are pointed to a snapshot of available packages frozen to a particular date when the managing team had intentionally tested and made them available. On some cadence, let’s say quarterly, the managing team goes through, performs testing again, and provides a new updated snapshot that is available for developers to switch to. There are a lot of advantages in switching with new features, resolved bugs, etc.\nSimilar to the shared baseline stratgey the difference is that changes to the package environment go through an approval and auditing process, and access to packages is strictly enforced."
  },
  {
    "objectID": "work/renv-environments.html#understanding-rs-startup-behavior",
    "href": "work/renv-environments.html#understanding-rs-startup-behavior",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Understanding R’s startup behavior",
    "text": "Understanding R’s startup behavior\nR has a lot of flexibility for different workflows, which is a great thing. However, it also means that the answer to trying to change specific pieces of that customized behavior can have complex answers that depend on example what has been implemented in your environment.\nThis diagram posted by Thomas Lin Pedersen on X showing the R startup flowchart went viral, and for good reason:\n\n\n\nR Startup diagram by Thomas Lin Pedersen on X\n\n\nPosit provides precompiled R binaries for anyone to use, free of charge. The public respository can be visited to understand how they are compiled."
  },
  {
    "objectID": "work/renv-environments.html#where-packages-come-from",
    "href": "work/renv-environments.html#where-packages-come-from",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Where packages come from",
    "text": "Where packages come from\nPackages can come from a couple places, a tarball, version control location, but most commonly is the URL of the repository that the package will be installed from. The package source can be set by assigning an environment variable with the desired location. More than one repository can be specified, for example with:\nrepos &lt;- c(CRAN = \"https://cloud.r-project.org\", WORK = \"https://work.example.org\")\noptions(repos = repos)\nSetting it this way would be a “one off” that would change the “package repository” for the current session. In order to persist the change of repository location, and other settings, various configurations can be applied.\nTypically “package repository”, among developers, is used to refer to R and Python package repositories (not to be confused with linux package repositories, etc). Most R and Python package managers serve only R and Python packages, and don’t handle additional management of system dependencies or packages, which would be risky in a shared server system where conflicts could come up.\nThe most famous R and Python package repositories are:\n\nCRAN - hosting public packages, checking, distributing, and archiving R packages for various platforms\nBioConductor - hosting public packages, checking, distributing, and archiving R packages for various platforms\nPyPi - hosting public packages, checking, distributing, and archiving Python packages for various platforms\n\nPosit Package Manager can be deployed within your organization, completely air-gapped, or with a sync service to Posit, to receive package sources and binaries.\n\nPosit Package Manager - hosting public packages, hosting internal packages, checking, distributing, blocking vulnerabilities, and archiving R and Python packages for various platforms"
  },
  {
    "objectID": "work/renv-environments.html#server-vs-individual-environments",
    "href": "work/renv-environments.html#server-vs-individual-environments",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Server vs individual environments",
    "text": "Server vs individual environments\nDevelopers can work locally on their local machines, in a cloud environment, or using a shared server environment (for example, by using Posit Workbench).\nHaving multiple developers working on a centralized server using Posit Workbench has a couple primary advantages:\n\nBetter IT oversight and security with encrypted traffic and restricted IP addresses\nAdditional configuration options and settings\nAuditing and logging\nLess time spent on software installation and management\nAccess to larger compute resources\nOptions for standardizing settings across all users\n\nWhen sharing a server environment users will sign in separately and work will live in separate user home directories. Workbench can act as an auth client to different data sources. However, the shared system dependencies will need to be carefully managed to support the different workflows that the users are doing."
  },
  {
    "objectID": "work/renv-environments.html#the-renv-package",
    "href": "work/renv-environments.html#the-renv-package",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "The renv package",
    "text": "The renv package\nRenv is an open source R package that allows users to better manage their package environments.\nEver had your code mysteriously stop working or start producing different results after upgrading packages, and had to spend hours debugging to find which package was the culprit? Ever tried to collaborate on code just to get stuck on trying to decipher various package dependencies?\nrenv helps you track and control package changes - making it easy to revert back if you need to. It works with your current methods of installing packages (install.packages()). It comes with a great degree of flexibility and supports a wide range of user workflows.\nRenv assumes:\n\nUsers are familiar with a version control system, like git\nUsers are following a project-centric methodology where the goal is to simultaneously work on different projects with different package environment needs\n\n\n\n\nThe Renv workflow\n\n\nThere is an excellent video by David Aja discussing why he started using renv at the 2022 RStudio Conference here: https://www.rstudio.com/conference/2022/talks/you-should-use-renv/\nUsefully, renv doesn’t have system requirements.\n\nThe lock file\nThe renv lock file is what is generated that allows the environment to be recreated on another system. It might look something like this:\n\n\nClick here to expand an example renv lock file\n\n{\n  \"R\": {\n    \"Version\": \"4.3.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://p3m.dev/cran/latest\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"MASS\": {\n      \"Package\": \"MASS\",\n      \"Version\": \"7.3-60\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Requirements\": [\n        \"R\",\n        \"grDevices\",\n        \"graphics\",\n        \"methods\",\n        \"stats\",\n        \"utils\"\n      ],\n      \"Hash\": \"a56a6365b3fa73293ea8d084be0d9bb0\"\n    },\n    \"Matrix\": {\n      \"Package\": \"Matrix\",\n      \"Version\": \"1.6-4\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Requirements\": [\n        \"R\",\n        \"grDevices\",\n        \"graphics\",\n        \"grid\",\n        \"lattice\",\n        \"methods\",\n        \"stats\",\n        \"utils\"\n      ],\n      \"Hash\": \"d9c655b30a2edc6bb2244c1d1e8d549d\"\n    },\n    \"yaml\": {\n      \"Package\": \"yaml\",\n      \"Version\": \"2.3.7\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"RSPM\",\n      \"Hash\": \"0d0056cc5383fbc240ccd0cb584bf436\"\n    }\n  }\n}\n\nIt’s in a json format. There are two main sections:\n\nHeader : This is where the R version is declared as well as package sources (if declared)\nPackages : This is where the specific package versions are specified, as well as various metadata\n\nFor an overview on package sources, see the Package Sources vignette.\nThe package source can be set for three different scenarios:\n\nRemoteType - packages installed by devtools, remotes, and pak\nRepository - packages installed from a package repository; CRAN, Posit Package Manager, etc\nbiocViews - packages installed from BioConductor repositories\n\nLet’s understand how the Repository is set. Notice how under each package the repository is declared like this:\nRepository: &lt;a name&gt;,\nThe Repository: &lt;a name&gt; field is used to denote the repository that the package was originally installed from. Most commonly it might like look:\n\nRepository: CRAN - This indicates that the package was installed from a repository call CRAN, likely a CRAN mirror\nRepository: RSPM - This indicates that the package was installed from Posit Package Manager, regardless of whether it was a binary or source package\n\nThere is a fail over order for determining the correct URL:\n\n\n\n\n\ngraph TD;\n    A(Assign repository URL) --&gt;lock; \n    \n    subgraph lock[renv.lock file]\n    B[Repository name in package definition]\n    c[Repository URL in header]\n    end\n    \n    lock -- Repository name in header --&gt;D;\n    D[Select matching URL] --&gt;END;\n    lock -- Repository name not in header --&gt;E;\n    \n    E{Check env for first repository listed &lt;br&gt; for required package version} -- package exists --&gt;F;\n    F[Select first repository URL] --&gt;END; \n    E -- package does not exist --&gt;G;\n\n    G{Check env for .. repository listed &lt;br&gt; for required package version} -- package exists --&gt;H;\n    H[Select .. repository URL] --&gt;END; \n    G -- package does not exist --&gt;I;\n    \n    I{Check env for last repository listed &lt;br&gt; for required package version} -- package exists --&gt;J;\n    J[Select last repository URL] --&gt;END; \n    I -- package does not exist --&gt;K;\n    \n    K{Check the cellar} -- package exists --&gt;L;\n    L[Select cellar] --&gt;END; \n    K -- package does not exist --&gt;M;    \n    \n    M[Package does not exist, unable to restore]\n    \n    END(End)\n\n\n\n\n\n\nIn words, for a package repository declaration of Repository: RSPM, if there happens to be a repository called RSPM in the repository list, then that repository will be preferred when restoring the package; otherwise, renv will check each repository from first to last for the required version of each package. The renv package cellar is meant to help with packages that aren’t available or accessible for installation. The cellar can be set to point at tarball locations for these tricky packages as an ultimate fail safe."
  },
  {
    "objectID": "work/renv-environments.html#the-pak-package",
    "href": "work/renv-environments.html#the-pak-package",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "The pak package",
    "text": "The pak package\nPak is a useful R package that can help with package installation and dependency look up.\nIf an error is encountered, we may need to enable the package pak to work with renv (or be patient and wait a couple minutes after installing pak). There is a useful git issue discussing this here.\nRenv can be told to use pak for package installation with: RENV_CONFIG_PAK_ENABLED = TRUE\nFor example temporarily with: Sys.setenv(\"RENV_CONFIG_PAK_ENABLED\" = TRUE))\nCheck that it set with: Sys.getenv('RENV_CONFIG_PAK_ENABLED')"
  },
  {
    "objectID": "work/renv-environments.html#package-installation",
    "href": "work/renv-environments.html#package-installation",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Package installation",
    "text": "Package installation\nPackages are installed into a package library, a directory that exists somewhere on disk.\nPackages are associated with that the OS, the particular version of R being used, and if using renv, with that particular project directory. The current library path(s) can be found with: .libPaths(). When packages are installed they will install to a sub folder that is specific to the combination of both of those.\n\nThe default library location\nThe default R installation will install packages into the users home directory, by default located at R_HOME/library. For example, on Windows:\n\\-- C:/Users/LisaAnders/AppData/Local/R\n    \\-- win-library\n        \\-- 4.3\n            \\-- ..packages\n\\-- C:/Program Files/R\n    \\-- R-4.3.1\n        \\-- library\n            \\-- ..packages\nLearn more about managing libraries in base R.\n\n\nShared site library location\nA shared site library can be set it up that will make packages from a global directory available to all users on the system, without the need for them to go through the installation steps. Through configuring Workbench, default repository locations can be set, an alternative directory can be set for use for package installation instead of user home directories, and user package installations can be disabled.\nA default site library can be used, at R_HOME/site-library (in this case /opt/R/3.4.4/lib/R/library), or a site library can be set up by setting .Library.site in R_HOME/etc/Rprofile.site / {$R_HOME}/etc/Rprofile.site. Multiple library locations can be set up to be used.\nWhen using a shared library, user options to change repository settings and package installation can be disabled if desired (typically as part of a validated environment management workflow). In this case, all users are accessing packages from that global site library and packages are added / updated by going through an approvals process with an admin ultimately running the commands that make the change.\nA site library can also be set up that allows users to access both the globally installed packages as well as install packages into the user directory. This is often “the best of both worlds”. New users are able to hit the ground running quickly, and advanced users have control over packages and package versions for their projects.\n\n\nRenv library location\nPackages installed with renv, depending on some configuration options, will use two locations:\n\nUser’s cache - ~/.cache/R/renv/\nProject cache - ~/renv/library/\n\nBy default, the project cache will symlink to the users cache in order to preserve space. Projects can be isolated in order to have the packages copied into the project library so that the project is completely independent of the broader renv cache.\nThe folder structure (note that it is specific to the possible OS’s, and the possible R versions and this is just an example) is:\n~/.cache/R/renv/\n+-- projects \n+-- index\n\\-- binary\n    \\-- linux-centos-7\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n        \\-- R-4.4\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n    \\-- linux-rocky-8.9\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n\\-- source\n    \\-- repository\n        \\-- ..packages\n~/renv/\n+-- activate.R\n+-- settings.json\n+-- staging\n\\-- library\n    \\-- linux-centos-7\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n        \\-- R-4.4\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n    \\-- linux-rocky-8.9\n        \\-- R-4.3\n            \\-- x86_64-pc-linux-gnu\n                \\-- repository\n                    \\-- ..packages\n\\-- source\n    \\-- repository\n        \\-- ..packages"
  },
  {
    "objectID": "work/renv-environments.html#local-r-config-files",
    "href": "work/renv-environments.html#local-r-config-files",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Local R config files",
    "text": "Local R config files\nThese two configuration files, that may or may not be set, are the moste common for changing the behavior as relates to setting the repository for package installations:\n\n.Renviron : The user R environ file contains all environment variables, often including renv settings, etc (typically located at ~/.Renviron)\n.Rprofile : The user R profile file contains various settings and configuration properties (typically located at ~/.Rprofile)\n\nThe easiest way to access either of this files is with the usethis package.\nlibrary(usethis)\nusethis::edit_r_environ() \nusethis::edit_r_profile()\nThese startup files can be disabled."
  },
  {
    "objectID": "work/renv-environments.html#shared-server-r-config-files",
    "href": "work/renv-environments.html#shared-server-r-config-files",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Shared server R config files",
    "text": "Shared server R config files\nInstead of setting individually with .Renviron and .Rprofile, the same parameters can be set at the server and R installation level. When set, any configuration will be active for any R sessions launched on that server.\n\nRprofile.site : The RProfile.site file is typically located at etc/Rprofile.site\nRenviron.site : The Renviron.site file is specific to the R installation, typically located at file.path(R.home(\"etc\"), \"Renviron.site\").\n\nFor example, this code can be used to maintain the repository configuration across R sessions by adding to the individual users .Rprofile file. It can be maintained across all users on the server by adding to the Rprofile.site file.\nlocal({\n  repos &lt;- c(PackageManager = \"https://packagemanager.posit.co/cran/__linux__/centos7/latest\")\n  repos[\"LocalPackages\"] &lt;- \"https://packagemanager.posit.co/local/__linux__/centos7/latest\"\n  # add the new repositories first, but keep the existing ones\n  options(repos = c(repos, getOption(\"repos\")))\n})\ngetOption(\"repos\")\nUsers can override the global settings in these files Rprofile.site and Renviron.site with their individual .Rprofile files."
  },
  {
    "objectID": "work/renv-environments.html#workbench-files-for-rstudio-pro-sessions",
    "href": "work/renv-environments.html#workbench-files-for-rstudio-pro-sessions",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Workbench files for RStudio Pro sessions",
    "text": "Workbench files for RStudio Pro sessions\nSimilarly, there are configuration files used in Workbench that can set repository preference for package installations:\n\n/etc/rstudio/repos.conf\n/etc/rstudio/rsession.conf\n\nWhen using a shared library, user options to change repository settings and package installation can be disabled if desired:\n# /etc/rstudio/rsession.conf\nallow-r-cran-repos-edit=0\nallow-package-installation=0"
  },
  {
    "objectID": "work/renv-environments.html#configuration-of-renv",
    "href": "work/renv-environments.html#configuration-of-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration of renv",
    "text": "Configuration of renv\nFor most users, renv’s default behavior is powerful and doesn’t need modification.\nHowever, the behavior can also be manually set / modified. Generally speaking though, relying on the defaults is the recommended happy path as renv is designed to just magically work. This does mean that troubleshooting when things go wrong can be tricky, see the troubleshooting section below for some tips on what to look out for.\nThere are also a number of environment variables that can be set that will also similarly effect the behavior as relates to setting the repositories being used as the source for package installation.\nCommonly, these settings are set in the .Renviron file to be set across all sessions for that user, or in the R installation’s Renviron.site file so it is active for all users on that server.\nSettings:\n\nRENV_PATHS_PREFIX : Used for sharing state across operating systems\nRENV_PATHS_CELLAR : Path to tarballs, used as a last ditch effort for installing tricky packages\nRENV_PATHS_CACHE : Path location for a cache shared across multiple users\nRENV_CACHE_USER : When using a shared cache, renv can re-assign ownershp of the cache’d package to a separate user account\nrenv.download.trace : Run options(renv.download.trace = TRUE) to temporarily have more verbose logging\n\nConfig settings:\n\nrenv.config.repos.override : Enforce the use of some repositories over what is defined in the renv.lock file\nrenv.config.ppm.enabled : Attempt to transform the repository URL in order to receive binaries on your behalf (defaults to TRUE)\nrenv.config.ppm.default : If repos have not already been set (for example, from the startup .Rprofile) then projects using renv will use the Posit Public Package Manager instance by default\nrenv.config.ppm.url : The URL for Posit Package Manager to be used for new renv projects\nrenv.config.user.environ : Load the users R environ file, usually encouraged (defaults to true)\nrenv.config.user.profile : Load the users R profile file, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.user.library : option to include the system library on the library paths for projects, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.external.libraries : Similar to renv.config.user.library, external libraries can be included with the project, usually discouraged since it can break project encapsulation (defaults to false)\nrenv.config.cache.enabled : Enable the global renv package cache, so that packages are installed into the global cache and then linked or copied into the users R library in order to save space (defaults to true)\nrenv.config.cache.symlinks : Use symlinks to reference packages installed into the global renv package cache (if set to FALSE packages are copied from the cache into your project library) (enabled by default, defaults to NULL)\nrenv.config.pak.enabled : Use pak with renv to install packages\n\nSince the configuration settings can be set in multiple places, the priority is given according to:\n\n\n\n\n\ngraph TD;\n    A(Renv configuration selection) --&gt;B;\n    B{R option &lt;br/&gt; renv.config.&lt;name&gt;} -- Not set --&gt;C;\n    B{R option &lt;br/&gt; renv.config.&lt;name&gt;} -- Set --&gt;F;\n    C{Environment variable &lt;br/&gt; RENV_CONFIG_&lt;NAME&gt;} -- Not set --&gt;D;\n    C{Environment variable &lt;br/&gt; RENV_CONFIG_&lt;NAME&gt;} -- Set --&gt;F;\n    D{Default} --&gt;F;\n    F(End)\n\n\n\n\n\n\nIf both the R option and the environment variable option are defined, the R option is preferred.\nWe can check the value of any of these parameters a couple ways:\n# Checking the renv options by reading environment variables and renv config properties\nrenv::paths$library()\nSys.getenv('RENV_PATHS_CACHE')\nSys.getenv('RENV_CACHE_USER')\nrenv::paths$cache()\n\n# Check the r_environ and r_profile contents using the usethis package\nlibrary(usethis)\nusethis::edit_r_environ() \nusethis::edit_r_profile()\n\nRenv and binary package OS and R version detection\nBy default, renv used with Package Manager will dynamically set the URL of your repository to pull package binaries for your respective system.\n\nStarting with R 4.4.0, renv automatically uses a platform prefix for library paths on linux (the equivalent to setting RENV_PATHS_PREFIX_AUTO = TRUE). This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required.\n\n\nSharing state across operating systems\nAs of renv 0.13.0, sharing state across operating systems is now possible. By default, it will construct a prefix based on fields within the system’s /etc/os-release file.\nalso possible to explicitly set with the RENV_PATHS_PREFIX environment variable. For example, it could be set like RENV_PATHS_PREFIX = \"ubuntu-bionic\" in order to programmatically generate a cache path like /mnt/shared/renv/cache/v2/ubuntu-bionic/R-3.5/x86_64-pc-linux-gnu. Alternatively the auto feature can be enabled with RENV_PATHS_PREFIX_AUTO = TRUE to automatically detect the environment and set the path.\nCommonly, this would be set in the .Renviron file to be set across all sessions for that user, or in the R installation’s Renviron.site file so it is active for all users on that server.\n\n\n\nRenv and binary package OS and R version detection\nRenv’s default behavior is powerful when using it with Posit Package Manager. It will automatically try to detect the details about your underlying system and set the corrrect URL path so that the appropriate binaries are downloading. If it is unable to find a binary, then it will fail over to the source URL."
  },
  {
    "objectID": "work/renv-environments.html#configuration-of-posit-package-manager",
    "href": "work/renv-environments.html#configuration-of-posit-package-manager",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration of Posit Package Manager",
    "text": "Configuration of Posit Package Manager\nPosit Package Manager is a hosting repository that can be deployed inside a companies network. It is often used in conjunction with vulnerability detection and package blocking for security. It is also useful for hosting internally developed packages that are meant to stay confidential and only used within that particular enterprise organization.\n\nConfiguring R Environments\nConfiguring Python Environments\n\nFor Workbench the URL for Package Manager is commonly configured so that it is at least used as the default repository for both R and Python packages from within the customers enterprise network.\nOptionally, the Posit Package Manager url can be configured to be specific to:\n\nSnapshot dates\nParticular curated repository/repositories\nParticular OS (in order to install binaries)\n\n\nPackage Manager and binary package OS and R version detection\nBinary packages are incredibly useful, enabling faster downloads by skipping the compilation step. When a binary package is requested (by using the __linux__ URL), Package Manager will make a best effort to serve the requested binary package. If that package is unavailable or unsupported on the user’s binary distribution Package Manager will fall back to serving the packages source version.\nPosit Package Manager has the option for the R user agent header can be configured. The user’s User-Agent request header indicates to Package manager which appropriate binary package to server, based on the R version and the OS. A diagnostic script is provided for generating a diagnostic to make sure this is set correctly. The diagnostic will fail to indicate that the OS and R version in the User-Agent request header needs to be updated.\n\n\nClick here to expand for the diagnostic script\n\n# User agent diagnostic script for Posit Package Manager binary packages\n\nlocal({\n  if (.Platform$OS.type != \"unix\" || Sys.info()[\"sysname\"] == \"Darwin\") {\n    message(\"Success! Posit Package Manager does not require additional configuration to install binary packages on macOS or Windows.\")\n    return(invisible())\n  }\n\n  dl_method &lt;- getOption(\"download.file.method\", \"\")\n  dl_extra_args &lt;- getOption(\"download.file.extra\", \"\")\n  user_agent &lt;- getOption(\"HTTPUserAgent\", \"\")\n\n  if (dl_method == \"\") {\n    dl_method &lt;- if (isTRUE(capabilities(\"libcurl\"))) \"libcurl\" else \"internal\"\n  }\n\n  default_ua &lt;- sprintf(\"R (%s)\", paste(getRversion(), R.version$platform, R.version$arch, R.version$os))\n\n  instruction_template &lt;- 'You must configure your HTTP user agent in R to install binary packages.\n\nIn your site-wide startup file (Rprofile.site) or user startup file (.Rprofile), add:\n\n# Set default user agent\n%s\n\n\nThen restart your R session and run this diagnostic script again.\n'\n\n  message(c(\n    sprintf(\"R installation path: %s\\n\", R.home()),\n    sprintf(\"R version: %s\\n\", R.version.string),\n    sprintf(\"OS version: %s\\n\", utils::sessionInfo()$running),\n    sprintf(\"HTTPUserAgent: %s\\n\", user_agent),\n    sprintf(\"Download method: %s\\n\", dl_method),\n    sprintf(\"Download extra args: %s\\n\", dl_extra_args),\n    \"\\n----------------------------\\n\"\n  ))\n\n  if (dl_method == \"libcurl\") {\n    if (!grepl(default_ua, user_agent, fixed = TRUE) ||\n        (getRversion() &gt;= \"3.6.0\" && substr(user_agent, 1, 3) == \"R (\")) {\n      config &lt;- 'options(HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])))'\n      message(sprintf(instruction_template, config))\n      return(invisible())\n    }\n  } else if (dl_method %in% c(\"curl\", \"wget\")) {\n    if (!grepl(sprintf(\"--header \\\"User-Agent: %s\\\"\", default_ua), dl_extra_args, fixed = TRUE)) {\n      ua_arg &lt;- \"sprintf(\\\"--header \\\\\\\"User-Agent: R (%s)\\\\\\\"\\\", paste(getRversion(), R.version[\\\"platform\\\"], R.version[\\\"arch\\\"], R.version[\\\"os\\\"]))\"\n      if (dl_extra_args == \"\") {\n        config &lt;- sprintf(\"options(download.file.extra = %s)\", ua_arg)\n      } else {\n        config &lt;- sprintf(\"options(download.file.extra = paste(%s, %s))\", shQuote(dl_extra_args), ua_arg)\n      }\n      message(sprintf(instruction_template, config))\n      return(invisible())\n    }\n  }\n\n  message(\"Success! Your user agent is correctly configured.\")\n})"
  },
  {
    "objectID": "work/renv-environments.html#configuration-on-workbench-for-r-repository-using-run.r-programmatically-setting-the-repository-location",
    "href": "work/renv-environments.html#configuration-on-workbench-for-r-repository-using-run.r-programmatically-setting-the-repository-location",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Configuration on Workbench for R repository using run.R / Programmatically setting the repository location",
    "text": "Configuration on Workbench for R repository using run.R / Programmatically setting the repository location\nInstead of the above, a run.R file can be used to programmatically set the repository and library location for users. This is commonly used in validated workflows, where the additional oversight is critical.\nExample created by Michael here."
  },
  {
    "objectID": "work/renv-environments.html#scenario-1-setting-up-a-shared-site-library-on-workbench",
    "href": "work/renv-environments.html#scenario-1-setting-up-a-shared-site-library-on-workbench",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 1: Setting up a shared site library on Workbench",
    "text": "Scenario 1: Setting up a shared site library on Workbench\nThe shared site library is specific to an installed version of R. For example for R version 4.3.2 installed to: /opt/R/4.3.2/lib/R/library:\n\nEdit the Rprofile.site file to set the repository URL\n\n# /opt/R/4.3.2/etc/Rprofile.site\nlocal({\n  options(repos = c(CRAN = \"https://r-pkgs.example.com/cran/128\"))\n})\n\n(optional) The default site library can be used, at R_HOME/site-library (in this case /opt/R/3.4.4/lib/R/library), or a site library can be set up by setting .Library.site in R_HOME/etc/Rprofile.site. Multiple library locations can be set up to be used.\nRun R as the root/admin account and install all desired packages\n\n# Multiple packages can be installed at the same time like this: \nexport R_VERSION=4.3.2\n\n/opt/R/${R_VERSION}/bin/R\n\nsudo /opt/R/${R_VERSION}/bin/Rscript -e 'install.packages(c(\"haven\",\"forcats\",\"readr\",\"lubridate\",\"shiny\", \"DBI\", \"odbc\", \"rvest\", \"plotly\",\"rmarkdown\", \"rsconnect\",\"pins\",\"png\",\"tidyverse\", \"Rcpp\"), repos = \"http://cran.us.r-project.org\")'\n\nq()\n\nUsers access packages on the system (without needing to install)\n\nWhen using a shared library, the ability for users to change repository settings and package installation can be disabled:\n# /etc/rstudio/rsession.conf\nallow-r-cran-repos-edit=0\nallow-package-installation=0"
  },
  {
    "objectID": "work/renv-environments.html#scenario-2-setting-up-a-project-to-use-renv",
    "href": "work/renv-environments.html#scenario-2-setting-up-a-project-to-use-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 2: Setting up a project to use renv",
    "text": "Scenario 2: Setting up a project to use renv\n# install renv\ninstall.package(\"renv\") \nlibrary(renv)\n\n# activate the project as an renv project\nrenv::activate()\n\n# generate the renv.lock file \nrenv::snapshot()\n\n# check the status of renv \nrenv::status()\n\n# On a separate system the snapshot can be used to install the specific packages and versions \nrenv::restore() \n\n# Restore a project with an explicit repository URL, note that this does not update the renv.lock file, it will need to be manually edited\nrenv::restore(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\"), rebuild=TRUE)\n\n# Add additional logging\noptions(renv.download.trace = TRUE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-3-determining-the-root-package-that-is-causing-a-failing-dependency",
    "href": "work/renv-environments.html#scenario-3-determining-the-root-package-that-is-causing-a-failing-dependency",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 3: Determining the root package that is causing a failing dependency",
    "text": "Scenario 3: Determining the root package that is causing a failing dependency\nFor example, error message:\n\n2024/05/17 9:24:10 AM: Error in dyn.load(file, DLLpath = DLLpath, …) : 2024/05/17 9:24:10 AM: unable to load shared object ‘/opt/rstudio-connect/mnt/app/packrat/lib/x86_64-pc-linux-gnu/4.3.2/magick/libs/magick.so’: 2024/05/17 9:24:10 AM: libMagick++-6.Q16.so.8: cannot open shared object file: No such file or directory 2024/05/17 9:24:10 AM: Calls: loadNamespace -&gt; library.dynam -&gt; dyn.load\n\nWe can look through our project repository and see that the magick package isn’t directly being called. So the question is, which package is calling it as dependency?\nThe easiest way to look up the dependency is to open the renv.lock file and find which package has it listed as a dependency.\nSome other tricks that might be useful are:\n\nWe can use renv to look at top level dependencies: renv::dependencies()\nWe can use base R to look up package dependencies: tools::package_dependencies(\"leaflet\", recursive = TRUE)[[1]]\nRenv can be told to use pak for package installation with: RENV_CONFIG_PAK_ENABLED = TRUE\nCheck that it set with: Sys.getenv('renv.config.pak.enabled')\nWe can use pak to look up all package dependencies in a tree format: pak::pkg_deps_tree(\"tibble\")\nWe can also get more details about the packages with: pak::pak_sitrep()\nIf an error is encountered, we may need to enable the package pak to work with renv (or be patient and wait a couple minutes after installing pak). There is a useful git issue discussing this here.\n\nWe can then clean up the project and remove packages that are installed, but no longer referenced in the project source, with renv::clean() and save that to the renv lock file with renv::snapshot(). Don’t forget to update your manifest.json file if this is a project being published to Connect with rsconnect::writeManifest()."
  },
  {
    "objectID": "work/renv-environments.html#scenario-4-upgrading-a-project-using-renv-from-r-4.1-to-r-4.4",
    "href": "work/renv-environments.html#scenario-4-upgrading-a-project-using-renv-from-r-4.1-to-r-4.4",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 4: Upgrading a project using renv from R 4.1 to R 4.4",
    "text": "Scenario 4: Upgrading a project using renv from R 4.1 to R 4.4\n\nWhy is this relevant? R CVE detection, vulnerability removed with R 4.4\n\nWhat is recommended: For each project, individually capture the requirements with renv. Change the R version and use the renv.lock file to install the captured requirements for the new R version. Perform tests, updating code and package versions as needed.\nWhat is not recommended: An in-place upgrading. Meaning, we do not recommend removing existing R versions and forcing all projects to use R 4.4. It is likely that code will break and will need developer work to make compatible with the new R version."
  },
  {
    "objectID": "work/renv-environments.html#scenario-5-os-migration-for-individual-r-projects-using-renv",
    "href": "work/renv-environments.html#scenario-5-os-migration-for-individual-r-projects-using-renv",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 5: OS migration for individual R projects using renv",
    "text": "Scenario 5: OS migration for individual R projects using renv\nRefer to here\nAll packages will need to be rebuilt.\nThese two locations in particular, the user home directories and global R or Python directories, will likely need to be flushed and rebuilt:\n\n~/R\n~/.local/lib/python3.*\n\nReference this script from David which programmatically reinstalls all packages installed into user home directories, or the global R or Python directories.\nRebuild renv:\n# Delete existing libraries\nunlink(\"renv/library\", recursive=TRUE)\n\n# Restart R session\n.rs.restartR()\n\n# Change anything that is needed, repository URL, etc\n\n# Re-install libraries\nrenv::restore(rebuild = TRUE)\nRebuild venv:\n# Activate existing venv\nsource .venv/bin/activate\n\n# Capture all installed packages\npython -m pip freeze &gt; requirements-freeze.txt\n\n# Deactivate and delete\ndeactivate\nrm -rf .venv/\n\n# Change anything that is needed, repository URL, etc\n\n# Create a new virtual environment\npython -m venv .venv\nsource .venv/bin/activate \npython -m pip install --upgrade pip wheel setuptools\npython -m pip install -r requirements-freeze.txt\nFor Connect, the content runtimes will need to be cleared and rebuilt. This can be done pre-emptively.\nDelete:\n# Enumerate the caches known to your server.\nrsconnect system caches list \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key\n\n# Validate cache targeted for deletion.\nrsconnect system caches delete \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --language Python \\\n    --version 3.9.5 \\\n    --dry-run\n\n# Delete one cache.\nrsconnect system caches delete \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --language Python \\\n    --version 3.9.5\nRebuild:\n# Enumerate every \"published\" content item and save its GUID.\nrsconnect content search \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key \\\n    --published | jq '.[].guid' &gt; guids.txt\n\n# Queue each GUID for build.\nxargs printf -- '-g %s\\n' &lt; guids.txt | xargs rsconnect content build add \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key\n\n# Build each queued content item.\nrsconnect content build run \\\n    --server https://connect.example.org:3939 \\\n    --api-key my-api-key"
  },
  {
    "objectID": "work/renv-environments.html#scenario-6-changing-the-project-repository-url",
    "href": "work/renv-environments.html#scenario-6-changing-the-project-repository-url",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 6: Changing the project repository URL",
    "text": "Scenario 6: Changing the project repository URL\nOften the package repository is set to a specific source URL. This can be due to it being within your network, or so that you are getting binaries for a specific OS version, etc.\nUsing the RENV_CONFIG_REPOS_OVERRIDE setting:\noptions('repos')\n\n# Set the override as a one off \nSys.setenv(\"RENV_CONFIG_REPOS_OVERRIDE\" = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \n\n# Check that it set \nSys.getenv(\"RENV_CONFIG_REPOS_OVERRIDE\")\n\n# Turn on debug logging so we can see more information about where packages are coming from and verify it's using the correct URL\noptions(renv.download.trace = TRUE)\n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE) \n\n#Override only applies during restore, and won't update the renv.lock file, so either manually update the renv.lock file with the appropriate URLor using renv::snapshot(repos = \"\")\nUsing the repos setting during rebuild:\n# Rebuild \nrenv::restore(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\"), rebuild=TRUE)\n\n# Snapshot s the URL change is reflected\nrenv::snapshot(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \nChanging it directly in the renv.lock file:\noptions('repos')\n\n# Either manually update the renv.lock file with the appropriate URL or using\nrenv::snapshot(repos = c(\"COLORADO\" = \"https://colorado.posit.co/rspm/all/latest\")) \n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE)"
  },
  {
    "objectID": "work/renv-environments.html#scenario-7-recovering-an-old-project-that-didnt-have-an-renv-and-isnt-working-with-latest-r-package-versions",
    "href": "work/renv-environments.html#scenario-7-recovering-an-old-project-that-didnt-have-an-renv-and-isnt-working-with-latest-r-package-versions",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 7: Recovering an old project that didn’t have an renv and isn’t working with latest R, package versions",
    "text": "Scenario 7: Recovering an old project that didn’t have an renv and isn’t working with latest R, package versions\nUse the snapshot date option with package manager to “guess” when the environment would have been built with renv so that package versions can be individually tweaked until the project works. Use the renv::revert feature with version control to update the packages with the ability to downgrade as needed."
  },
  {
    "objectID": "work/renv-environments.html#scenario-8-going-between-os-on-the-same-workbench-system-using-slurm-singularity-with-a-renv-project",
    "href": "work/renv-environments.html#scenario-8-going-between-os-on-the-same-workbench-system-using-slurm-singularity-with-a-renv-project",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Scenario 8: Going between OS on the same Workbench system using slurm / singularity with a renv project",
    "text": "Scenario 8: Going between OS on the same Workbench system using slurm / singularity with a renv project\nWith the interaction between renv and package manager, as well as the additions with recognition from renv when the OS and R version has changed, things should just work magically as long as the project is configured to use these pieces:\n\nrenv\npackage manager (binaries enabled)\n\nOn a system that has been configured to use slurm with singularity images (that are different OS’s) we can run these lines to get a feel for what is going on:\n# Turn on debug logging so we can see more information about where packages are coming from and verify it's using the correct URL\noptions(renv.download.trace = TRUE)\n\n# Check the default repository URL\noptions('repos')\n\n# Check the OS version\nsystem(\"cat /etc/os-release\")\n\n# Check the details of our singularity environment\nsystem(\"env | grep SINGULARITY\")\n\n# Check that auto-path prefix re-writing is set\nSys.getenv(\"RENV_PATHS_PREFIX_AUTO\")\n\n# We can attempt to set the URL to a specific binary, when we snapshot it will update the lock file to have the generic URL\nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/centos8/latest\")) \n\n# We can attempt to set the URL to a specific binary, when we snapshot it will update the lock file to have the generic URL\nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\")) \n\n# Update the renv to use a source URL as RSPM \nrenv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/latest\")) \n\n# We can also manually set the repo outside of renv this way, for example to successfully download renv\noptions(repos=c(CRAN=\"https://cran.r-project.org\"))\n\n# Rebuild the environment using that URL\nrenv::restore(rebuild=TRUE) \nInside the renv lock file we might see a couple different things:\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://packagemanager.posit.co/cran/__linux__/centos8/latest\"\n      },\nThis will cause problems and will tell renv to install the wrong version of packages for the wrong OS.\nIf we try to snapshot a binary repository URL with renv::snapshot(repos = c(\"RSPM\" = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\")) then we will see the renv.lock will be updated to:\n    \"Repositories\": [\n      {\n        \"Name\": \"RSPM\",\n        \"URL\": \"https://packagemanager.posit.co/cran/latest\"\n      }\nThis correction from the binary URL to the base URL will happen regardless of whether the OS matches the one we are using or not.\nWhen we install a package we will see that it is downloading the binary. This is the magic of RENV_PATHS_PREFIX_AUTO! This happens regardless of whether our package source is CRAN or RSPM.\nWe can test what the outputs are for each scenario:\n\nBefore a project has been initialized\nOnce a project has been initialized, with renv\nClosing the project and re-opening it with a different image (different OS) and restoring packages (‘renv::restore(rebuild=TRUE)’)\n\nThe auto-path prefix re-writing is really powerful. This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required. The caveats to know are:\n\nStarting with 4.4, renv automatically uses a platform prefix for library paths on linux.\nR versions below this may need to have the paths prefix set (for example for just the session with Sys.setenv(\"RENV_PATHS_PREFIX_AUTO\" = TRUE), though most likely this should be set at the user or global level).\n\nWe can set auto-path prefix re-writing at the user level by adding RENV_PATHS_PREFIX_AUTO = TRUE into the user r environ file:\nlibrary(usethis)\nusethis::edit_r_environ()"
  },
  {
    "objectID": "work/renv-environments.html#package-installation-errors-on-workbench",
    "href": "work/renv-environments.html#package-installation-errors-on-workbench",
    "title": "Debugging R Package Environments (renv): A long winded writeup",
    "section": "Package installation errors on Workbench",
    "text": "Package installation errors on Workbench\nHere’s an example error message that occurred during package installation inside Workbench (install.packages(askpass)):\n\n* installing binary package ‘askpass’ … cp: cannot open ‘./libs/askpass.so’ for reading: Operation not permitted /usr/bin/gtar: You may not specify more than one ‘-Acdtrux’, ‘–delete’ or ‘–test-label’ option Try ‘/usr/bin/gtar –help’ or ‘/usr/bin/gtar –usage’ for more information. /usr/bin/gtar: This does not look like a tar archive /usr/bin/gtar: Exiting with failure status due to previous errors\n\nA good first trouble shooting step is to SSH on the server and open an R session as root and attempt to install the same package. This helps to rule out where the issue is coming from, the global R configuration, the server, or a specific user issue or something with the Workbench configuration. Create a R session after SSH-ing into the server with /opt/R/${R_VERSION}/bin/R\n\nWhere to start\nGet the system information: Sys.info()\nGet session details: sessionInfo()\n\n\nProblems with pak\nGet details about pak (if used): pak::pak_sitrep()\nCheck if renv has been configured to use pak: Sys.getenv('renv.config.pak.enabled')\n\n\nProblems with renv : where to start\nCan they provide a renv diagnostic? It is generated by running this: renv::diagnostics().\n\n\nProblems with renv : cache location\nCheck the location of the renv cache:\nrenv::paths$library()\nSys.getenv('RENV_PATHS_CACHE')\noptions('renv.config.external.libraries')\noptions('renv.download.trace')\nrenv::paths$cache()\nSys.getenv('RENV_PATHS_PREFIX_AUTO')\nMake sure that it is located to a writeable location (if it is a mount, see the note about file mounts below, this could be a source of issues):\nsystem('namei -l /rsspdata/common/renv_cache/renv/v5/R-3.6/x86_64-pc-linux-gnu')\nCheck that the renv cache location matches the library locations: .libPaths()\nBy default packages are installed into the global cache at ~/.cache/R/renv/ and symlinked from the users cache within the project at ~/renv/library/.\nAre they using a shared renv cache, or an external library,\nDo they know if they’ve implemented settings in either of these, and could they share the contents?\n\nRprofile.site : The RProfile.site file is typically located at etc/Rprofile.site\nRenviron.site : The Renviron.site file is specific to the R installation (in this case I’m interested in if it exists for R 4.3 and R 3.6), typically located at file.path(R.home(\"etc\"), \"Renviron.site\").\nCheck if an external library is referenced in the environment: options('renv.config.external.libraries')\n\nIs the goal to use a shared renv cache location? There are a couple caveats with shared cache’s that can make them tricky. (1) cache permissions can be set with ACL’s, needing admin oversight to make sure are set correctly, (2) packages in the cache are owned by the requesting user, unless the RENV_CACHE_USER option is set. When set, renv will attempt to run chown -R &lt;package&gt; &lt;user&gt; to update cache ownership after the package has been copied into the cache.\nIf the desired behavior is to have a shared renv cache then these two settings will likely need to be added to the project .Renviron, user .Renviron, or site Renviron.site file:\n\nRENV_PATHS_CACHE : Path location for a cache shared across multiple users\nRENV_CACHE_USER : When using a shared cache, renv can re-assign ownership of the cache’d package to a separate user account\n\nI’d be curious, if it’s possible for them, to see if they are able to use R 4.4, or to set that parameter RENV_PATHS_PREFIX_AUTO to true (for example for just the session with Sys.setenv(\"RENV_PATHS_PREFIX_AUTO\" = TRUE)) using their current version of R, and repeat the steps of installing a package:\n\nStarting with R 4.4.0, renv automatically uses a platform prefix for library paths on linux (the equivalent to setting RENV_PATHS_PREFIX_AUTO = TRUE). This means that, for example, upgrading to a new version of an OS will automatically signal to renv that new library + cache directories will be required.\n\nOf course, they could also try this for installing the package, bypassing the cache, and see if it works (but I’m worried that there is a ghost setting somewhere that needs to be removed so that issues don’t keep popping up):\n# install a package, bypassing the cache\nrenv::install(\"&lt;package&gt;\", rebuild = TRUE)\n\n# restore packages from the lockfile, bypassing the cache\nrenv::restore(rebuild = TRUE)\n\n\nProblems with renv : other\nCheck:\n\nAre you running the latest renv? If not, upgrade\nAdd additional logging: options(renv.download.trace = TRUE)\nTake a diagnostic: renv::diagnostics()\n\nIf you are having particular issue with a package and it keeps being pulled in from the cache then doing a complete purge and reinstall can be useful:\nrenv::purge(\"stringr\")\nrenv::purge(\"stringi\")\ninstall.packages(\"stringr\")\nrenv::purge removes packages completely from the package cache (which may be shared across projects) rather than just removing the package from the project which is what renv::remove does. This can be useful if a package which had previously been installed in the cache has become corrupted or unusable, and needs to be re-installed.\nFollow these steps to “flush” and rebuild the renv environment, without losing the important parts of your renv.lock that are defining the R version and package versions:\nrenv::snapshot()\n# Make the appropriate changes (for example, changing OS) \n# Update the renv.lock file manually to reflect any needed changes (for example, changing the repository URL) \nrenv::deactivate()\nrenv::activate()\nrenv::restore(rebuild=TRUE) \nCheck that the packages either installed into the global cache at ~/.cache/R/renv/ or the users cache within the project at ~/renv/library/. The folder structure will give some clues for whether source, binaries were installed, and which OS and R version they were installed for if specified.\n\n\nProblems with packages not persisting\nIs this on a cloud vendor? IE sagemaker, google workstations, azureml? Check that the package repository location is being saved to the mounted drive. If it is saved to the general OS that is ephemeral it will be lost when the session is spun down. This also applies for things like git credentials.\n\n\nIncorrect / corrupted R installation\nCheck for an incorrect R installation for the OS, or a R installation that has gotten corrupted. An easy way to test this is to install a new R version, making sure to closely follow the instructions as well as verifying the OS version.\n\n\nIncorrect package repository source URL for the particular system OS\nWhen R installs a binary package, it doesn’t actually check if the package can be loaded after installation, which is different from source packages. So it is unfortunately possible to install a binary package only to find out later that it can’t actually be loaded.\nCheck the URL that the user is installing from: options('repos')\nTemporarily point the repository to global CRAN and check if the packages will successfully install. For example by running this: options(repos=c(CRAN=\"https://cran.r-project.org\")) and then installing any package with install.packages(\"ggplot2\")\nCheck in /etc/rstudio/rsession.conf if there is anything that would set the library location, for example r-libs-user=~/R/library.\nIt may also be useful to verify both the OS you are currently useing as well as checking that the repository you are pointing towards is using the correct OS if it is pulling in the binaries.\nFor debian/ubuntu distributions:\nlsb_release -a\nFor other distributions (more broadly cross-linux compatible command):\ncat /etc/os-release\n\n\nUsers lacking read/write permissions to their home directory\nCheck the home directory permissions on /home/username/. For example with namei -l /home/username/.\nIf useful, could try recursively chown-ing the directory with the user experiencing the issue and chmod 750 to make sure there is access.\nThis can commonly happen after a migration from one server to another, if the correct permissions weren’t correctly carried over. This is why we commonly recommend using rsync with the -a flag for transfer any files / directories. This syncs directories recursively and preserve symbolic links, groups, ownership, and permissions. Additionally, rsync needs to be used in root mode in order to completely move the various software and home directory components as it includes files with restrictive read and write permissions.\nFor example, the permissions should look something like: -rwx-r--r--\n\n\nUsers lacking permissions to ./libs\nCheck the permissions on ./libs/. For example with namei -l ./libs and ls -la ./libs\n\n\nIncorrect PAM configuration for users\nCheck the output of sudo getent passwd username\nFrom a workbench session the output of the environment, Sys.getenv() and compare between a Workbench session and logged into a R session as root on the server (after SSH-ing in)\nFrom an SSH session as root check the outputs of the user verification commands: sudo /usr/lib/rstudio-server/bin/pamtester --verbose &lt;session-profile&gt; &lt;user&gt; authenticate acct_mgmt setcred open_session\nFor example this command will likely look like: sudo /usr/lib/rstudio-server/bin/pamtester --verbose rstudio-session username authenticate acct_mgmt setcred open_session\nCheck for any umask or mask lines used during user provisioning, in the /etc/sssd/sssd.conf file\n\n\nServer hardening\nAnother thing to check is whether SELinux is enabled on the system. Check the mode with getenforce\nThis can result in user specific errors, in that case compare the SELinux context for a user that has successfully package installations to the one that is having errors.\nOften the following command will work to fix SELinux context issues: restorecon -Rv /home/users/username\nGreat article from our support team discussing how to use selinux\nDisable SELINUX (RHEL only): setenforce 0 && sudo sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config\nCheck for FIPS being enabled: fips-mode-setup --check\nThis article from redhat on FIPS mode is also very useful.\n\n\nMounted share drive\nCheck if /home on the server, or is it a network mount (NFS or CIFS). In NFS, for example, there can be the use of access control lists which can impact permissions. Similarly, when working in a system that has a mounted share drive then would want to check that libraries are being written to that share so you get persistence. Typically this means writing to inside the home directory. Check mounted drives with: df -h\nCheck /etc/fstab to see if the home directories are mounted with noexec\nFor example, this shows that the home directories were mounted with noexec: /dev/mapper/rhel-home  /home  xfs    defaults,noexec,nosuid,nodev   0 0\nThis resulted in this error message:\nlibrary(stringi)Error: package or namespace load failed for 'stringi' in dyn.load(file, DLLpath = DLLpath, ...):\nunable to load shared object '/home/c_jjones/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so':\n  /home/c_jjones/R/x86_64-pc-linux-gnu-library/4.3/stringi/libs/stringi.so: failed to map segment from shared object\n\n\nAzure cloud images\nThe default Azure RHEL images are unfortunately constricted in their ability to do some things.\n\n\nSlurm\nThe Slurm service account should have full privileges to the Slurm environment (like killing jobs).\nIn regards to not being able to run the diagnostics command, could you please provide the following:\n\nEnable debug logging by setting enable-debug-logging=1 in /etc/rstudio/launcher.slurm.conf\nTrigger the issue you are experiencing after restarting the launcher.\nResulting logs will be in: - /var/lib/rstudio-launcher/Slurm/rstudio-slurm-launcher.log\nThe Slurm version, which can be found by running sinfo –version\nThe installation location of Slurm on the host\nYour /etc/slurm.conf (or equivalent) configuration file\nThe output of running sinfo as the Slurm service user configured in /etc/rstudio/launcher.slurm.conf\nRun test job with srun date\nReplace  with a valid username of a user that is set up to run Posit - Workbench in your installation, in the commands below:\nsudo rstudio-server stop\nsudo rstudio-server verify-installation –verify-user=\nsudo rstudio-server start\nThe output of running sudo rstudio-launcher status"
  },
  {
    "objectID": "work/google-resources.html",
    "href": "work/google-resources.html",
    "title": "Access to resources in Google, an exploration",
    "section": "",
    "text": "Access to resources in google (bigquery, drive, etc) will depend on where the user is connecting from:\n\nLocal desktop: any method is fine\nWorkbench / server web app based: “OOB” workflows or non-interactive\nConnect / server web app non-interactive: Non-interactive only"
  },
  {
    "objectID": "work/google-resources.html#service-account-token",
    "href": "work/google-resources.html#service-account-token",
    "title": "Access to resources in Google, an exploration",
    "section": "Service account token",
    "text": "Service account token\nFollow these steps:\n\nCreate a service account\nFrom the GCP Console, in the target GCP Project, go to IAM & Admin &gt; Service accounts\nAssign permissions: googledrive docs does not have any explicit roles, service account used to test bigrquery has roles BigQuery Admin and Storage Admin\nAfter creating the service account, click on the three dots and “actions”, manage keys, add key as json, download credentials as json file\nThis key is a secret! Consider how it should be protected\nProvide path of json file to authentication\nGrant the service account permissions to resources as needed (it doesn’t inherit permissions) (For example, I had to visit https://console.developers.google.com/apis/api/drive.googleapis.com/overview?project=redacted to enable access for google drive and gogle sheets, which it gave me the link to in an error message for my specific project)\n\nReference: https://gargle.r-lib.org/articles/non-interactive-auth.html#provide-a-service-account-token-directly and https://gargle.r-lib.org/articles/get-api-credentials.html#service-account-token\n# use a service account token, like drive_auth(path = \"/path/to/your/service-account-token.json\")\n# drive_auth(path = Sys.getenv(\"google_drive_service_account\"), scopes = \"drive.readonly\")\n# drive_auth(path = Sys.getenv(\"google_drive_service_account\"), scopes = \"drive.readonly\")\ncredentials_service_account(\n  #scopes = \"https://www.googleapis.com/auth/userinfo.email\",\n  path = Sys.getenv(\"google_drive_service_account\")\n)\n# now use googledrive\ngoogledrive::drive_find(n_max = 5)"
  },
  {
    "objectID": "work/delta-lake-and-azure.html",
    "href": "work/delta-lake-and-azure.html",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "",
    "text": "This is some work I did exploring how to access the underlying databricks data storage, without having to go through databricks. Wanted to squirrel this aware so it’s easy to find in the future!"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#azure-data-lake",
    "href": "work/delta-lake-and-azure.html#azure-data-lake",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Azure Data Lake",
    "text": "Azure Data Lake\n\nSet up\nLanding page for Azure: &lt;https://portal.azure.com/ &gt;\nFollow this article: &lt;https://learn.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account &gt;\nThe trick: ADL isn’t it’s own separate category, it gets created as part of a storage account.\nSteps:\n\nGo to storage account\nCreate and give it a name\nSelect: LRS\nSwitch to premium: block blobs\nChange to hierarchical blob\nSet tags:\n\n\nrs:owner\nrs:project = soleng\nrs:environment = dev\n\n\nOnce in just need access keys or shared access signature in order to gain access\n\n\n\nAdd data\nYou can add data manually by creating a container and then using the upload icon.\n\n\n\nimage"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#authentication",
    "href": "work/delta-lake-and-azure.html#authentication",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Authentication",
    "text": "Authentication\nAccess your authentication details through the Access Keys or Shared Access Signature links on the left. I prefer Access Keys since they are easier to use.\nFor authentication from an R script we’ll be using https://github.com/Azure/AzureStor\nYou’ll need to know:\n\nThe Blob endpoint for your Azure Data Lake storage\nAn Access Key (this can also be done with a Shared Access Signature)\n\nlibrary(AzureStor)\n\nblob_endpoint &lt;- \"https://REDACTED.blob.core.windows.net/\"\n\nbl_endp_key &lt;- storage_endpoint(blob_endpoint, key=\"REDACTED\")\n\n# List containers and files in containers\nlist_storage_containers(bl_endp_key)\ncont1 &lt;- storage_container(bl_endp_key, \"container1\")\nlist_storage_files(cont1)\n\n# Download a file\nstorage_download(cont1, \"/crm_call_center_logs.parquet\")\n\n# Upload a file \nstorage_upload(cont1, \"crm_call_center_logs.parquet\", \"newdir/crm_call_center_logs.parquet\")\nYou can also create and delete containers:\n# Create a container\nnewcont &lt;- create_storage_container(bl_endp_key, \"container3\")\n\n# Create a directory in the container\ncont3 &lt;- storage_container(bl_endp_key, \"container3\")\ncreate_storage_dir(cont3, \"newdir\")\n\n# Delete a container\ndelete_storage_container(newcont)"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#reading-and-writing-delta-files",
    "href": "work/delta-lake-and-azure.html#reading-and-writing-delta-files",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Reading and Writing Delta Files",
    "text": "Reading and Writing Delta Files\nDelta files can be read by using the sparklyr package: https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_read_delta.html Thanks for the help with the magic incantation below!\nIn order to do this we will need to manage a Spark cluster. We can run it in local mode so that we aren’t connecting to an external cluster with billing:\nlibrary(sparklyr)\n\n#Install a local version of Spark\nspark_install(version=3.4)\n\n# Set Spark configuration to be able to read delta tables\nconf &lt;- spark_config()\nconf$`spark.sql.extensions` &lt;- \"io.delta.sql.DeltaSparkSessionExtension\"\nconf$`spark.sql.catalog.spark_catalog` &lt;- \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n\n# For spark 3.4 \nconf$sparklyr.defaultPackages &lt;- \"io.delta:delta-core_2.12:2.4.0\"\n\n# Open a connection\nsc &lt;- spark_connect(\"local\", version = 3.4, packages = \"delta\", conf = conf)\n\n# For this example we will use a built-in dataframe to save example data files, including one for delta tables\ntbl_mtcars &lt;- copy_to(sc, mtcars, \"spark_mtcars\")\n\n# Write spark dataframe to disk\nspark_write_csv(tbl_mtcars,  path = \"test_file_csv\", mode = \"overwrite\")\nspark_write_parquet(tbl_mtcars,  path = \"test_file_parquet\", mode = \"overwrite\")\nspark_write_delta(tbl_mtcars,  path = \"test_file_delta\", mode = \"overwrite\")\n\n# Read dataframes into normal memory\nspark_tbl_handle &lt;- spark_read_csv(sc, path = \"test_file_csv\")\nregular_df_csv &lt;- collect(spark_tbl_handle)\nspark_tbl_handle &lt;- spark_read_parquet(sc, path = \"test_file_parquet\")\nregular_df_parquet &lt;- collect(spark_tbl_handle)\nspark_tbl_handle &lt;- spark_read_delta(sc, path = \"test_file_delta\")\nregular_df_delta &lt;- collect(spark_tbl_handle)\n\n# Disconnect\nspark_disconnect(sc)\nYou should now have normal dataframes in your regular R environment that can be used for further analytics:\n\n\n\nimage\n\n\nNote: For Spark 3.5 you might have success with “io.delta:delta-core_2.12:3.0.0”"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#troubleshooting",
    "href": "work/delta-lake-and-azure.html#troubleshooting",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFrom R:\n# See spark details (troubleshooting)\nspark_config()\nspark_get_java()\nspark_available_versions()\nspark_installed_versions()\n\n# See session details\nutils::sessionInfo() \nFrom bash:\nnamei -l /usr/lib/spark\nRecommended troubleshooting: https://spark.rstudio.com/guides/troubleshooting.html"
  },
  {
    "objectID": "work/delta-lake-and-azure.html#about",
    "href": "work/delta-lake-and-azure.html#about",
    "title": "Accessing data in Azure Data Lake (delta files)",
    "section": "About",
    "text": "About\n\nAzure Data Lake: Azure Data Lake Storage Gen2 Introduction - Azure Storage\n\nAzure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob Storage.\n\n\nData Lake Storage Gen2 converges the capabilities of Azure Data Lake Storage Gen1 with Azure Blob Storage. For example, Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you’ll also get low-cost, tiered storage, with high availability/disaster recovery capabilities.\n\n\nA superset of POSIX permissions: The security model for Data Lake Gen2 supports ACL and POSIX permissions along with some extra granularity specific to Data Lake Storage Gen2. Settings may be configured through Storage Explorer or through frameworks like Hive and Spark.\n\nTLDR: Azure Data Lake is a place where data can be saved (similar to S3 buckets on Amazon).\n\n\nDelta Tables: https://docs.delta.io/latest/delta-intro.html\n\nDelta Lake is an open source project that enables building a Lakehouse architecture on top of data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\n\nYou can check Delta Lake releases here: https://docs.delta.io/latest/releases.html\nTLDR: Delta tables are a data file format, specifically used with Spark clusters (for example Databricks).\n\n\n\nimage\nimage"
  },
  {
    "objectID": "presentations_posit.html",
    "href": "presentations_posit.html",
    "title": "Lisa Anders - questionable.quarto",
    "section": "",
    "text": "Check out some of the presentations I’ve made while working at Posit:\n\n2024\n\n\n\n\n\n&gt;\n\n\nQuarto for Reproduceable Reporting\n\n\nQuarto is an amazing technology, this presentation goes over the basics of setting up quarto projects and touches on advanced topics like content types, version control, and other bells and whistles (presentation now with Confetti!)\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nIt’s dangerous to go alone - take this!\n\n\nLisa’s best practices for pain-free data science\n\n\n Check out the slides Check out the repo\n\n\n\n\n\n2023 and 2022\n\n\n\n\n\n&gt;\n\n\nReporting in R with Posit Team\n\n\nUsing the Posit Team set of products to create data science reports.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nImproving app performance\n\n\nAnalyzing your R scripts for performance improvements using profvis, shinyloadtest, and shinytest2.\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nReproducible Workflows\n\n\nCreating reproducible data science workflows.\n\n\n Check out the slides Check out the repo"
  },
  {
    "objectID": "lists.html",
    "href": "lists.html",
    "title": "Lists",
    "section": "",
    "text": "Have you heard of awesome lists? Probably some of the best compilations out there on there on the internet.\nWith that said here are some random compilations of my own.\n\n\n\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nCoding write-ups and resources\n\n\n1 min\n\n\n\n\nWorkplace articles\n\n\n1 min\n\n\n\n\nThe importance of good programming\n\n\n6 min\n\n\n\n\nSound\n\n\n4 min\n\n\n\n\nPhone games\n\n\n1 min\n\n\n\n\nA little data goes a long way\n\n\n1 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lists/sound.html",
    "href": "lists/sound.html",
    "title": "Sound",
    "section": "",
    "text": "Make beautiful music with technology!"
  },
  {
    "objectID": "lists/sound.html#otomata",
    "href": "lists/sound.html#otomata",
    "title": "Sound",
    "section": "Otomata",
    "text": "Otomata\nMake beautiful generative algorithm inspired music using cellular automation.\n\nLink to the app: https://marwahaha.github.io/otomata/\n\nSome historic links:\n\nOriginally at (but now no longer works): https://earslap.com/page/otomata.html\nFrom the reddit post: https://www.reddit.com/r/otomata/comments/lrd7n4/otomata_lives_again_web_version/\nGithub: https://github.com/marwahaha/otomata\n\n\n\n\nreddit post"
  },
  {
    "objectID": "lists/sound.html#sonic-pi",
    "href": "lists/sound.html#sonic-pi",
    "title": "Sound",
    "section": "Sonic Pi",
    "text": "Sonic Pi\nMake your own music using programming with Sonic Pi: https://sonic-pi.net/"
  },
  {
    "objectID": "lists/sound.html#sky-cotl",
    "href": "lists/sound.html#sky-cotl",
    "title": "Sound",
    "section": "Sky COTL",
    "text": "Sky COTL\nSky Children of Light is a phone game that has a built in music player that I really love. Instead of having to turn pages in sheet music it has floating icons that change shape over the notes you need to play, making it really easy to pick up once you get the hang of it.\n\nSky music library: https://sky-music.github.io/\nSky music app library: https://sky-music.herokuapp.com/songLibrary.html\nSky music app: https://sky-music.herokuapp.com/\n\nHere’s an example of one that I created (an attempt to translate “Regret” by Gackt):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView this post on Instagram\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA post shared by @lisa.needs.to.make"
  },
  {
    "objectID": "lists/good-programming.html",
    "href": "lists/good-programming.html",
    "title": "The importance of good programming",
    "section": "",
    "text": "A handful of articles that have stood out over the years as interesting case studies into the importance of good programming, and what that actually means."
  },
  {
    "objectID": "lists/good-programming.html#uk-post-office-scandal",
    "href": "lists/good-programming.html#uk-post-office-scandal",
    "title": "The importance of good programming",
    "section": "UK Post Office Scandal",
    "text": "UK Post Office Scandal\n\nDevelopers blamed for the post office scandal?\nVictim testimony\nRemote access and mistakes blamed on post masters, a smoking gun\nExample of the terrible quality of the code - were they paid by line of code submitted? Does this imply a fauly language conversion? Risks of overflow errors?\nOne of the independent investigators from 2012 breaks silence\nEvidence that back in 2012 the indepent investigators were told about remote access, implying pejury in all later cases\nThey Could, And They Did Change Branch Transaction Data\nMiscarriage of justice - the Rose report\nList of current issues / bugs as of 2017? with the software. One can only imagine how much worse it used to be\nProject manager on the original project discusses his impressions - how not to commission a complex project Inquiry Phase 2: Star Witness – Dave gives it both barrels\nKnown errors in the software, perjury, and lack of disclosure\nInterview with IT - throwing coconuts\nInterview with the other IT\nAttempt by the post to recuse the judge - right after a verdict was handed down. Sour grapes, anyone?\nThe cover up\nWhy did the lawyer for the post office act this way?\nThinking of alleging or pleading fraud: better read this first"
  },
  {
    "objectID": "lists/code.html",
    "href": "lists/code.html",
    "title": "Coding write-ups and resources",
    "section": "",
    "text": "Source for good articles\nThe true cost of the cloud\nThe case for professional services\nOpinionated article on Python installations\nThis page addresses poetry and conda\nObject oriented programming is the biggest mistake of all time\nGoogle drive is production\nParadox of choice\nWhy we’re leaving the cloud\nMoving beyond “algorithmic bias is a data problem”\nThat keynote talk “USENIX Security ’18-Q: Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?” : 4:08 is where he talks about ml\nHere’s What Ethical AI Really Means - YouTube philosophy tube\nPodcast on Stackoverflow Architecture\nData Feminism\nInvisible Women\nQuality Jam 2017: Michael Bolton “A Ridiculously Rapid Introduction to Rapid Software Testing”\nHome not so sweet home gist\n\nIncludes links to pages like Everything that uses configuration files should report where they’re located\n\nHelp linux ate my RAM\nFree images for including in presentations\nDatadog sales people are annoying\nCounting explosions at Unity, a data analysts perspective\nDatabase rebuild incident\nHow do lava lamps help with Internet encryption?\nFree email for testing things: mailtrap\nIf you haven’t done a Pandas data analysis project in awhile, it’s probably not a bad idea to watch this guy’s vids\nPassword purgatory\nEverything I learned about concurrency and reliability I learned at the Waffle House\nPositive affirmations for site reliability engineers\nvaporwave RStudio theme"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome!\nI’m a former product engineer with experience in biotech, semiconductor, and MEMS/BioMEMS manufacturing that stumbled into computational shenanigans in my journey of trying to make the most out of the data available to my team. I was fortunate enough to set up a RStudio / Posit Connect server at a previous company and from then on was hooked on helping folks set up infrastructure for doing good data science.\nIn my spare time I enjoy playing board games at the local game shop, building things, hot chocolate, and talking about anything SciFi.\nResume"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello there! You must be pretty awesome if you are checking out my humble website. Welcome!"
  },
  {
    "objectID": "lists/data.html",
    "href": "lists/data.html",
    "title": "A little data goes a long way",
    "section": "",
    "text": "A little data goes a long way… here are some examples that I think illustrte this in a particularly fascinating way (at least to me)."
  },
  {
    "objectID": "lists/data.html#fighting-the-gender-pay-gap-one-twitter-bot-at-a-time",
    "href": "lists/data.html#fighting-the-gender-pay-gap-one-twitter-bot-at-a-time",
    "title": "A little data goes a long way",
    "section": "Fighting the gender pay gap one twitter bot at a time",
    "text": "Fighting the gender pay gap one twitter bot at a time\nCompanies tweeted for International Women’s Day. Then this account called out their pay gaps.\nCheck out the bot for yourself."
  },
  {
    "objectID": "lists/data.html#for-better-science",
    "href": "lists/data.html#for-better-science",
    "title": "A little data goes a long way",
    "section": "For Better Science",
    "text": "For Better Science"
  },
  {
    "objectID": "lists/data.html#for-better-science-1",
    "href": "lists/data.html#for-better-science-1",
    "title": "A little data goes a long way",
    "section": "",
    "text": "The rabbit hole of learning about how bad science is uncovered and recalled started with:\n\nSylvain Lesné is a failed scientist\nCassava fraud and Alzheimer’s capitalism"
  },
  {
    "objectID": "lists/data.html#researchers-uncover-the-use-of-coal-ash-for-playground-materials-in-small-town",
    "href": "lists/data.html#researchers-uncover-the-use-of-coal-ash-for-playground-materials-in-small-town",
    "title": "A little data goes a long way",
    "section": "Researchers uncover the use of coal ash for playground materials in small town",
    "text": "Researchers uncover the use of coal ash for playground materials in small town\nA recent story about coal ash that was used in a playground and the work the local newspaper and the University (Duke) is doing to figure out the path forward:\n\nhttps://www.knoxnews.com/story/news/local/tennessee/tvacoalash/2020/05/17/duke-testing-shows-kingston-coal-ash-uranium-triple-report-levels/5035210002/\nhttps://www.knoxnews.com/story/news/crime/2021/08/09/claxton-playground-contaminated-radioactive-dust-still-open/5470284001/\nhttps://www.knoxnews.com/story/news/2017/07/21/kingston-coal-ash-spill-workers-treated-expendables-lawsuit-sick-and-dying-contends/451537001/"
  },
  {
    "objectID": "lists/data.html#unprecedented-access-to-food-safety-and-consumer-recalls",
    "href": "lists/data.html#unprecedented-access-to-food-safety-and-consumer-recalls",
    "title": "A little data goes a long way",
    "section": "Unprecedented access to food safety and consumer recalls",
    "text": "Unprecedented access to food safety and consumer recalls\n\nFood safety\nConsumer products"
  },
  {
    "objectID": "lists/phone_games.html",
    "href": "lists/phone_games.html",
    "title": "Phone games",
    "section": "",
    "text": "Seedship\n\n\nA fun (and free game) where you are an AI exploring space trying to find the new home for the 1000 remaining survivors of the human race stored onboard.\nLink: https://philome.la/johnayliff/seedship/play/index.html\n\n\n\nhttps://garticphone.com/\n\n\n\nhttps://www.urbandead.com/\n\n\n\nhttps://robotodyssey.online/\n\n\n\nhttps://skribbl.io/\n\n\n\nhttps://www.trickstercards.com/home/euchre/\n\n\n\nhttps://codenames.game/\n\n\n\nTrack your coverage gaps with: https://www.mkttoolbox.com/login/?ret_url=%2F\n\n\n\nhttps://semantle.com/\n\n\n\nhttps://knucklebones.io/en"
  },
  {
    "objectID": "lists/phone_games.html#random-assortment-of-games-that-i-particularly-enjoy-and-would-endorse",
    "href": "lists/phone_games.html#random-assortment-of-games-that-i-particularly-enjoy-and-would-endorse",
    "title": "Phone games",
    "section": "",
    "text": "Seedship\n\n\nA fun (and free game) where you are an AI exploring space trying to find the new home for the 1000 remaining survivors of the human race stored onboard.\nLink: https://philome.la/johnayliff/seedship/play/index.html\n\n\n\nhttps://garticphone.com/\n\n\n\nhttps://www.urbandead.com/\n\n\n\nhttps://robotodyssey.online/\n\n\n\nhttps://skribbl.io/\n\n\n\nhttps://www.trickstercards.com/home/euchre/\n\n\n\nhttps://codenames.game/\n\n\n\nTrack your coverage gaps with: https://www.mkttoolbox.com/login/?ret_url=%2F\n\n\n\nhttps://semantle.com/\n\n\n\nhttps://knucklebones.io/en"
  },
  {
    "objectID": "lists/workplace.html",
    "href": "lists/workplace.html",
    "title": "Workplace articles",
    "section": "",
    "text": "Yolo manifesto\nComfy software\nOpen source as the “new normal”\nOvercoming matrix madness\nMatrix organizations\nOvercoming matrix madness\nCall center burnout\nProduct led vs sales led organizations\nLAYER\nBeing glue\nDiscussion of cognitive load and how big of a barrier it can be\nCognitive load\nHow organizations are like slime molds\nForbes - let the engineers lead (boeing)\nFailure to Learn: the BP refinery disaster"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Check out some of the presentations I’ve made while working at Posit:\n\n2024\n\n\n\n\n\n&gt;\n\n\nQuarto for Reproduceable Reporting\n\n\nQuarto is an amazing technology, this presentation goes over the basics of setting up quarto projects and touches on advanced topics like content types, version control, and other bells and whistles (presentation now with Confetti!)\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nIt’s dangerous to go alone - take this!\n\n\nLisa’s best practices for pain-free data science\n\n\n Check out the slides Check out the repo\n\n\n\n\n\n2023 and 2022\n\n\n\n\n\n&gt;\n\n\nReporting in R with Posit Team\n\n\nUsing the Posit Team set of products to create data science reports.\n\n\nCheck out the slides\n\n\n\n\n&gt;\n\n\nImproving app performance\n\n\nAnalyzing your R scripts for performance improvements using profvis, shinyloadtest, and shinytest2.\n\n\n Check out the slides Check out the repo\n\n\n\n\n&gt;\n\n\nReproducible Workflows\n\n\nCreating reproducible data science workflows.\n\n\n Check out the slides Check out the repo"
  },
  {
    "objectID": "site_map.html",
    "href": "site_map.html",
    "title": "Site Map",
    "section": "",
    "text": "Site Map Listing\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nAug 2, 2022\n\n\nA little data goes a long way\n\n\n1 min\n\n\n\n\n\n\n\nAug 4, 2022\n\n\nPhone games\n\n\n1 min\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSound\n\n\n4 min\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nPublish and version your models with Vetiver\n\n\n3 min\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nConnecting to resources in Microsoft 365 / Sharepoint\n\n\n10 min\n\n\n\n\n\n\n\nAug 9, 2023\n\n\nSecuring credentials\n\n\n5 min\n\n\n\n\n\n\n\nAug 29, 2023\n\n\nProblems with persistence when in the cloud\n\n\n3 min\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nParallelization in R\n\n\n7 min\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nAccessing data in Azure Data Lake (delta files)\n\n\n4 min\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nThe importance of good programming\n\n\n6 min\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nWorkplace articles\n\n\n1 min\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nCoding write-ups and resources\n\n\n1 min\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nDebugging R Package Environments (renv): A long winded writeup\n\n\n48 min\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nAccess to resources in Google, an exploration\n\n\n6 min\n\n\n\n\n\n\n\n \n\n\n \n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nTechnical writeups and musings\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nPresentations\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nLists\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nHome\n\n\n1 min\n\n\n\n\n\n\n\n \n\n\nAbout\n\n\n1 min\n\n\n\n\n\nNo matching items\n\n\nRefer to: https://quarto.org/docs/websites/website-listings.html"
  },
  {
    "objectID": "work/git-and-sagemaker.html",
    "href": "work/git-and-sagemaker.html",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "",
    "text": "This is a random trick that took me longer than I care to admit to figure out - and wanted to squirrel it away so it’s easy to find in the future!"
  },
  {
    "objectID": "work/git-and-sagemaker.html#problem-when-on-linux",
    "href": "work/git-and-sagemaker.html#problem-when-on-linux",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Problem when on Linux:",
    "text": "Problem when on Linux:\n\nIn general gitcreds doesn’t work well on linux (which has led to this git issue (Ship our own credential helper on Linux · Issue #47 · r-lib/gitcreds ). There is an excellent blog post that is very useful that goes deeper into what is going on: Notes from a data witch - Managing GitHub credentials from R, difficulty level linux"
  },
  {
    "objectID": "work/git-and-sagemaker.html#problem-when-on-sagemaker",
    "href": "work/git-and-sagemaker.html#problem-when-on-sagemaker",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Problem when on Sagemaker:",
    "text": "Problem when on Sagemaker:\n\nAdditionally, on Sagemaker things like credentials will be by default stored to the ephemeral EC2 instance and lost when the session is closed. A different method needs to be pursued in order for the token to persist."
  },
  {
    "objectID": "work/git-and-sagemaker.html#tldr-solution",
    "href": "work/git-and-sagemaker.html#tldr-solution",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "TLDR Solution:",
    "text": "TLDR Solution:\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'"
  },
  {
    "objectID": "work/git-and-sagemaker.html#testing",
    "href": "work/git-and-sagemaker.html#testing",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Testing",
    "text": "Testing\nI’ll add a disclaimer that it is similar to the .Renviron approach where the credentials would be stored as plain text, however to a location chosen by the user.\nLoad libraries:\nlibrary(usethis) \nlibrary(gitcreds) \nlibrary(gh) \nlibrary(credentials)\nConfigure the global git to cache instead of store the credentials to a local file (from bash/terminal):\ngit config --global credential.helper 'store --file ~/.my-credentials'\nFrom the documentation:\n\nThe “store” mode saves the credentials to a plain-text file on disk, and they never expire. This means that until you change your password for the Git host, you won’t ever have to type in your credentials again. The downside of this approach is that your passwords are stored in cleartext in a plain file in your home directory. The other options involve needing to change the root container to include alternative git credential helpers (libsecret or gcm core) which as far as I can tell are not currently available and would be something I’d recommend reaching out to Amazon about as they control that image.\n\nGenerate the PAT:\nusethis::create_github_token()\nCopy the generated PAT to your clipboard. Provide the PAT to this function when asked for it:\ngitcreds::gitcreds_set()\nCheck that it stored with:\ngitcreds_get()"
  },
  {
    "objectID": "work/git-and-sagemaker.html#alternatives",
    "href": "work/git-and-sagemaker.html#alternatives",
    "title": "Problems with git credential persistence when in the cloud",
    "section": "Alternatives",
    "text": "Alternatives\nThe old way “store a PAT as the GITHUB_PAT environment variable in .Renviron.” is typically what is recommended as being more compatible with linux if you are able to switch back to it, but it can present a security issue. We’ve also commonly seen folks using the gh package for generating PATs like in Managing Personal Access Tokens\nAlternatively, there are some git config options from the terminal. See: Chapter 9 Personal access token for HTTPS | Happy Git and GitHub for the useR"
  },
  {
    "objectID": "work/parallelization.html",
    "href": "work/parallelization.html",
    "title": "Parallelization in R",
    "section": "",
    "text": "https://colorado.posit.co/rsc/parallel_thinking/Parallel_Thinking.html#/title-slide\nhttps://edavidaja.github.io/parallelooza/#/parallelooza\nhttps://github.com/edavidaja/parallelooza\nShiny apps on Connect that feature long running jobs https://wlandau.github.io/crew/articles/shiny.html\nworkbench jobs and AWS batch: https://github.com/wlandau/crew.aws.batch"
  },
  {
    "objectID": "work/parallelization.html#profvis",
    "href": "work/parallelization.html#profvis",
    "title": "Parallelization in R",
    "section": "profvis",
    "text": "profvis\nlibrary(profvis)\nlibrary(ggplot2)\nlibrary(shiny)\n#library(deSolve)\n\n# Simple example\nprofvis({\n  data(diamonds, package = \"ggplot2\")\n  \n  plot(price ~ carat, data = diamonds)\n  m &lt;- lm(price ~ carat, data = diamonds)\n  abline(m, col = \"red\")\n})\nMore complex example:\nprofvis({\n  # generate a dataset\n  data(diamonds, package = \"ggplot2\")\n  \n  # save it \n  write.csv(diamonds, \"diamonds.csv\")\n  \n  # load it\n  csv_diamonds &lt;- read.csv(\"diamonds.csv\")\n  \n  # summarize\n  summary(diamonds)\n  \n  # plot it  \n  plot(price ~ carat, data = csv_diamonds)\n  m &lt;- lm(price ~ carat, data = csv_diamonds)\n  abline(m, col = \"red\")\n  \n  #create histogram of values for price\n  ggplot(data=csv_diamonds, aes(x=price)) +\n    geom_histogram(fill=\"steelblue\", color=\"black\") +\n    ggtitle(\"Histogram of Price Values\")\n  \n  #create scatterplot of carat vs. price, using cut as color variable\n  ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + \n    geom_point()\n  \n  #create scatterplot of carat vs. price, using cut as color variable\n  ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + \n    geom_point()\n  \n  # Examples from: https://www.statology.org/diamonds-dataset-r/#:~:text=The%20diamonds%20dataset%20is%20a,the%20diamonds%20dataset%20in%20R. \n})\nShiny app example:\n\n#profvis({runApp()})\n\n#profvis({runApp(appDir = \"./test_profvis/\")})\n\nprofvis({runApp(appDir = \".\")})"
  },
  {
    "objectID": "work/parallelization.html#other-packages",
    "href": "work/parallelization.html#other-packages",
    "title": "Parallelization in R",
    "section": "Other packages",
    "text": "Other packages\n\nbench\nmicrobenchmark"
  },
  {
    "objectID": "work/parallelization.html#taking-advantage-of-native-parallelization",
    "href": "work/parallelization.html#taking-advantage-of-native-parallelization",
    "title": "Parallelization in R",
    "section": "Taking advantage of native parallelization",
    "text": "Taking advantage of native parallelization\nUse packages like data.table that implement parallelism natively\nlibrary(data.table)\n\ngetDTthreads()\nDT = as.data.table(iris)\nDT[Petal.Width &gt; 1.0, mean(Petal.Length), by = Species]"
  },
  {
    "objectID": "work/parallelization.html#explicitly-programming-parallelization",
    "href": "work/parallelization.html#explicitly-programming-parallelization",
    "title": "Parallelization in R",
    "section": "Explicitly programming parallelization",
    "text": "Explicitly programming parallelization\nRead more: https://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745\n\nfutureverse\nRead more: https://www.futureverse.org/\nlibrary(future)\nplan(multisession)\n\n## Evaluate an R expression sequentially\ny &lt;- slow_fcn(X[1])\n\n## Evaluate it in parallel in the background\nf &lt;- future(slow_fcn(X[1]))\ny &lt;- value(f)\n\n## future.apply: futurized version of base R apply\nlibrary(future.apply)\ny &lt;-        lapply(X, slow_fcn)\ny &lt;- future_lapply(X, slow_fcn)\n\n## furrr: futurized version of purrr\nlibrary(furrr)\ny &lt;- X |&gt;        map(slow_fcn)\ny &lt;- X |&gt; future_map(slow_fcn)\n\n## foreach: futurized version (modern)\nlibrary(foreach)\ny &lt;- foreach(x = X) %do%       slow_fcn(x)\ny &lt;- foreach(x = X) %dofuture% slow_fcn(x)\n\n## foreach: futurized version (traditional)\nlibrary(foreach)\ndoFuture::registerDoFuture()\ny &lt;- foreach(x = X) %do%    slow_fcn(x)\ny &lt;- foreach(x = X) %dopar% slow_fcn(x)\n\n\nparallel\nExample from: https://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745\nlibrary(parallel)\n\n# Generate data\ndata &lt;- 1:1e9\ndata_list &lt;- list(\"1\" = data,\n                  \"2\" = data,\n                  \"3\" = data,\n                  \"4\" = data)\n\n# Single core\ntime = Sys.time()\n\ntime_benchmark &lt;- system.time(\n  lapply(data_list, mean)\n)\nsingle_core_time = difftime(Sys.time(), time)\n\n\n# Detect the number of available cores and create cluster\ntime = Sys.time()\n\ncores_avail= detectCores()\n\ncl &lt;- parallel::makeCluster(detectCores())\n# Run parallel computation\ntime_parallel &lt;- system.time(\n  parallel::parLapply(cl,\n                      data_list,\n                      mean)\n)\n\nmultiple_core_time = difftime(Sys.time(), time)\n\n# Close cluster\nparallel::stopCluster(cl)\n\n\nprint(single_core_time)\nprint(multiple_core_time)\nprint(cores_avail)\nRunning sequentially took 18.33 seconds. Running in parallel shortened that to 4.99 seconds.\n\n\nparallely (part of futureverse)\nRead more: https://parallelly.futureverse.org/\nlibrary(parallelly)\nparallelly::availableCores()\n\n\nforeach and futureverse\nlibrary(foreach)\nlibrary(doFuture)\n\nyears = 2024\n\nplan(multisession, workers = 20)\nresults &lt;- foreach(i=years, \n                   .combine = rbind) %dofuture% {\n                     get_api_stats(yr=i, tmt=tmt, product = \"litter\")}\n\n\nfuture.apply\nBefore:\ntable(okay2 &lt;- apply(tab2, 1, function(x) {...\nAfter:\nlibrary(future.apply)\nplan(multisession)\n\ntable(okay2 &lt;- future_apply(tab2, 1, function(x) {...\n\n\npurrr\nBefore:\nlibrary(tidyverse)\nsales_data_tbl %&gt;%\n  nest(data=c(date, value)) %&gt;%\n  mutate(model = purrr::map(data, function(df) {\n    lm(value ~month(date) + as.numeric(date) data=df)\n  })) \nAfter:\nlibrary(tidyverse)\nlibrary(purrr)\nsales_data_tbl %&gt;%\n  nest(data=c(date, value)) %&gt;%\n  mutate(model = furrr::map(data, function(df) {\n    lm(value ~month(date) + as.numeric(date) data=df)\n  })) \n\n\nforeach and doParallel\nlibrary(foreach)\nlibrary(doParallel)\n\ncl &lt;- makeCluster(20)\nregisterDoParallel(cl)\nresults &lt;- foreach(i=years, \n                   .combine = rbind) %dopar% {\n                     get_api_stats(yr=i, tmt=tmt, product = \"litter\")}\nstopCluster(cl)\n\n\nclustermq\nLifesaver, when working in a HPC environment.\nlibrary(foreach)\nlibrary(clustermq)\n\nn_cores &lt;- parallel::detectCores() - 1\noptions(clustermq.scheduler=\"multicore\")\ngetOption(\"clustermq.scheduler\")\n\nregister_dopar_cmq(n_jobs = n_cores)\nforeach(i = seq_len(n_cores)) %dopar% sqrt(i)\nThe future of clustermq: crew/mirai/nanonext"
  },
  {
    "objectID": "work/parallelization.html#other-tips-and-tricks",
    "href": "work/parallelization.html#other-tips-and-tricks",
    "title": "Parallelization in R",
    "section": "Other tips and tricks",
    "text": "Other tips and tricks\nit seems they are testing quite a few models against their data that are indeed hard to parallelize. Couple of zero order recommendations\nonly select the data they really need from ess_multilevel\nparallelize the individual models via something like https://stackoverflow.com/a/44010698/20032741, suggested by the author of future himself.\nremove the ess_multilevel data structure right after the select statement, not when the user is running out of memory\nAdditionally, for real benchmarking of functions I recommend against Sys.time() in favour of packages like tictoc and microbenchmark , the latter especially if the time for each function call is short and you want to get rid of the so-called OS jitter (influence from OS level processes)\nLast but not least, I also would point out the existence of https://luwidmer.github.io/fastR-website/materials.html where an ex-colleague of Novartis and me are talking in great length about debugging, performance optimisations and eventually about parallelization in R. This material is good for a 3.5 hour workshop.\nAlso use setwd() with caution. Read a really good review of when to use setwd() and when not to here: https://rstats.wtf/projects#setwd"
  },
  {
    "objectID": "work/parallelization.html#but-what-about-python",
    "href": "work/parallelization.html#but-what-about-python",
    "title": "Parallelization in R",
    "section": "But what about Python?",
    "text": "But what about Python?\n\ndask-jobqueue\nray"
  },
  {
    "objectID": "work/securing-credentials.html",
    "href": "work/securing-credentials.html",
    "title": "Securing credentials",
    "section": "",
    "text": "When working with pulling data from secure databases or other sources a developer might find themselves in a situation of needing to provide very sensitive information, such as a password or a token, in order to access the data that is needed or to successfully deploy a project. Handling those secrets in way that doesn’t expose them in the code directly is critical and where using environmental variable’s for securing sensitive variables is strongly recommended.\nAdditionally there may be parameters that are often needed that can be accessed as a variable more easily rather than having to type in every time.\nFor both of these cases knowing how environment variables can be leveraged can be very rewarding and it is surprising how little effort it can to take to set up.\n\n\nWhen R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files:\n\nUser\nProject\n\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\n\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\n\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\n\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\n\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\n\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\n\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI.\nUse Connect to manage environment variables through the UI: https://docs.posit.co/connect/user/content-settings/index.html#content-vars\nHave your admin use a supervisor script to add Environment Variables automatically: https://docs.posit.co/connect/admin/process-management/index.html#example-environment-variables\nSecuring credentials solutions article: https://solutions.posit.co/connections/db/best-practices/managing-credentials/"
  },
  {
    "objectID": "work/securing-credentials.html#working-with-the-.renviron-file",
    "href": "work/securing-credentials.html#working-with-the-.renviron-file",
    "title": "Securing credentials",
    "section": "",
    "text": "When R starts it loads a bunch of variables, settings, and configs for the user. This is really powerful and some of the magic for how it can work so apparently seamlessly.\nHowever for power users we can leverage these behind the scenes config files so that we can include such things as variables in our project without including it in our code. The .Renviron file is the one most commonly interacted with for adding sensitive variables to a project in order to protect them from being exposed in the code.\nWith increased use of these behind the scenes config files and the growing direction of arranging code into projects there was the development of giving, on startup, having multiple options for each config file that can be loaded depending on what the user specifies. Broadly speaking there are two levels of config files:\n\nUser\nProject\n\nOn startup, since R is trying to make things as seamless as possible for the user, it will use some logic to figure out which config to use. It will assume that if a project level config exists it should load that one (and not any others). If that project level config doesn’t exist, then it will default to the user level config. For more details on the different config files and the nuances see Managing R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf.\nJust to re-iterate the key takeaway: When in doubt note that the project level file is given preference over user level config files. Only if the project level config file doesn’t exist will the user level file be sourced/pulled in.\nThere is a really excellent overview of R’s startup process here but in short it can be thought of this way:\n\n\nusethis has a function for creating and editing the .Renviron file\nlibrary(usethis)\nusethis::edit_r_environ()\nAdd the variables to that file in the format variable_name = \"variable_value\" and save it. Restart the session so the new environment variables will be loaded with ctrl shift f10 or through the RStudio IDE\nSaved variables can be accessed with:\nvariable_name &lt;- Sys.getenv(\"variable_name\")\nWhen working in a more complex environment structure where separate project, site, and user environments are being support this support article has useful information with a deeper dive into R’s startup here.\n\n\n\nStoring secrets securely can be done using the edit_r_environ function from the usethis package. For more overview see this overview.\nExample:\nlibrary(usethis)\nusethis::edit_r_environ(scope = \"project\")\nAccessing those stored parameters later can be done using Sys.getenv(\"DB_NAME\").\nBe sure to add the project level .Renviron file to your .gitignore so you aren’t exposing secrets when code is being saved to your git repository. Similarly this can be done with the edit_git_ignore(scope = c(\"user\", \"project\")) function. For more best practices see securing credentials.\n\nWhile typically explicitly listing the file name is the desired addition, wildcards can be added to exclude a type of file. For example: *.html.\n\nAfter updating these files the project should be closed and re-opened for any additions to be pulled in. One way to do this is through session -&gt; restart R (ctrl-shift-f10).\n\n\n\nAnother approach, particularly useful when automating testing and deployments using github actions, is to include the environment variables as secrets. Once this has been added through the git UI for the project they can then be referenced in the relevant deployment .yaml file with something like CONNECT_ENV_SET_ZD_USER: ${{ secrets.ZD_USER }}. In the R scripts they will be referenced as usual with something like Sys.getenv(\"DB_NAME\").\n\n\n\nStarting with version 1.6, RStudio Connect allows Environment Variables. The variables are encrypted on-disk, and in-memory.\nThis can be done at the project level with securing deployment through the Connect UI.\nUse Connect to manage environment variables through the UI: https://docs.posit.co/connect/user/content-settings/index.html#content-vars\nHave your admin use a supervisor script to add Environment Variables automatically: https://docs.posit.co/connect/admin/process-management/index.html#example-environment-variables\nSecuring credentials solutions article: https://solutions.posit.co/connections/db/best-practices/managing-credentials/"
  },
  {
    "objectID": "work/vetiver.html",
    "href": "work/vetiver.html",
    "title": "Publish and version your models with Vetiver",
    "section": "",
    "text": "This is a minimal example of the ML development workflow in R using Vetiver and the Posit Pro products.\nFavorite resources:"
  },
  {
    "objectID": "work/vetiver.html#the-steps",
    "href": "work/vetiver.html#the-steps",
    "title": "Publish and version your models with Vetiver",
    "section": "The steps:",
    "text": "The steps:\n\n\ncreate a deployable vetiver model: https://vetiver.rstudio.com/get-started/\n\n\npublish and version your model: https://vetiver.rstudio.com/get-started/version.html\n\n\ndeploy your model as a REST API: https://vetiver.rstudio.com/get-started/deploy.html"
  },
  {
    "objectID": "work/vetiver.html#in-action",
    "href": "work/vetiver.html#in-action",
    "title": "Publish and version your models with Vetiver",
    "section": "In action",
    "text": "In action\nBike Predict (R): https://solutions.posit.co/gallery/bike_predict/ Bike Predict (Python): https://github.com/sol-eng/bike_predict_python\nFollowing steps outline in: https://vetiver.rstudio.com/get-started/version.html"
  },
  {
    "objectID": "work/vetiver.html#train-a-model",
    "href": "work/vetiver.html#train-a-model",
    "title": "Publish and version your models with Vetiver",
    "section": "Train a model",
    "text": "Train a model\nlibrary(tidymodels)\n\ncar_mod &lt;-\n    workflow(mpg ~ ., linear_reg()) %&gt;%\n    fit(mtcars)"
  },
  {
    "objectID": "work/vetiver.html#create-a-vetiver-model",
    "href": "work/vetiver.html#create-a-vetiver-model",
    "title": "Publish and version your models with Vetiver",
    "section": "Create a vetiver model",
    "text": "Create a vetiver model\nlibrary(vetiver)\nv &lt;- vetiver_model(car_mod, \"lisa.anders/cars_mpg_model\")\nv"
  },
  {
    "objectID": "work/vetiver.html#store-and-version-your-model",
    "href": "work/vetiver.html#store-and-version-your-model",
    "title": "Publish and version your models with Vetiver",
    "section": "Store and version your model",
    "text": "Store and version your model\nStore secrets in your environment so they aren’t exposed in your code directly with usethis::edit_r_environ(). Pinned to: https://colorado.posit.co/rsc/connect/#/apps/b858985c-4e7f-4e05-be35-1bb6f21c9cd4/access\nauth = “envvar” uses environment variables CONNECT_API_KEY and CONNECT_SERVER.\nlibrary(pins)\nmodel_board &lt;- board_connect(auth = \"envvar\") # authenticates to Connect via environment variables\nvetiver_pin_write(model_board, v)\nTo read the vetiver model object from your board, use model_board %&gt;% vetiver_pin_read(\"cars_mpg\")."
  },
  {
    "objectID": "work/vetiver.html#create-a-rest-api-for-deployment",
    "href": "work/vetiver.html#create-a-rest-api-for-deployment",
    "title": "Publish and version your models with Vetiver",
    "section": "Create a REST API for deployment",
    "text": "Create a REST API for deployment\nTo start a server using this object, pipe (%&gt;%) to pr_run(port = 8080) or your port of choice.\nFor RStudio Connect, you can deploy your versioned model with a single function, either vetiver_deploy_rsconnect() for R or vetiver.deploy_rsconnect() for Python. For more on these options, see the Connect documentation for using vetiver.\n# Run while developing\n\n# library(plumber)\n# pr() %&gt;%\n#   vetiver_api(v) %&gt;% \n#   pr_run(port = 8080)\n\n# system2(\"ls\")"
  },
  {
    "objectID": "work/vetiver.html#deploy-to-connect",
    "href": "work/vetiver.html#deploy-to-connect",
    "title": "Publish and version your models with Vetiver",
    "section": "Deploy to Connect",
    "text": "Deploy to Connect\nAPI deployed to: https://colorado.posit.co/rsc/connect/#/apps/01ef1a23-3cd3-459f-b5e1-9c002104c065/access\nvetiver_deploy_rsconnect( \n     model_board, \n     \"lisa.anders/cars_mpg_model\", \n     predict_args = list(debug = TRUE), \n     #server=\"\",\n     account = \"lisa.anders\" \n     )"
  },
  {
    "objectID": "work/vetiver.html#predict-from-your-model-endpoint",
    "href": "work/vetiver.html#predict-from-your-model-endpoint",
    "title": "Publish and version your models with Vetiver",
    "section": "Predict from your model endpoint",
    "text": "Predict from your model endpoint\nendpoint &lt;- vetiver_endpoint(\"https://server.url.co/rsc/content/01ef1a23-3cd3-459f-b5e1-9c002104c065/predict\")\n\napiKey &lt;- Sys.getenv(\"CONNECT_API_KEY\")\n\n\nnew_car &lt;- tibble(cyl = 4,  disp = 200,\n                  hp = 100, drat = 3,\n                  wt = 3,   qsec = 17,\n                  vs = 0,   am = 1,\n                  gear = 4, carb = 2)\n\n\ntime_start &lt;- Sys.time()\n\npredict(endpoint, new_car, httr::add_headers(Authorization = paste(\"Key\", apiKey)))\n\ntime_end &lt;- Sys.time()\n\ntime_diff &lt;- difftime(time_end, time_start)"
  },
  {
    "objectID": "work/vetiver.html#compute-metrics",
    "href": "work/vetiver.html#compute-metrics",
    "title": "Publish and version your models with Vetiver",
    "section": "Compute metrics",
    "text": "Compute metrics\nlibrary(vetiver)\nlibrary(tidyverse)\ncars &lt;- read_csv(\"https://vetiver.rstudio.com/get-started/new-cars.csv\")\noriginal_cars &lt;- slice(cars, 1:14)\n\noriginal_metrics &lt;-\n    augment(v, new_data = original_cars) %&gt;%\n    vetiver_compute_metrics(date_obs, \"week\", mpg, .pred)\n\noriginal_metrics\nSpecify the metrics you want to have for monitoring your model over time."
  },
  {
    "objectID": "work/vetiver.html#pin-the-metrics",
    "href": "work/vetiver.html#pin-the-metrics",
    "title": "Publish and version your models with Vetiver",
    "section": "Pin the metrics",
    "text": "Pin the metrics\nCreate a new board:\nmodel_board &lt;- board_connect(auth = \"envvar\") # authenticates to Connect via environment variables\n\nmodel_board %&gt;% pin_write(original_metrics, \"lisa.anders/tree_metrics\")\nLater we will want to update it so we can look back at how it is changing over time:\n# dates overlap with existing metrics:\nnew_cars &lt;- slice(cars, -1:-7)\nnew_metrics &lt;-\n    augment(v, new_data = new_cars) %&gt;%\n    vetiver_compute_metrics(date_obs, \"week\", mpg, .pred)\n\nmodel_board %&gt;%\n    vetiver_pin_metrics(new_metrics, \"lisa.anders/tree_metrics\", overwrite = TRUE)"
  },
  {
    "objectID": "work/vetiver.html#visualize-performance-over-time",
    "href": "work/vetiver.html#visualize-performance-over-time",
    "title": "Publish and version your models with Vetiver",
    "section": "Visualize performance over time",
    "text": "Visualize performance over time\nlibrary(ggplot2)\nmonitoring_metrics &lt;- model_board %&gt;% pin_read(\"lisa.anders/tree_metrics\")\nvetiver_plot_metrics(monitoring_metrics) +\n    scale_size(range = c(2, 4))"
  },
  {
    "objectID": "work/job-templating-k8s.html",
    "href": "work/job-templating-k8s.html",
    "title": "Job templating in Kubernetes",
    "section": "",
    "text": "This short writeup addresses a need to add an additional label to Workbench sessions started in a kubernetes environment. Specifically this was for an environemnt where an external Workbench server was launching sessions into a Kubernetes cluster. The recommended way to do this is to use job templating."
  },
  {
    "objectID": "work/job-templating-k8s.html#job-templating-for-custom-labels-for-posit-workbench-with-k8s",
    "href": "work/job-templating-k8s.html#job-templating-for-custom-labels-for-posit-workbench-with-k8s",
    "title": "Job templating in Kubernetes",
    "section": "Job templating for custom labels for Posit Workbench with k8s",
    "text": "Job templating for custom labels for Posit Workbench with k8s\nThese are the granular steps for taking advantage of kubernetes object templating that would work with an external Workbench instance launching into k8s:\n\nAdd to launcher.kubernetes.conf, use-templating = 1\nUse the launcher CLI to create the job.tpl and service.tpl files (see below for an example)\nConfirm that the version called out in job.tpl and service.tpl (using head job.tpl and head service.tpl for example) are compatible with your workbench version (version 2.3.1 is latest)\nCreate rstudio-library-templates-data.tpl - for now this is a helm-only construct so it will need to be copied from the helm output (helm repo) (see example below for doing this with the helm template command)\nCopy job.tpl, service.tpl, and rstudio-library-templates-data.tpl into the launcher scratch directory on the host machine (in this case your Workbench server outside of k8s) (see below for default for where this is located) and make sure that rstudio-server has read/write access\n\nFor adding the new labels/values, here are good options:\n\nModify rstudio-library-templates-data.tpl directly (either in the header bit, or lower under values/labels)\nModify the values in the helm values file (example)\nRecommended: Add the values at time of making the template, IE with --set launcher.templateValues.job.labels.test=value \\ (see below)\n\nFrom talking with a colleague they shared this example using helm to create the template with the added label values:\nhelm template my-release rstudio/rstudio-workbench \\\n --set launcher.useTemplates=true \\\n --set launcherPem=test \\\n --set session.defaultConfigMount=false \\\n --set launcher.templateValues.job.labels.test=value \\\n --set launcher.templateValues.job.labels.other=something-else \\\n | less\nIn that helm example:\n\nsession.defaultConfigMount=false prevents defining volumes and volumeMounts that are only relevant for the helm chart\nlauncherPem=test speeds up templating\nlauncher.useTemplates=true turns on templating (instead of job json overrides)\n\nWhat if things go wrong?\nThe rstudio-kubernetes-launcher CLI can verify templates now!\nPlaying with the rstudio-kubernetes-launcher CLI:\n/usr/lib/rstudio-server/bin/rstudio-kubernetes-launcher --help\n/usr/lib/rstudio-server/bin/rstudio-kubernetes-launcher --generate-templates --scratch-path=/tmp/hello\nDefault scratch path:\n # mount into the default scratch-path\n - name: session-templates\n  mountPath: \"/var/lib/rstudio-launcher/Kubernetes/rstudio-library-templates-data.tpl\"\n  subPath: \"rstudio-library-templates-data.tpl\"\n - name: session-templates\n  mountPath: \"/var/lib/rstudio-launcher/Kubernetes/job.tpl\"\n  subPath: \"job.tpl\"\n - name: session-templates\n  mountPath: \"/var/lib/rstudio-launcher/Kubernetes/service.tpl\"\n  subPath: \"service.tpl\""
  },
  {
    "objectID": "work/job-templating-k8s.html#another-approach-using-json-over-rides-on-posit-workbench-with-k8s",
    "href": "work/job-templating-k8s.html#another-approach-using-json-over-rides-on-posit-workbench-with-k8s",
    "title": "Job templating in Kubernetes",
    "section": "Another approach using json over-rides on Posit Workbench with k8s",
    "text": "Another approach using json over-rides on Posit Workbench with k8s\nPotentially json over-rides could work but my understanding is that this would overwrite ALL labels.\nuse this to add a label:\n\n\n/etc/rstudio/launcher.kubernetes.profiles.conf\n\njob-json-overrides=\"/spec/template/spec/labels\":\"/etc/rstudio/labels\"\n\n\n\n/etc/rstudio/labels\n\n[\n  {\n    \"applicationid\": \"workbench\",\n    \"label\": [\"value\"]\n  }\n]"
  },
  {
    "objectID": "work/shiny-apps-and-analytics.html",
    "href": "work/shiny-apps-and-analytics.html",
    "title": "Shiny apps and Analytics",
    "section": "",
    "text": "Adding instrumentation to your shiny apps can help you understand:\n\nUser interaction\nMost viewed dashboards\nWhat is being exported and by who\nTrends with access over time\nTime spent on interactions\n\nConnect (as a software) isn’t aggregating and making available user interaction information, though it is aggregating higher level app specific access auditing type information. HOwever, advanced interaction instrumentation can be accomplished through adding tracking into the content itself using services like hotjar, google analytics, or saving into the log file or pushing into somewhere external.\n\nThere’s more on the Connect API here: https://solutions.posit.co/operations/connect-apis/\nGoogle analytics likely requires adding some JavaScript to your site which you can do with Shiny, https://shiny.posit.co/r/reference/shiny/latest/builder.html\nThis page in the Shiny docs give a walkthrough as well: https://shiny.posit.co/r/articles/build/google-analytics/\nTracking where within the Shiny app users are spending their time (which tab, etc) is interesting.\nMonitoring user behavior with hotjar (paid): https://www.r-bloggers.com/2022/06/r-shiny-hotjar-how-to-monitor-user-behavior-in-r-shiny-apps/\nHere’s an article that outlines more free options: https://www.appsilon.com/post/monitoring-r-shiny-user-adoption\nJavascript might need to be added to your site to support using google analytics: https://shiny.posit.co/r/reference/shiny/latest/builder.html\nVarious examples for tracking built in to Connect: https://solutions.rstudio.com/data-science-admin/tracking/ and https://github.com/sol-eng/connect-usage\nVideo showing how to use the Connect API: https://www.youtube.com/watch?v=0iljqY9j64U"
  },
  {
    "objectID": "work/r-projects-setup.html",
    "href": "work/r-projects-setup.html",
    "title": "R projects set up and maintenance",
    "section": "",
    "text": "Setup the renv environment:\nrenv::activate()\nrenv::restore()\nTo run the app either open app/app.R and click the “Run App” button at the top of the IDE code pane or use:\nshiny::runApp(\"app\")"
  },
  {
    "objectID": "work/r-projects-setup.html#r-projects-setup",
    "href": "work/r-projects-setup.html#r-projects-setup",
    "title": "R projects set up and maintenance",
    "section": "",
    "text": "Setup the renv environment:\nrenv::activate()\nrenv::restore()\nTo run the app either open app/app.R and click the “Run App” button at the top of the IDE code pane or use:\nshiny::runApp(\"app\")"
  },
  {
    "objectID": "work/r-projects-setup.html#deployment",
    "href": "work/r-projects-setup.html#deployment",
    "title": "R projects set up and maintenance",
    "section": "Deployment",
    "text": "Deployment\n\nPush Button\nOpen app/app.R and use the blue publish icon in the upper right corner of the IDE code pane.\n\n\nrsconnect package\nYou can also deploy using the rsconnect package:\nrsconnect::deployApp(\n  appDir = \"app\",\n  appTitle = \"Shiny Penguins\"\n)\n\n\nGit-backed\nUpdate the code, and then run:\nrsconnect::writeManifest(\"app\")\nCommit the new manifest.json file to the git repo along with the code."
  },
  {
    "objectID": "work/r-projects-setup.html#project-updates",
    "href": "work/r-projects-setup.html#project-updates",
    "title": "R projects set up and maintenance",
    "section": "Project updates",
    "text": "Project updates\nUse renv to record the r package versions used\nCreate a manifest.json file to support git-backed publishing"
  },
  {
    "objectID": "work/r-projects-setup.html#all-about-renv",
    "href": "work/r-projects-setup.html#all-about-renv",
    "title": "R projects set up and maintenance",
    "section": "All about renv",
    "text": "All about renv\nWhy use renv?\nThere is an excellent video by David Aja discussing why he started using renv at the 2022 RStudio Conference here.\nEver had your code mysteriously stop working or start producing different results after upgrading packages, and had to spend hours debugging to find which package was the culprit? Ever tried to collaborate on code just to get stuck on trying to decipher various package dependencies?\nrenv helps you track and control package changes - making it easy to revert back if you need to. It works with your current methods of installing packages (install.packages()), and was designed to work with most data science workflows.\nWho shouldn’t use renv?\n\nPackage developers\n?\n\n# Terms\n\nR Project - a special kind of directory of files and supporting functionality.\nPackage - a collection of functions beyond base R that developers can install and use.\nLibrary - a directory containing installed packages."
  },
  {
    "objectID": "work/python-projects-setup.html",
    "href": "work/python-projects-setup.html",
    "title": "Python projects set up and maintenance",
    "section": "",
    "text": "Setup the venv environment:\npython -m venv .venv\n. .venv/bin/activate\n# .venv\\Scripts\\activate # Windows\nUpgrade pip and then install needed packages:\npip install --upgrade pip\npython -m pip install --upgrade pip wheel setuptools rsconnect-python\npip install -r requirements.txt\nRun the application:\nshiny run --reload app.py\nLeave a virtual environment with:\ndeactivate"
  },
  {
    "objectID": "work/python-projects-setup.html#python-projects-setup",
    "href": "work/python-projects-setup.html#python-projects-setup",
    "title": "Python projects set up and maintenance",
    "section": "",
    "text": "Setup the venv environment:\npython -m venv .venv\n. .venv/bin/activate\n# .venv\\Scripts\\activate # Windows\nUpgrade pip and then install needed packages:\npip install --upgrade pip\npython -m pip install --upgrade pip wheel setuptools rsconnect-python\npip install -r requirements.txt\nRun the application:\nshiny run --reload app.py\nLeave a virtual environment with:\ndeactivate"
  },
  {
    "objectID": "work/python-projects-setup.html#deploy",
    "href": "work/python-projects-setup.html#deploy",
    "title": "Python projects set up and maintenance",
    "section": "Deploy",
    "text": "Deploy\n\nrsconnect-python CLI\nrsconnect deploy shiny .\n\n\nGit-backed\nUpdate the code, and then run:\nrsconnect write-manifest shiny --overwrite .\nCommit the new manifest.json file to the git repo along with the code."
  },
  {
    "objectID": "work/python-projects-setup.html#project-updates",
    "href": "work/python-projects-setup.html#project-updates",
    "title": "Python projects set up and maintenance",
    "section": "Project updates",
    "text": "Project updates\nCreate the requirements file:\npython -m pip freeze &gt; requirements.txt\nrsconnect write-manifest shiny .\nIf you are running into deploy issues where there are breaking packages you can edit the requirements file explicitly:\n# requirements.txt generated by rsconnect-python on 2022-09-21 14:59:58.865441\nstreamlit==1.11.0\nTo use a Package Manager repository with a specific project defined by a requirements.txt file, add -i [repositoryURL] to the top of your file, for example:\n-i https://packagemanager.posit.co/pypi/latest/simple\npandas\nscipy\nHow to configure a pypi repository globally (using pip.conf): https://docs.posit.co/resources/install-python/#optional-configure-a-pypi-repository"
  },
  {
    "objectID": "work/python-projects-setup.html#troubleshooting",
    "href": "work/python-projects-setup.html#troubleshooting",
    "title": "Python projects set up and maintenance",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIssues with Python not being on path\nSet it manually to an installed Python version with:\nalias python=\"/opt/python/3.9.17/bin/python\"\nSet it in your .bashrc on mac or linux so that it is set for your profile every time you log in (typically this is located in the root directory of your home folder):\n# add this to your .bashrc\nexport PATH=/opt/python/3.11.9/bin:$PATH\nCheck for the available python versions (if typically installed):\nls -ld /opt/python/*"
  },
  {
    "objectID": "work/python-projects-setup.html#resources",
    "href": "work/python-projects-setup.html#resources",
    "title": "Python projects set up and maintenance",
    "section": "Resources",
    "text": "Resources\nPosit Connect User Guide: Shiny for Python"
  }
]